{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5EBz6KbQBGZ",
        "outputId": "e15146e3-697d-41cd-980f-29e2572b1f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# List of weights/Mean square Error (Mse) for each input\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    # Print the weights and initialize the lost\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    print(\"MSE=\", l_sum / len(x_data))\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / len(x_data))\n",
        "\n",
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0  # a random guess: random value\n",
        "\n",
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "    return 2 * x * (x * w - y)\n",
        "\n",
        "\n",
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # Compute derivative w.r.t to the learned weights\n",
        "        # Update the weights\n",
        "        # Compute the loss and print progress\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        l = loss(x_val, y_val)\n",
        "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
        "\n",
        "# After training\n",
        "print(\"Predicted score (after training)\",  \"4 hours of studying: \", forward(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVcU2Ig5QMnX",
        "outputId": "2518432f-28a2-4184-f357-929e2727bce4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 2.69\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1.47\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 0.8\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 0.24\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 0.02\n",
            "Predicted score (after training) 4 hours of studying:  7.804863933862125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pdb\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "\n",
        "# our model forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss function\n",
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2\n",
        "\n",
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4).item())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val) # 1) Forward pass\n",
        "        l = loss(y_pred, y_val) # 2) Compute loss\n",
        "        l.backward() # 3) Back propagation to update weights\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "# After training\n",
        "print(\"Prediction (after training)\",  4, forward(4).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2vbW0lLQSOf",
        "outputId": "1c5da2d0-033f-455a-d0c6-6c20e33593ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction (after training) 4 7.804864406585693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_PIZB-1QXNY",
        "outputId": "f27d69cc-4b57-4191-8674-d0687ffa6320"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 49.31557083129883 \n",
            "Epoch: 1 | Loss: 22.259464263916016 \n",
            "Epoch: 2 | Loss: 10.210461616516113 \n",
            "Epoch: 3 | Loss: 4.842262268066406 \n",
            "Epoch: 4 | Loss: 2.4482226371765137 \n",
            "Epoch: 5 | Loss: 1.378260850906372 \n",
            "Epoch: 6 | Loss: 0.8977992534637451 \n",
            "Epoch: 7 | Loss: 0.6798266768455505 \n",
            "Epoch: 8 | Loss: 0.578765332698822 \n",
            "Epoch: 9 | Loss: 0.5298075675964355 \n",
            "Epoch: 10 | Loss: 0.5041013360023499 \n",
            "Epoch: 11 | Loss: 0.48880231380462646 \n",
            "Epoch: 12 | Loss: 0.4781920909881592 \n",
            "Epoch: 13 | Loss: 0.4697237014770508 \n",
            "Epoch: 14 | Loss: 0.4622627794742584 \n",
            "Epoch: 15 | Loss: 0.4553026854991913 \n",
            "Epoch: 16 | Loss: 0.44861900806427 \n",
            "Epoch: 17 | Loss: 0.442108690738678 \n",
            "Epoch: 18 | Loss: 0.4357268214225769 \n",
            "Epoch: 19 | Loss: 0.42945247888565063 \n",
            "Epoch: 20 | Loss: 0.4232748746871948 \n",
            "Epoch: 21 | Loss: 0.4171890914440155 \n",
            "Epoch: 22 | Loss: 0.4111926555633545 \n",
            "Epoch: 23 | Loss: 0.4052827060222626 \n",
            "Epoch: 24 | Loss: 0.39945781230926514 \n",
            "Epoch: 25 | Loss: 0.3937172293663025 \n",
            "Epoch: 26 | Loss: 0.388058602809906 \n",
            "Epoch: 27 | Loss: 0.38248181343078613 \n",
            "Epoch: 28 | Loss: 0.3769844174385071 \n",
            "Epoch: 29 | Loss: 0.3715668320655823 \n",
            "Epoch: 30 | Loss: 0.3662269115447998 \n",
            "Epoch: 31 | Loss: 0.3609634339809418 \n",
            "Epoch: 32 | Loss: 0.35577595233917236 \n",
            "Epoch: 33 | Loss: 0.35066282749176025 \n",
            "Epoch: 34 | Loss: 0.34562334418296814 \n",
            "Epoch: 35 | Loss: 0.3406563401222229 \n",
            "Epoch: 36 | Loss: 0.3357604742050171 \n",
            "Epoch: 37 | Loss: 0.33093497157096863 \n",
            "Epoch: 38 | Loss: 0.3261791467666626 \n",
            "Epoch: 39 | Loss: 0.32149121165275574 \n",
            "Epoch: 40 | Loss: 0.31687062978744507 \n",
            "Epoch: 41 | Loss: 0.3123171627521515 \n",
            "Epoch: 42 | Loss: 0.30782830715179443 \n",
            "Epoch: 43 | Loss: 0.3034045994281769 \n",
            "Epoch: 44 | Loss: 0.2990442216396332 \n",
            "Epoch: 45 | Loss: 0.2947462499141693 \n",
            "Epoch: 46 | Loss: 0.290510356426239 \n",
            "Epoch: 47 | Loss: 0.2863353192806244 \n",
            "Epoch: 48 | Loss: 0.2822200655937195 \n",
            "Epoch: 49 | Loss: 0.2781643867492676 \n",
            "Epoch: 50 | Loss: 0.2741665244102478 \n",
            "Epoch: 51 | Loss: 0.27022624015808105 \n",
            "Epoch: 52 | Loss: 0.26634281873703003 \n",
            "Epoch: 53 | Loss: 0.26251503825187683 \n",
            "Epoch: 54 | Loss: 0.258742094039917 \n",
            "Epoch: 55 | Loss: 0.2550237774848938 \n",
            "Epoch: 56 | Loss: 0.25135862827301025 \n",
            "Epoch: 57 | Loss: 0.24774624407291412 \n",
            "Epoch: 58 | Loss: 0.2441854625940323 \n",
            "Epoch: 59 | Loss: 0.24067634344100952 \n",
            "Epoch: 60 | Loss: 0.23721753060817719 \n",
            "Epoch: 61 | Loss: 0.23380820453166962 \n",
            "Epoch: 62 | Loss: 0.23044809699058533 \n",
            "Epoch: 63 | Loss: 0.22713613510131836 \n",
            "Epoch: 64 | Loss: 0.22387179732322693 \n",
            "Epoch: 65 | Loss: 0.22065457701683044 \n",
            "Epoch: 66 | Loss: 0.217483252286911 \n",
            "Epoch: 67 | Loss: 0.21435773372650146 \n",
            "Epoch: 68 | Loss: 0.21127690374851227 \n",
            "Epoch: 69 | Loss: 0.2082405984401703 \n",
            "Epoch: 70 | Loss: 0.20524799823760986 \n",
            "Epoch: 71 | Loss: 0.20229807496070862 \n",
            "Epoch: 72 | Loss: 0.19939090311527252 \n",
            "Epoch: 73 | Loss: 0.19652530550956726 \n",
            "Epoch: 74 | Loss: 0.19370080530643463 \n",
            "Epoch: 75 | Loss: 0.19091704487800598 \n",
            "Epoch: 76 | Loss: 0.18817338347434998 \n",
            "Epoch: 77 | Loss: 0.18546906113624573 \n",
            "Epoch: 78 | Loss: 0.18280357122421265 \n",
            "Epoch: 79 | Loss: 0.1801764816045761 \n",
            "Epoch: 80 | Loss: 0.17758715152740479 \n",
            "Epoch: 81 | Loss: 0.17503482103347778 \n",
            "Epoch: 82 | Loss: 0.1725190132856369 \n",
            "Epoch: 83 | Loss: 0.17003993690013885 \n",
            "Epoch: 84 | Loss: 0.16759620606899261 \n",
            "Epoch: 85 | Loss: 0.1651875078678131 \n",
            "Epoch: 86 | Loss: 0.16281351447105408 \n",
            "Epoch: 87 | Loss: 0.16047357022762299 \n",
            "Epoch: 88 | Loss: 0.1581675112247467 \n",
            "Epoch: 89 | Loss: 0.1558942198753357 \n",
            "Epoch: 90 | Loss: 0.15365378558635712 \n",
            "Epoch: 91 | Loss: 0.1514453887939453 \n",
            "Epoch: 92 | Loss: 0.1492689996957779 \n",
            "Epoch: 93 | Loss: 0.14712370932102203 \n",
            "Epoch: 94 | Loss: 0.14500944316387177 \n",
            "Epoch: 95 | Loss: 0.14292529225349426 \n",
            "Epoch: 96 | Loss: 0.1408713459968567 \n",
            "Epoch: 97 | Loss: 0.13884687423706055 \n",
            "Epoch: 98 | Loss: 0.13685132563114166 \n",
            "Epoch: 99 | Loss: 0.13488446176052094 \n",
            "Epoch: 100 | Loss: 0.13294589519500732 \n",
            "Epoch: 101 | Loss: 0.13103541731834412 \n",
            "Epoch: 102 | Loss: 0.12915214896202087 \n",
            "Epoch: 103 | Loss: 0.12729619443416595 \n",
            "Epoch: 104 | Loss: 0.12546652555465698 \n",
            "Epoch: 105 | Loss: 0.12366348505020142 \n",
            "Epoch: 106 | Loss: 0.12188630551099777 \n",
            "Epoch: 107 | Loss: 0.12013454735279083 \n",
            "Epoch: 108 | Loss: 0.11840798705816269 \n",
            "Epoch: 109 | Loss: 0.11670629680156708 \n",
            "Epoch: 110 | Loss: 0.11502893269062042 \n",
            "Epoch: 111 | Loss: 0.11337590217590332 \n",
            "Epoch: 112 | Loss: 0.11174644529819489 \n",
            "Epoch: 113 | Loss: 0.11014062911272049 \n",
            "Epoch: 114 | Loss: 0.1085575520992279 \n",
            "Epoch: 115 | Loss: 0.10699747502803802 \n",
            "Epoch: 116 | Loss: 0.10545985400676727 \n",
            "Epoch: 117 | Loss: 0.10394412279129028 \n",
            "Epoch: 118 | Loss: 0.10245037078857422 \n",
            "Epoch: 119 | Loss: 0.10097789764404297 \n",
            "Epoch: 120 | Loss: 0.0995268002152443 \n",
            "Epoch: 121 | Loss: 0.09809626638889313 \n",
            "Epoch: 122 | Loss: 0.0966867059469223 \n",
            "Epoch: 123 | Loss: 0.0952969416975975 \n",
            "Epoch: 124 | Loss: 0.09392742812633514 \n",
            "Epoch: 125 | Loss: 0.09257753193378448 \n",
            "Epoch: 126 | Loss: 0.09124711155891418 \n",
            "Epoch: 127 | Loss: 0.08993569016456604 \n",
            "Epoch: 128 | Loss: 0.0886431634426117 \n",
            "Epoch: 129 | Loss: 0.08736927807331085 \n",
            "Epoch: 130 | Loss: 0.08611370623111725 \n",
            "Epoch: 131 | Loss: 0.08487600833177567 \n",
            "Epoch: 132 | Loss: 0.08365622162818909 \n",
            "Epoch: 133 | Loss: 0.08245404064655304 \n",
            "Epoch: 134 | Loss: 0.08126892149448395 \n",
            "Epoch: 135 | Loss: 0.08010101318359375 \n",
            "Epoch: 136 | Loss: 0.07894974946975708 \n",
            "Epoch: 137 | Loss: 0.07781527936458588 \n",
            "Epoch: 138 | Loss: 0.0766969621181488 \n",
            "Epoch: 139 | Loss: 0.07559459656476974 \n",
            "Epoch: 140 | Loss: 0.07450822740793228 \n",
            "Epoch: 141 | Loss: 0.07343731820583344 \n",
            "Epoch: 142 | Loss: 0.07238202542066574 \n",
            "Epoch: 143 | Loss: 0.07134167850017548 \n",
            "Epoch: 144 | Loss: 0.07031641900539398 \n",
            "Epoch: 145 | Loss: 0.06930593401193619 \n",
            "Epoch: 146 | Loss: 0.06830982863903046 \n",
            "Epoch: 147 | Loss: 0.06732812523841858 \n",
            "Epoch: 148 | Loss: 0.0663604587316513 \n",
            "Epoch: 149 | Loss: 0.06540680676698685 \n",
            "Epoch: 150 | Loss: 0.0644666999578476 \n",
            "Epoch: 151 | Loss: 0.06354037672281265 \n",
            "Epoch: 152 | Loss: 0.06262708455324173 \n",
            "Epoch: 153 | Loss: 0.06172705814242363 \n",
            "Epoch: 154 | Loss: 0.060840021818876266 \n",
            "Epoch: 155 | Loss: 0.059965528547763824 \n",
            "Epoch: 156 | Loss: 0.05910385772585869 \n",
            "Epoch: 157 | Loss: 0.05825436860322952 \n",
            "Epoch: 158 | Loss: 0.05741716921329498 \n",
            "Epoch: 159 | Loss: 0.05659203231334686 \n",
            "Epoch: 160 | Loss: 0.05577868968248367 \n",
            "Epoch: 161 | Loss: 0.05497708544135094 \n",
            "Epoch: 162 | Loss: 0.05418705940246582 \n",
            "Epoch: 163 | Loss: 0.05340820923447609 \n",
            "Epoch: 164 | Loss: 0.052640728652477264 \n",
            "Epoch: 165 | Loss: 0.05188402533531189 \n",
            "Epoch: 166 | Loss: 0.051138412207365036 \n",
            "Epoch: 167 | Loss: 0.05040358752012253 \n",
            "Epoch: 168 | Loss: 0.04967925697565079 \n",
            "Epoch: 169 | Loss: 0.0489652045071125 \n",
            "Epoch: 170 | Loss: 0.04826153814792633 \n",
            "Epoch: 171 | Loss: 0.047567836940288544 \n",
            "Epoch: 172 | Loss: 0.04688429459929466 \n",
            "Epoch: 173 | Loss: 0.04621041566133499 \n",
            "Epoch: 174 | Loss: 0.04554629325866699 \n",
            "Epoch: 175 | Loss: 0.044891759753227234 \n",
            "Epoch: 176 | Loss: 0.04424663633108139 \n",
            "Epoch: 177 | Loss: 0.043610718101263046 \n",
            "Epoch: 178 | Loss: 0.04298388957977295 \n",
            "Epoch: 179 | Loss: 0.04236619547009468 \n",
            "Epoch: 180 | Loss: 0.041757307946681976 \n",
            "Epoch: 181 | Loss: 0.04115721583366394 \n",
            "Epoch: 182 | Loss: 0.04056568443775177 \n",
            "Epoch: 183 | Loss: 0.039982788264751434 \n",
            "Epoch: 184 | Loss: 0.03940814733505249 \n",
            "Epoch: 185 | Loss: 0.03884178400039673 \n",
            "Epoch: 186 | Loss: 0.038283608853816986 \n",
            "Epoch: 187 | Loss: 0.03773333504796028 \n",
            "Epoch: 188 | Loss: 0.03719109296798706 \n",
            "Epoch: 189 | Loss: 0.03665656968951225 \n",
            "Epoch: 190 | Loss: 0.03612982854247093 \n",
            "Epoch: 191 | Loss: 0.03561047464609146 \n",
            "Epoch: 192 | Loss: 0.03509880602359772 \n",
            "Epoch: 193 | Loss: 0.034594323486089706 \n",
            "Epoch: 194 | Loss: 0.03409714996814728 \n",
            "Epoch: 195 | Loss: 0.0336071252822876 \n",
            "Epoch: 196 | Loss: 0.03312409669160843 \n",
            "Epoch: 197 | Loss: 0.03264801204204559 \n",
            "Epoch: 198 | Loss: 0.03217891976237297 \n",
            "Epoch: 199 | Loss: 0.03171636909246445 \n",
            "Epoch: 200 | Loss: 0.03126060962677002 \n",
            "Epoch: 201 | Loss: 0.03081129491329193 \n",
            "Epoch: 202 | Loss: 0.030368462204933167 \n",
            "Epoch: 203 | Loss: 0.029932117089629173 \n",
            "Epoch: 204 | Loss: 0.029501864686608315 \n",
            "Epoch: 205 | Loss: 0.029077857732772827 \n",
            "Epoch: 206 | Loss: 0.02866002358496189 \n",
            "Epoch: 207 | Loss: 0.0282481350004673 \n",
            "Epoch: 208 | Loss: 0.027842175215482712 \n",
            "Epoch: 209 | Loss: 0.027442052960395813 \n",
            "Epoch: 210 | Loss: 0.02704767882823944 \n",
            "Epoch: 211 | Loss: 0.026658909395337105 \n",
            "Epoch: 212 | Loss: 0.02627578377723694 \n",
            "Epoch: 213 | Loss: 0.02589816227555275 \n",
            "Epoch: 214 | Loss: 0.025525975972414017 \n",
            "Epoch: 215 | Loss: 0.025159098207950592 \n",
            "Epoch: 216 | Loss: 0.024797562509775162 \n",
            "Epoch: 217 | Loss: 0.024441156536340714 \n",
            "Epoch: 218 | Loss: 0.024089999496936798 \n",
            "Epoch: 219 | Loss: 0.0237437654286623 \n",
            "Epoch: 220 | Loss: 0.02340240776538849 \n",
            "Epoch: 221 | Loss: 0.023066138848662376 \n",
            "Epoch: 222 | Loss: 0.022734683007001877 \n",
            "Epoch: 223 | Loss: 0.02240792289376259 \n",
            "Epoch: 224 | Loss: 0.022085893899202347 \n",
            "Epoch: 225 | Loss: 0.02176852524280548 \n",
            "Epoch: 226 | Loss: 0.021455595269799232 \n",
            "Epoch: 227 | Loss: 0.02114720642566681 \n",
            "Epoch: 228 | Loss: 0.02084329165518284 \n",
            "Epoch: 229 | Loss: 0.02054380252957344 \n",
            "Epoch: 230 | Loss: 0.020248539745807648 \n",
            "Epoch: 231 | Loss: 0.019957516342401505 \n",
            "Epoch: 232 | Loss: 0.01967068761587143 \n",
            "Epoch: 233 | Loss: 0.01938803307712078 \n",
            "Epoch: 234 | Loss: 0.019109409302473068 \n",
            "Epoch: 235 | Loss: 0.01883477158844471 \n",
            "Epoch: 236 | Loss: 0.018564078956842422 \n",
            "Epoch: 237 | Loss: 0.01829727366566658 \n",
            "Epoch: 238 | Loss: 0.018034327775239944 \n",
            "Epoch: 239 | Loss: 0.017775144428014755 \n",
            "Epoch: 240 | Loss: 0.017519671469926834 \n",
            "Epoch: 241 | Loss: 0.017267892137169838 \n",
            "Epoch: 242 | Loss: 0.017019737511873245 \n",
            "Epoch: 243 | Loss: 0.016775116324424744 \n",
            "Epoch: 244 | Loss: 0.016534000635147095 \n",
            "Epoch: 245 | Loss: 0.01629636064171791 \n",
            "Epoch: 246 | Loss: 0.016062185168266296 \n",
            "Epoch: 247 | Loss: 0.015831358730793 \n",
            "Epoch: 248 | Loss: 0.015603822655975819 \n",
            "Epoch: 249 | Loss: 0.015379614196717739 \n",
            "Epoch: 250 | Loss: 0.015158567577600479 \n",
            "Epoch: 251 | Loss: 0.014940720051527023 \n",
            "Epoch: 252 | Loss: 0.014726042747497559 \n",
            "Epoch: 253 | Loss: 0.0145143773406744 \n",
            "Epoch: 254 | Loss: 0.01430574618279934 \n",
            "Epoch: 255 | Loss: 0.014100163243710995 \n",
            "Epoch: 256 | Loss: 0.013897509314119816 \n",
            "Epoch: 257 | Loss: 0.013697799295186996 \n",
            "Epoch: 258 | Loss: 0.013500952161848545 \n",
            "Epoch: 259 | Loss: 0.013306879438459873 \n",
            "Epoch: 260 | Loss: 0.01311570219695568 \n",
            "Epoch: 261 | Loss: 0.012927204370498657 \n",
            "Epoch: 262 | Loss: 0.012741382233798504 \n",
            "Epoch: 263 | Loss: 0.012558321468532085 \n",
            "Epoch: 264 | Loss: 0.012377799488604069 \n",
            "Epoch: 265 | Loss: 0.01219986379146576 \n",
            "Epoch: 266 | Loss: 0.01202452927827835 \n",
            "Epoch: 267 | Loss: 0.011851703748106956 \n",
            "Epoch: 268 | Loss: 0.011681387200951576 \n",
            "Epoch: 269 | Loss: 0.011513538658618927 \n",
            "Epoch: 270 | Loss: 0.011348124593496323 \n",
            "Epoch: 271 | Loss: 0.011185001581907272 \n",
            "Epoch: 272 | Loss: 0.0110242934897542 \n",
            "Epoch: 273 | Loss: 0.010865814052522182 \n",
            "Epoch: 274 | Loss: 0.010709687136113644 \n",
            "Epoch: 275 | Loss: 0.01055574044585228 \n",
            "Epoch: 276 | Loss: 0.010404011234641075 \n",
            "Epoch: 277 | Loss: 0.01025454606860876 \n",
            "Epoch: 278 | Loss: 0.010107144713401794 \n",
            "Epoch: 279 | Loss: 0.009961854666471481 \n",
            "Epoch: 280 | Loss: 0.00981870573014021 \n",
            "Epoch: 281 | Loss: 0.00967766810208559 \n",
            "Epoch: 282 | Loss: 0.009538514539599419 \n",
            "Epoch: 283 | Loss: 0.009401433169841766 \n",
            "Epoch: 284 | Loss: 0.009266316890716553 \n",
            "Epoch: 285 | Loss: 0.009133163839578629 \n",
            "Epoch: 286 | Loss: 0.009001879021525383 \n",
            "Epoch: 287 | Loss: 0.008872530423104763 \n",
            "Epoch: 288 | Loss: 0.008745014667510986 \n",
            "Epoch: 289 | Loss: 0.008619307540357113 \n",
            "Epoch: 290 | Loss: 0.00849547516554594 \n",
            "Epoch: 291 | Loss: 0.008373362943530083 \n",
            "Epoch: 292 | Loss: 0.008253059349954128 \n",
            "Epoch: 293 | Loss: 0.008134433999657631 \n",
            "Epoch: 294 | Loss: 0.008017531596124172 \n",
            "Epoch: 295 | Loss: 0.007902281358838081 \n",
            "Epoch: 296 | Loss: 0.007788676768541336 \n",
            "Epoch: 297 | Loss: 0.007676801178604364 \n",
            "Epoch: 298 | Loss: 0.007566452957689762 \n",
            "Epoch: 299 | Loss: 0.007457692176103592 \n",
            "Epoch: 300 | Loss: 0.007350546307861805 \n",
            "Epoch: 301 | Loss: 0.007244942709803581 \n",
            "Epoch: 302 | Loss: 0.007140766363590956 \n",
            "Epoch: 303 | Loss: 0.007038149517029524 \n",
            "Epoch: 304 | Loss: 0.006936967838555574 \n",
            "Epoch: 305 | Loss: 0.006837302353233099 \n",
            "Epoch: 306 | Loss: 0.006739073898643255 \n",
            "Epoch: 307 | Loss: 0.006642182357609272 \n",
            "Epoch: 308 | Loss: 0.006546739488840103 \n",
            "Epoch: 309 | Loss: 0.00645268801599741 \n",
            "Epoch: 310 | Loss: 0.006359911058098078 \n",
            "Epoch: 311 | Loss: 0.006268518976867199 \n",
            "Epoch: 312 | Loss: 0.0061784107238054276 \n",
            "Epoch: 313 | Loss: 0.006089648697525263 \n",
            "Epoch: 314 | Loss: 0.006002114154398441 \n",
            "Epoch: 315 | Loss: 0.005915857385843992 \n",
            "Epoch: 316 | Loss: 0.005830802023410797 \n",
            "Epoch: 317 | Loss: 0.005747037008404732 \n",
            "Epoch: 318 | Loss: 0.0056644645519554615 \n",
            "Epoch: 319 | Loss: 0.005583032965660095 \n",
            "Epoch: 320 | Loss: 0.005502777174115181 \n",
            "Epoch: 321 | Loss: 0.005423706024885178 \n",
            "Epoch: 322 | Loss: 0.005345793906599283 \n",
            "Epoch: 323 | Loss: 0.005268935114145279 \n",
            "Epoch: 324 | Loss: 0.0051932320930063725 \n",
            "Epoch: 325 | Loss: 0.005118567496538162 \n",
            "Epoch: 326 | Loss: 0.005045042373239994 \n",
            "Epoch: 327 | Loss: 0.004972496535629034 \n",
            "Epoch: 328 | Loss: 0.00490102544426918 \n",
            "Epoch: 329 | Loss: 0.004830629099160433 \n",
            "Epoch: 330 | Loss: 0.004761179443448782 \n",
            "Epoch: 331 | Loss: 0.004692762158811092 \n",
            "Epoch: 332 | Loss: 0.004625299014151096 \n",
            "Epoch: 333 | Loss: 0.004558838903903961 \n",
            "Epoch: 334 | Loss: 0.004493334796279669 \n",
            "Epoch: 335 | Loss: 0.0044287689961493015 \n",
            "Epoch: 336 | Loss: 0.004365120083093643 \n",
            "Epoch: 337 | Loss: 0.0043023936450481415 \n",
            "Epoch: 338 | Loss: 0.0042405519634485245 \n",
            "Epoch: 339 | Loss: 0.00417962484061718 \n",
            "Epoch: 340 | Loss: 0.00411953404545784 \n",
            "Epoch: 341 | Loss: 0.0040603154338896275 \n",
            "Epoch: 342 | Loss: 0.004001948516815901 \n",
            "Epoch: 343 | Loss: 0.003944457042962313 \n",
            "Epoch: 344 | Loss: 0.003887765808030963 \n",
            "Epoch: 345 | Loss: 0.0038318843580782413 \n",
            "Epoch: 346 | Loss: 0.0037768182810395956 \n",
            "Epoch: 347 | Loss: 0.0037225340493023396 \n",
            "Epoch: 348 | Loss: 0.003669047961011529 \n",
            "Epoch: 349 | Loss: 0.00361630879342556 \n",
            "Epoch: 350 | Loss: 0.0035643507726490498 \n",
            "Epoch: 351 | Loss: 0.0035131163895130157 \n",
            "Epoch: 352 | Loss: 0.003462626365944743 \n",
            "Epoch: 353 | Loss: 0.0034128648694604635 \n",
            "Epoch: 354 | Loss: 0.003363831201568246 \n",
            "Epoch: 355 | Loss: 0.0033154941629618406 \n",
            "Epoch: 356 | Loss: 0.003267852356657386 \n",
            "Epoch: 357 | Loss: 0.003220858983695507 \n",
            "Epoch: 358 | Loss: 0.003174595534801483 \n",
            "Epoch: 359 | Loss: 0.003128956537693739 \n",
            "Epoch: 360 | Loss: 0.0030840029940009117 \n",
            "Epoch: 361 | Loss: 0.0030396762304008007 \n",
            "Epoch: 362 | Loss: 0.0029959948733448982 \n",
            "Epoch: 363 | Loss: 0.0029529358725994825 \n",
            "Epoch: 364 | Loss: 0.002910490380600095 \n",
            "Epoch: 365 | Loss: 0.0028686379082500935 \n",
            "Epoch: 366 | Loss: 0.0028274073265492916 \n",
            "Epoch: 367 | Loss: 0.002786803524941206 \n",
            "Epoch: 368 | Loss: 0.0027467403560876846 \n",
            "Epoch: 369 | Loss: 0.0027072394732385874 \n",
            "Epoch: 370 | Loss: 0.0026683551259338856 \n",
            "Epoch: 371 | Loss: 0.002630002563819289 \n",
            "Epoch: 372 | Loss: 0.0025922146160155535 \n",
            "Epoch: 373 | Loss: 0.002554964506998658 \n",
            "Epoch: 374 | Loss: 0.0025182440876960754 \n",
            "Epoch: 375 | Loss: 0.0024820498656481504 \n",
            "Epoch: 376 | Loss: 0.002446376485750079 \n",
            "Epoch: 377 | Loss: 0.002411238383501768 \n",
            "Epoch: 378 | Loss: 0.002376558491960168 \n",
            "Epoch: 379 | Loss: 0.0023424169048666954 \n",
            "Epoch: 380 | Loss: 0.0023087598383426666 \n",
            "Epoch: 381 | Loss: 0.0022755595855414867 \n",
            "Epoch: 382 | Loss: 0.00224286993034184 \n",
            "Epoch: 383 | Loss: 0.002210613340139389 \n",
            "Epoch: 384 | Loss: 0.002178867580369115 \n",
            "Epoch: 385 | Loss: 0.0021475614048540592 \n",
            "Epoch: 386 | Loss: 0.0021166878286749125 \n",
            "Epoch: 387 | Loss: 0.002086268737912178 \n",
            "Epoch: 388 | Loss: 0.0020562675781548023 \n",
            "Epoch: 389 | Loss: 0.0020267274230718613 \n",
            "Epoch: 390 | Loss: 0.00199760589748621 \n",
            "Epoch: 391 | Loss: 0.0019688913598656654 \n",
            "Epoch: 392 | Loss: 0.0019406125647947192 \n",
            "Epoch: 393 | Loss: 0.0019127114210277796 \n",
            "Epoch: 394 | Loss: 0.0018852216890081763 \n",
            "Epoch: 395 | Loss: 0.0018581366166472435 \n",
            "Epoch: 396 | Loss: 0.0018314140615984797 \n",
            "Epoch: 397 | Loss: 0.001805102569051087 \n",
            "Epoch: 398 | Loss: 0.0017791532445698977 \n",
            "Epoch: 399 | Loss: 0.001753605785779655 \n",
            "Epoch: 400 | Loss: 0.0017283689230680466 \n",
            "Epoch: 401 | Loss: 0.0017035451019182801 \n",
            "Epoch: 402 | Loss: 0.0016790488734841347 \n",
            "Epoch: 403 | Loss: 0.0016549380961805582 \n",
            "Epoch: 404 | Loss: 0.001631142687983811 \n",
            "Epoch: 405 | Loss: 0.001607717713341117 \n",
            "Epoch: 406 | Loss: 0.0015846078749746084 \n",
            "Epoch: 407 | Loss: 0.001561826327815652 \n",
            "Epoch: 408 | Loss: 0.0015393762150779366 \n",
            "Epoch: 409 | Loss: 0.0015172652201727033 \n",
            "Epoch: 410 | Loss: 0.001495460281148553 \n",
            "Epoch: 411 | Loss: 0.0014739473117515445 \n",
            "Epoch: 412 | Loss: 0.0014527852181345224 \n",
            "Epoch: 413 | Loss: 0.001431892509572208 \n",
            "Epoch: 414 | Loss: 0.001411321572959423 \n",
            "Epoch: 415 | Loss: 0.0013910513371229172 \n",
            "Epoch: 416 | Loss: 0.0013710461789742112 \n",
            "Epoch: 417 | Loss: 0.0013513440499082208 \n",
            "Epoch: 418 | Loss: 0.0013319309800863266 \n",
            "Epoch: 419 | Loss: 0.0013127995189279318 \n",
            "Epoch: 420 | Loss: 0.001293908804655075 \n",
            "Epoch: 421 | Loss: 0.0012753191404044628 \n",
            "Epoch: 422 | Loss: 0.0012569979298859835 \n",
            "Epoch: 423 | Loss: 0.0012389279436320066 \n",
            "Epoch: 424 | Loss: 0.0012211331631988287 \n",
            "Epoch: 425 | Loss: 0.0012035658583045006 \n",
            "Epoch: 426 | Loss: 0.00118628004565835 \n",
            "Epoch: 427 | Loss: 0.0011692186817526817 \n",
            "Epoch: 428 | Loss: 0.0011524211149662733 \n",
            "Epoch: 429 | Loss: 0.001135872327722609 \n",
            "Epoch: 430 | Loss: 0.0011195424012839794 \n",
            "Epoch: 431 | Loss: 0.0011034371564164758 \n",
            "Epoch: 432 | Loss: 0.0010875845327973366 \n",
            "Epoch: 433 | Loss: 0.0010719667188823223 \n",
            "Epoch: 434 | Loss: 0.001056558103300631 \n",
            "Epoch: 435 | Loss: 0.0010413887212052941 \n",
            "Epoch: 436 | Loss: 0.0010264146840199828 \n",
            "Epoch: 437 | Loss: 0.0010116545017808676 \n",
            "Epoch: 438 | Loss: 0.000997113180346787 \n",
            "Epoch: 439 | Loss: 0.0009827755857259035 \n",
            "Epoch: 440 | Loss: 0.0009686594712547958 \n",
            "Epoch: 441 | Loss: 0.0009547513909637928 \n",
            "Epoch: 442 | Loss: 0.000941029516980052 \n",
            "Epoch: 443 | Loss: 0.0009274955955334008 \n",
            "Epoch: 444 | Loss: 0.0009141686605289578 \n",
            "Epoch: 445 | Loss: 0.0009010254289023578 \n",
            "Epoch: 446 | Loss: 0.0008880691602826118 \n",
            "Epoch: 447 | Loss: 0.0008753219153732061 \n",
            "Epoch: 448 | Loss: 0.000862743821926415 \n",
            "Epoch: 449 | Loss: 0.0008503466378897429 \n",
            "Epoch: 450 | Loss: 0.0008381115039810538 \n",
            "Epoch: 451 | Loss: 0.0008260685135610402 \n",
            "Epoch: 452 | Loss: 0.0008142090518958867 \n",
            "Epoch: 453 | Loss: 0.0008024978451430798 \n",
            "Epoch: 454 | Loss: 0.0007909549167379737 \n",
            "Epoch: 455 | Loss: 0.0007795994752086699 \n",
            "Epoch: 456 | Loss: 0.0007683906587772071 \n",
            "Epoch: 457 | Loss: 0.0007573494222015142 \n",
            "Epoch: 458 | Loss: 0.0007464737864211202 \n",
            "Epoch: 459 | Loss: 0.0007357430877164006 \n",
            "Epoch: 460 | Loss: 0.0007251636707223952 \n",
            "Epoch: 461 | Loss: 0.0007147510186769068 \n",
            "Epoch: 462 | Loss: 0.0007044769590720534 \n",
            "Epoch: 463 | Loss: 0.000694340153131634 \n",
            "Epoch: 464 | Loss: 0.0006843694718554616 \n",
            "Epoch: 465 | Loss: 0.0006745373830199242 \n",
            "Epoch: 466 | Loss: 0.0006648259586654603 \n",
            "Epoch: 467 | Loss: 0.0006552801933139563 \n",
            "Epoch: 468 | Loss: 0.0006458713905885816 \n",
            "Epoch: 469 | Loss: 0.0006365863955579698 \n",
            "Epoch: 470 | Loss: 0.000627439992967993 \n",
            "Epoch: 471 | Loss: 0.0006184190860949457 \n",
            "Epoch: 472 | Loss: 0.0006095251883380115 \n",
            "Epoch: 473 | Loss: 0.0006007726769894361 \n",
            "Epoch: 474 | Loss: 0.0005921357660554349 \n",
            "Epoch: 475 | Loss: 0.0005836316850036383 \n",
            "Epoch: 476 | Loss: 0.0005752365104854107 \n",
            "Epoch: 477 | Loss: 0.0005669686361216009 \n",
            "Epoch: 478 | Loss: 0.0005588251515291631 \n",
            "Epoch: 479 | Loss: 0.0005507829482667148 \n",
            "Epoch: 480 | Loss: 0.0005428680451586843 \n",
            "Epoch: 481 | Loss: 0.0005350804422050714 \n",
            "Epoch: 482 | Loss: 0.0005273891147226095 \n",
            "Epoch: 483 | Loss: 0.0005197945865802467 \n",
            "Epoch: 484 | Loss: 0.0005123406299389899 \n",
            "Epoch: 485 | Loss: 0.0005049706087447703 \n",
            "Epoch: 486 | Loss: 0.0004977136850357056 \n",
            "Epoch: 487 | Loss: 0.000490555539727211 \n",
            "Epoch: 488 | Loss: 0.00048351965961046517 \n",
            "Epoch: 489 | Loss: 0.000476570101454854 \n",
            "Epoch: 490 | Loss: 0.00046971876872703433 \n",
            "Epoch: 491 | Loss: 0.00046296953223645687 \n",
            "Epoch: 492 | Loss: 0.000456311390735209 \n",
            "Epoch: 493 | Loss: 0.0004497545014601201 \n",
            "Epoch: 494 | Loss: 0.00044329033698886633 \n",
            "Epoch: 495 | Loss: 0.0004369204107206315 \n",
            "Epoch: 496 | Loss: 0.0004306399787310511 \n",
            "Epoch: 497 | Loss: 0.000424448138801381 \n",
            "Epoch: 498 | Loss: 0.0004183558630757034 \n",
            "Epoch: 499 | Loss: 0.00041232770308852196 \n",
            "Prediction (after training) 4 7.976657390594482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import tensor\n",
        "from torch import nn\n",
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Training data and ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data.\n",
        "        \"\"\"\n",
        "        y_pred = sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch {epoch + 1}/1000 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMztjLOWQftj",
        "outputId": "d4c8894b-4f9d-42ea-eea7-92d69c8c6270"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 | Loss: 0.8207\n",
            "Epoch 2/1000 | Loss: 0.8197\n",
            "Epoch 3/1000 | Loss: 0.8187\n",
            "Epoch 4/1000 | Loss: 0.8177\n",
            "Epoch 5/1000 | Loss: 0.8168\n",
            "Epoch 6/1000 | Loss: 0.8159\n",
            "Epoch 7/1000 | Loss: 0.8150\n",
            "Epoch 8/1000 | Loss: 0.8141\n",
            "Epoch 9/1000 | Loss: 0.8132\n",
            "Epoch 10/1000 | Loss: 0.8123\n",
            "Epoch 11/1000 | Loss: 0.8115\n",
            "Epoch 12/1000 | Loss: 0.8106\n",
            "Epoch 13/1000 | Loss: 0.8098\n",
            "Epoch 14/1000 | Loss: 0.8090\n",
            "Epoch 15/1000 | Loss: 0.8082\n",
            "Epoch 16/1000 | Loss: 0.8074\n",
            "Epoch 17/1000 | Loss: 0.8067\n",
            "Epoch 18/1000 | Loss: 0.8059\n",
            "Epoch 19/1000 | Loss: 0.8052\n",
            "Epoch 20/1000 | Loss: 0.8044\n",
            "Epoch 21/1000 | Loss: 0.8037\n",
            "Epoch 22/1000 | Loss: 0.8030\n",
            "Epoch 23/1000 | Loss: 0.8023\n",
            "Epoch 24/1000 | Loss: 0.8016\n",
            "Epoch 25/1000 | Loss: 0.8010\n",
            "Epoch 26/1000 | Loss: 0.8003\n",
            "Epoch 27/1000 | Loss: 0.7996\n",
            "Epoch 28/1000 | Loss: 0.7990\n",
            "Epoch 29/1000 | Loss: 0.7983\n",
            "Epoch 30/1000 | Loss: 0.7977\n",
            "Epoch 31/1000 | Loss: 0.7971\n",
            "Epoch 32/1000 | Loss: 0.7965\n",
            "Epoch 33/1000 | Loss: 0.7959\n",
            "Epoch 34/1000 | Loss: 0.7953\n",
            "Epoch 35/1000 | Loss: 0.7947\n",
            "Epoch 36/1000 | Loss: 0.7941\n",
            "Epoch 37/1000 | Loss: 0.7935\n",
            "Epoch 38/1000 | Loss: 0.7930\n",
            "Epoch 39/1000 | Loss: 0.7924\n",
            "Epoch 40/1000 | Loss: 0.7919\n",
            "Epoch 41/1000 | Loss: 0.7913\n",
            "Epoch 42/1000 | Loss: 0.7908\n",
            "Epoch 43/1000 | Loss: 0.7903\n",
            "Epoch 44/1000 | Loss: 0.7897\n",
            "Epoch 45/1000 | Loss: 0.7892\n",
            "Epoch 46/1000 | Loss: 0.7887\n",
            "Epoch 47/1000 | Loss: 0.7882\n",
            "Epoch 48/1000 | Loss: 0.7877\n",
            "Epoch 49/1000 | Loss: 0.7872\n",
            "Epoch 50/1000 | Loss: 0.7867\n",
            "Epoch 51/1000 | Loss: 0.7862\n",
            "Epoch 52/1000 | Loss: 0.7857\n",
            "Epoch 53/1000 | Loss: 0.7852\n",
            "Epoch 54/1000 | Loss: 0.7847\n",
            "Epoch 55/1000 | Loss: 0.7843\n",
            "Epoch 56/1000 | Loss: 0.7838\n",
            "Epoch 57/1000 | Loss: 0.7833\n",
            "Epoch 58/1000 | Loss: 0.7829\n",
            "Epoch 59/1000 | Loss: 0.7824\n",
            "Epoch 60/1000 | Loss: 0.7820\n",
            "Epoch 61/1000 | Loss: 0.7815\n",
            "Epoch 62/1000 | Loss: 0.7811\n",
            "Epoch 63/1000 | Loss: 0.7806\n",
            "Epoch 64/1000 | Loss: 0.7802\n",
            "Epoch 65/1000 | Loss: 0.7797\n",
            "Epoch 66/1000 | Loss: 0.7793\n",
            "Epoch 67/1000 | Loss: 0.7789\n",
            "Epoch 68/1000 | Loss: 0.7784\n",
            "Epoch 69/1000 | Loss: 0.7780\n",
            "Epoch 70/1000 | Loss: 0.7776\n",
            "Epoch 71/1000 | Loss: 0.7772\n",
            "Epoch 72/1000 | Loss: 0.7767\n",
            "Epoch 73/1000 | Loss: 0.7763\n",
            "Epoch 74/1000 | Loss: 0.7759\n",
            "Epoch 75/1000 | Loss: 0.7755\n",
            "Epoch 76/1000 | Loss: 0.7751\n",
            "Epoch 77/1000 | Loss: 0.7747\n",
            "Epoch 78/1000 | Loss: 0.7743\n",
            "Epoch 79/1000 | Loss: 0.7739\n",
            "Epoch 80/1000 | Loss: 0.7735\n",
            "Epoch 81/1000 | Loss: 0.7731\n",
            "Epoch 82/1000 | Loss: 0.7727\n",
            "Epoch 83/1000 | Loss: 0.7723\n",
            "Epoch 84/1000 | Loss: 0.7719\n",
            "Epoch 85/1000 | Loss: 0.7715\n",
            "Epoch 86/1000 | Loss: 0.7711\n",
            "Epoch 87/1000 | Loss: 0.7707\n",
            "Epoch 88/1000 | Loss: 0.7703\n",
            "Epoch 89/1000 | Loss: 0.7699\n",
            "Epoch 90/1000 | Loss: 0.7695\n",
            "Epoch 91/1000 | Loss: 0.7692\n",
            "Epoch 92/1000 | Loss: 0.7688\n",
            "Epoch 93/1000 | Loss: 0.7684\n",
            "Epoch 94/1000 | Loss: 0.7680\n",
            "Epoch 95/1000 | Loss: 0.7676\n",
            "Epoch 96/1000 | Loss: 0.7673\n",
            "Epoch 97/1000 | Loss: 0.7669\n",
            "Epoch 98/1000 | Loss: 0.7665\n",
            "Epoch 99/1000 | Loss: 0.7661\n",
            "Epoch 100/1000 | Loss: 0.7658\n",
            "Epoch 101/1000 | Loss: 0.7654\n",
            "Epoch 102/1000 | Loss: 0.7650\n",
            "Epoch 103/1000 | Loss: 0.7646\n",
            "Epoch 104/1000 | Loss: 0.7643\n",
            "Epoch 105/1000 | Loss: 0.7639\n",
            "Epoch 106/1000 | Loss: 0.7635\n",
            "Epoch 107/1000 | Loss: 0.7632\n",
            "Epoch 108/1000 | Loss: 0.7628\n",
            "Epoch 109/1000 | Loss: 0.7624\n",
            "Epoch 110/1000 | Loss: 0.7621\n",
            "Epoch 111/1000 | Loss: 0.7617\n",
            "Epoch 112/1000 | Loss: 0.7614\n",
            "Epoch 113/1000 | Loss: 0.7610\n",
            "Epoch 114/1000 | Loss: 0.7606\n",
            "Epoch 115/1000 | Loss: 0.7603\n",
            "Epoch 116/1000 | Loss: 0.7599\n",
            "Epoch 117/1000 | Loss: 0.7596\n",
            "Epoch 118/1000 | Loss: 0.7592\n",
            "Epoch 119/1000 | Loss: 0.7588\n",
            "Epoch 120/1000 | Loss: 0.7585\n",
            "Epoch 121/1000 | Loss: 0.7581\n",
            "Epoch 122/1000 | Loss: 0.7578\n",
            "Epoch 123/1000 | Loss: 0.7574\n",
            "Epoch 124/1000 | Loss: 0.7571\n",
            "Epoch 125/1000 | Loss: 0.7567\n",
            "Epoch 126/1000 | Loss: 0.7564\n",
            "Epoch 127/1000 | Loss: 0.7560\n",
            "Epoch 128/1000 | Loss: 0.7556\n",
            "Epoch 129/1000 | Loss: 0.7553\n",
            "Epoch 130/1000 | Loss: 0.7549\n",
            "Epoch 131/1000 | Loss: 0.7546\n",
            "Epoch 132/1000 | Loss: 0.7542\n",
            "Epoch 133/1000 | Loss: 0.7539\n",
            "Epoch 134/1000 | Loss: 0.7535\n",
            "Epoch 135/1000 | Loss: 0.7532\n",
            "Epoch 136/1000 | Loss: 0.7529\n",
            "Epoch 137/1000 | Loss: 0.7525\n",
            "Epoch 138/1000 | Loss: 0.7522\n",
            "Epoch 139/1000 | Loss: 0.7518\n",
            "Epoch 140/1000 | Loss: 0.7515\n",
            "Epoch 141/1000 | Loss: 0.7511\n",
            "Epoch 142/1000 | Loss: 0.7508\n",
            "Epoch 143/1000 | Loss: 0.7504\n",
            "Epoch 144/1000 | Loss: 0.7501\n",
            "Epoch 145/1000 | Loss: 0.7497\n",
            "Epoch 146/1000 | Loss: 0.7494\n",
            "Epoch 147/1000 | Loss: 0.7491\n",
            "Epoch 148/1000 | Loss: 0.7487\n",
            "Epoch 149/1000 | Loss: 0.7484\n",
            "Epoch 150/1000 | Loss: 0.7480\n",
            "Epoch 151/1000 | Loss: 0.7477\n",
            "Epoch 152/1000 | Loss: 0.7473\n",
            "Epoch 153/1000 | Loss: 0.7470\n",
            "Epoch 154/1000 | Loss: 0.7467\n",
            "Epoch 155/1000 | Loss: 0.7463\n",
            "Epoch 156/1000 | Loss: 0.7460\n",
            "Epoch 157/1000 | Loss: 0.7456\n",
            "Epoch 158/1000 | Loss: 0.7453\n",
            "Epoch 159/1000 | Loss: 0.7450\n",
            "Epoch 160/1000 | Loss: 0.7446\n",
            "Epoch 161/1000 | Loss: 0.7443\n",
            "Epoch 162/1000 | Loss: 0.7439\n",
            "Epoch 163/1000 | Loss: 0.7436\n",
            "Epoch 164/1000 | Loss: 0.7433\n",
            "Epoch 165/1000 | Loss: 0.7429\n",
            "Epoch 166/1000 | Loss: 0.7426\n",
            "Epoch 167/1000 | Loss: 0.7423\n",
            "Epoch 168/1000 | Loss: 0.7419\n",
            "Epoch 169/1000 | Loss: 0.7416\n",
            "Epoch 170/1000 | Loss: 0.7412\n",
            "Epoch 171/1000 | Loss: 0.7409\n",
            "Epoch 172/1000 | Loss: 0.7406\n",
            "Epoch 173/1000 | Loss: 0.7402\n",
            "Epoch 174/1000 | Loss: 0.7399\n",
            "Epoch 175/1000 | Loss: 0.7396\n",
            "Epoch 176/1000 | Loss: 0.7392\n",
            "Epoch 177/1000 | Loss: 0.7389\n",
            "Epoch 178/1000 | Loss: 0.7386\n",
            "Epoch 179/1000 | Loss: 0.7382\n",
            "Epoch 180/1000 | Loss: 0.7379\n",
            "Epoch 181/1000 | Loss: 0.7376\n",
            "Epoch 182/1000 | Loss: 0.7372\n",
            "Epoch 183/1000 | Loss: 0.7369\n",
            "Epoch 184/1000 | Loss: 0.7366\n",
            "Epoch 185/1000 | Loss: 0.7362\n",
            "Epoch 186/1000 | Loss: 0.7359\n",
            "Epoch 187/1000 | Loss: 0.7356\n",
            "Epoch 188/1000 | Loss: 0.7352\n",
            "Epoch 189/1000 | Loss: 0.7349\n",
            "Epoch 190/1000 | Loss: 0.7346\n",
            "Epoch 191/1000 | Loss: 0.7343\n",
            "Epoch 192/1000 | Loss: 0.7339\n",
            "Epoch 193/1000 | Loss: 0.7336\n",
            "Epoch 194/1000 | Loss: 0.7333\n",
            "Epoch 195/1000 | Loss: 0.7329\n",
            "Epoch 196/1000 | Loss: 0.7326\n",
            "Epoch 197/1000 | Loss: 0.7323\n",
            "Epoch 198/1000 | Loss: 0.7320\n",
            "Epoch 199/1000 | Loss: 0.7316\n",
            "Epoch 200/1000 | Loss: 0.7313\n",
            "Epoch 201/1000 | Loss: 0.7310\n",
            "Epoch 202/1000 | Loss: 0.7306\n",
            "Epoch 203/1000 | Loss: 0.7303\n",
            "Epoch 204/1000 | Loss: 0.7300\n",
            "Epoch 205/1000 | Loss: 0.7297\n",
            "Epoch 206/1000 | Loss: 0.7293\n",
            "Epoch 207/1000 | Loss: 0.7290\n",
            "Epoch 208/1000 | Loss: 0.7287\n",
            "Epoch 209/1000 | Loss: 0.7284\n",
            "Epoch 210/1000 | Loss: 0.7280\n",
            "Epoch 211/1000 | Loss: 0.7277\n",
            "Epoch 212/1000 | Loss: 0.7274\n",
            "Epoch 213/1000 | Loss: 0.7271\n",
            "Epoch 214/1000 | Loss: 0.7267\n",
            "Epoch 215/1000 | Loss: 0.7264\n",
            "Epoch 216/1000 | Loss: 0.7261\n",
            "Epoch 217/1000 | Loss: 0.7258\n",
            "Epoch 218/1000 | Loss: 0.7254\n",
            "Epoch 219/1000 | Loss: 0.7251\n",
            "Epoch 220/1000 | Loss: 0.7248\n",
            "Epoch 221/1000 | Loss: 0.7245\n",
            "Epoch 222/1000 | Loss: 0.7241\n",
            "Epoch 223/1000 | Loss: 0.7238\n",
            "Epoch 224/1000 | Loss: 0.7235\n",
            "Epoch 225/1000 | Loss: 0.7232\n",
            "Epoch 226/1000 | Loss: 0.7228\n",
            "Epoch 227/1000 | Loss: 0.7225\n",
            "Epoch 228/1000 | Loss: 0.7222\n",
            "Epoch 229/1000 | Loss: 0.7219\n",
            "Epoch 230/1000 | Loss: 0.7216\n",
            "Epoch 231/1000 | Loss: 0.7212\n",
            "Epoch 232/1000 | Loss: 0.7209\n",
            "Epoch 233/1000 | Loss: 0.7206\n",
            "Epoch 234/1000 | Loss: 0.7203\n",
            "Epoch 235/1000 | Loss: 0.7200\n",
            "Epoch 236/1000 | Loss: 0.7196\n",
            "Epoch 237/1000 | Loss: 0.7193\n",
            "Epoch 238/1000 | Loss: 0.7190\n",
            "Epoch 239/1000 | Loss: 0.7187\n",
            "Epoch 240/1000 | Loss: 0.7184\n",
            "Epoch 241/1000 | Loss: 0.7180\n",
            "Epoch 242/1000 | Loss: 0.7177\n",
            "Epoch 243/1000 | Loss: 0.7174\n",
            "Epoch 244/1000 | Loss: 0.7171\n",
            "Epoch 245/1000 | Loss: 0.7168\n",
            "Epoch 246/1000 | Loss: 0.7165\n",
            "Epoch 247/1000 | Loss: 0.7161\n",
            "Epoch 248/1000 | Loss: 0.7158\n",
            "Epoch 249/1000 | Loss: 0.7155\n",
            "Epoch 250/1000 | Loss: 0.7152\n",
            "Epoch 251/1000 | Loss: 0.7149\n",
            "Epoch 252/1000 | Loss: 0.7146\n",
            "Epoch 253/1000 | Loss: 0.7142\n",
            "Epoch 254/1000 | Loss: 0.7139\n",
            "Epoch 255/1000 | Loss: 0.7136\n",
            "Epoch 256/1000 | Loss: 0.7133\n",
            "Epoch 257/1000 | Loss: 0.7130\n",
            "Epoch 258/1000 | Loss: 0.7127\n",
            "Epoch 259/1000 | Loss: 0.7123\n",
            "Epoch 260/1000 | Loss: 0.7120\n",
            "Epoch 261/1000 | Loss: 0.7117\n",
            "Epoch 262/1000 | Loss: 0.7114\n",
            "Epoch 263/1000 | Loss: 0.7111\n",
            "Epoch 264/1000 | Loss: 0.7108\n",
            "Epoch 265/1000 | Loss: 0.7105\n",
            "Epoch 266/1000 | Loss: 0.7102\n",
            "Epoch 267/1000 | Loss: 0.7098\n",
            "Epoch 268/1000 | Loss: 0.7095\n",
            "Epoch 269/1000 | Loss: 0.7092\n",
            "Epoch 270/1000 | Loss: 0.7089\n",
            "Epoch 271/1000 | Loss: 0.7086\n",
            "Epoch 272/1000 | Loss: 0.7083\n",
            "Epoch 273/1000 | Loss: 0.7080\n",
            "Epoch 274/1000 | Loss: 0.7077\n",
            "Epoch 275/1000 | Loss: 0.7073\n",
            "Epoch 276/1000 | Loss: 0.7070\n",
            "Epoch 277/1000 | Loss: 0.7067\n",
            "Epoch 278/1000 | Loss: 0.7064\n",
            "Epoch 279/1000 | Loss: 0.7061\n",
            "Epoch 280/1000 | Loss: 0.7058\n",
            "Epoch 281/1000 | Loss: 0.7055\n",
            "Epoch 282/1000 | Loss: 0.7052\n",
            "Epoch 283/1000 | Loss: 0.7049\n",
            "Epoch 284/1000 | Loss: 0.7045\n",
            "Epoch 285/1000 | Loss: 0.7042\n",
            "Epoch 286/1000 | Loss: 0.7039\n",
            "Epoch 287/1000 | Loss: 0.7036\n",
            "Epoch 288/1000 | Loss: 0.7033\n",
            "Epoch 289/1000 | Loss: 0.7030\n",
            "Epoch 290/1000 | Loss: 0.7027\n",
            "Epoch 291/1000 | Loss: 0.7024\n",
            "Epoch 292/1000 | Loss: 0.7021\n",
            "Epoch 293/1000 | Loss: 0.7018\n",
            "Epoch 294/1000 | Loss: 0.7015\n",
            "Epoch 295/1000 | Loss: 0.7012\n",
            "Epoch 296/1000 | Loss: 0.7009\n",
            "Epoch 297/1000 | Loss: 0.7005\n",
            "Epoch 298/1000 | Loss: 0.7002\n",
            "Epoch 299/1000 | Loss: 0.6999\n",
            "Epoch 300/1000 | Loss: 0.6996\n",
            "Epoch 301/1000 | Loss: 0.6993\n",
            "Epoch 302/1000 | Loss: 0.6990\n",
            "Epoch 303/1000 | Loss: 0.6987\n",
            "Epoch 304/1000 | Loss: 0.6984\n",
            "Epoch 305/1000 | Loss: 0.6981\n",
            "Epoch 306/1000 | Loss: 0.6978\n",
            "Epoch 307/1000 | Loss: 0.6975\n",
            "Epoch 308/1000 | Loss: 0.6972\n",
            "Epoch 309/1000 | Loss: 0.6969\n",
            "Epoch 310/1000 | Loss: 0.6966\n",
            "Epoch 311/1000 | Loss: 0.6963\n",
            "Epoch 312/1000 | Loss: 0.6960\n",
            "Epoch 313/1000 | Loss: 0.6957\n",
            "Epoch 314/1000 | Loss: 0.6954\n",
            "Epoch 315/1000 | Loss: 0.6951\n",
            "Epoch 316/1000 | Loss: 0.6948\n",
            "Epoch 317/1000 | Loss: 0.6945\n",
            "Epoch 318/1000 | Loss: 0.6942\n",
            "Epoch 319/1000 | Loss: 0.6939\n",
            "Epoch 320/1000 | Loss: 0.6936\n",
            "Epoch 321/1000 | Loss: 0.6932\n",
            "Epoch 322/1000 | Loss: 0.6929\n",
            "Epoch 323/1000 | Loss: 0.6926\n",
            "Epoch 324/1000 | Loss: 0.6923\n",
            "Epoch 325/1000 | Loss: 0.6920\n",
            "Epoch 326/1000 | Loss: 0.6917\n",
            "Epoch 327/1000 | Loss: 0.6914\n",
            "Epoch 328/1000 | Loss: 0.6911\n",
            "Epoch 329/1000 | Loss: 0.6908\n",
            "Epoch 330/1000 | Loss: 0.6905\n",
            "Epoch 331/1000 | Loss: 0.6902\n",
            "Epoch 332/1000 | Loss: 0.6899\n",
            "Epoch 333/1000 | Loss: 0.6896\n",
            "Epoch 334/1000 | Loss: 0.6893\n",
            "Epoch 335/1000 | Loss: 0.6890\n",
            "Epoch 336/1000 | Loss: 0.6887\n",
            "Epoch 337/1000 | Loss: 0.6885\n",
            "Epoch 338/1000 | Loss: 0.6882\n",
            "Epoch 339/1000 | Loss: 0.6879\n",
            "Epoch 340/1000 | Loss: 0.6876\n",
            "Epoch 341/1000 | Loss: 0.6873\n",
            "Epoch 342/1000 | Loss: 0.6870\n",
            "Epoch 343/1000 | Loss: 0.6867\n",
            "Epoch 344/1000 | Loss: 0.6864\n",
            "Epoch 345/1000 | Loss: 0.6861\n",
            "Epoch 346/1000 | Loss: 0.6858\n",
            "Epoch 347/1000 | Loss: 0.6855\n",
            "Epoch 348/1000 | Loss: 0.6852\n",
            "Epoch 349/1000 | Loss: 0.6849\n",
            "Epoch 350/1000 | Loss: 0.6846\n",
            "Epoch 351/1000 | Loss: 0.6843\n",
            "Epoch 352/1000 | Loss: 0.6840\n",
            "Epoch 353/1000 | Loss: 0.6837\n",
            "Epoch 354/1000 | Loss: 0.6834\n",
            "Epoch 355/1000 | Loss: 0.6831\n",
            "Epoch 356/1000 | Loss: 0.6828\n",
            "Epoch 357/1000 | Loss: 0.6825\n",
            "Epoch 358/1000 | Loss: 0.6822\n",
            "Epoch 359/1000 | Loss: 0.6819\n",
            "Epoch 360/1000 | Loss: 0.6816\n",
            "Epoch 361/1000 | Loss: 0.6814\n",
            "Epoch 362/1000 | Loss: 0.6811\n",
            "Epoch 363/1000 | Loss: 0.6808\n",
            "Epoch 364/1000 | Loss: 0.6805\n",
            "Epoch 365/1000 | Loss: 0.6802\n",
            "Epoch 366/1000 | Loss: 0.6799\n",
            "Epoch 367/1000 | Loss: 0.6796\n",
            "Epoch 368/1000 | Loss: 0.6793\n",
            "Epoch 369/1000 | Loss: 0.6790\n",
            "Epoch 370/1000 | Loss: 0.6787\n",
            "Epoch 371/1000 | Loss: 0.6784\n",
            "Epoch 372/1000 | Loss: 0.6781\n",
            "Epoch 373/1000 | Loss: 0.6778\n",
            "Epoch 374/1000 | Loss: 0.6776\n",
            "Epoch 375/1000 | Loss: 0.6773\n",
            "Epoch 376/1000 | Loss: 0.6770\n",
            "Epoch 377/1000 | Loss: 0.6767\n",
            "Epoch 378/1000 | Loss: 0.6764\n",
            "Epoch 379/1000 | Loss: 0.6761\n",
            "Epoch 380/1000 | Loss: 0.6758\n",
            "Epoch 381/1000 | Loss: 0.6755\n",
            "Epoch 382/1000 | Loss: 0.6752\n",
            "Epoch 383/1000 | Loss: 0.6749\n",
            "Epoch 384/1000 | Loss: 0.6747\n",
            "Epoch 385/1000 | Loss: 0.6744\n",
            "Epoch 386/1000 | Loss: 0.6741\n",
            "Epoch 387/1000 | Loss: 0.6738\n",
            "Epoch 388/1000 | Loss: 0.6735\n",
            "Epoch 389/1000 | Loss: 0.6732\n",
            "Epoch 390/1000 | Loss: 0.6729\n",
            "Epoch 391/1000 | Loss: 0.6726\n",
            "Epoch 392/1000 | Loss: 0.6724\n",
            "Epoch 393/1000 | Loss: 0.6721\n",
            "Epoch 394/1000 | Loss: 0.6718\n",
            "Epoch 395/1000 | Loss: 0.6715\n",
            "Epoch 396/1000 | Loss: 0.6712\n",
            "Epoch 397/1000 | Loss: 0.6709\n",
            "Epoch 398/1000 | Loss: 0.6706\n",
            "Epoch 399/1000 | Loss: 0.6703\n",
            "Epoch 400/1000 | Loss: 0.6701\n",
            "Epoch 401/1000 | Loss: 0.6698\n",
            "Epoch 402/1000 | Loss: 0.6695\n",
            "Epoch 403/1000 | Loss: 0.6692\n",
            "Epoch 404/1000 | Loss: 0.6689\n",
            "Epoch 405/1000 | Loss: 0.6686\n",
            "Epoch 406/1000 | Loss: 0.6684\n",
            "Epoch 407/1000 | Loss: 0.6681\n",
            "Epoch 408/1000 | Loss: 0.6678\n",
            "Epoch 409/1000 | Loss: 0.6675\n",
            "Epoch 410/1000 | Loss: 0.6672\n",
            "Epoch 411/1000 | Loss: 0.6669\n",
            "Epoch 412/1000 | Loss: 0.6666\n",
            "Epoch 413/1000 | Loss: 0.6664\n",
            "Epoch 414/1000 | Loss: 0.6661\n",
            "Epoch 415/1000 | Loss: 0.6658\n",
            "Epoch 416/1000 | Loss: 0.6655\n",
            "Epoch 417/1000 | Loss: 0.6652\n",
            "Epoch 418/1000 | Loss: 0.6650\n",
            "Epoch 419/1000 | Loss: 0.6647\n",
            "Epoch 420/1000 | Loss: 0.6644\n",
            "Epoch 421/1000 | Loss: 0.6641\n",
            "Epoch 422/1000 | Loss: 0.6638\n",
            "Epoch 423/1000 | Loss: 0.6635\n",
            "Epoch 424/1000 | Loss: 0.6633\n",
            "Epoch 425/1000 | Loss: 0.6630\n",
            "Epoch 426/1000 | Loss: 0.6627\n",
            "Epoch 427/1000 | Loss: 0.6624\n",
            "Epoch 428/1000 | Loss: 0.6621\n",
            "Epoch 429/1000 | Loss: 0.6619\n",
            "Epoch 430/1000 | Loss: 0.6616\n",
            "Epoch 431/1000 | Loss: 0.6613\n",
            "Epoch 432/1000 | Loss: 0.6610\n",
            "Epoch 433/1000 | Loss: 0.6607\n",
            "Epoch 434/1000 | Loss: 0.6605\n",
            "Epoch 435/1000 | Loss: 0.6602\n",
            "Epoch 436/1000 | Loss: 0.6599\n",
            "Epoch 437/1000 | Loss: 0.6596\n",
            "Epoch 438/1000 | Loss: 0.6593\n",
            "Epoch 439/1000 | Loss: 0.6591\n",
            "Epoch 440/1000 | Loss: 0.6588\n",
            "Epoch 441/1000 | Loss: 0.6585\n",
            "Epoch 442/1000 | Loss: 0.6582\n",
            "Epoch 443/1000 | Loss: 0.6580\n",
            "Epoch 444/1000 | Loss: 0.6577\n",
            "Epoch 445/1000 | Loss: 0.6574\n",
            "Epoch 446/1000 | Loss: 0.6571\n",
            "Epoch 447/1000 | Loss: 0.6569\n",
            "Epoch 448/1000 | Loss: 0.6566\n",
            "Epoch 449/1000 | Loss: 0.6563\n",
            "Epoch 450/1000 | Loss: 0.6560\n",
            "Epoch 451/1000 | Loss: 0.6557\n",
            "Epoch 452/1000 | Loss: 0.6555\n",
            "Epoch 453/1000 | Loss: 0.6552\n",
            "Epoch 454/1000 | Loss: 0.6549\n",
            "Epoch 455/1000 | Loss: 0.6546\n",
            "Epoch 456/1000 | Loss: 0.6544\n",
            "Epoch 457/1000 | Loss: 0.6541\n",
            "Epoch 458/1000 | Loss: 0.6538\n",
            "Epoch 459/1000 | Loss: 0.6535\n",
            "Epoch 460/1000 | Loss: 0.6533\n",
            "Epoch 461/1000 | Loss: 0.6530\n",
            "Epoch 462/1000 | Loss: 0.6527\n",
            "Epoch 463/1000 | Loss: 0.6525\n",
            "Epoch 464/1000 | Loss: 0.6522\n",
            "Epoch 465/1000 | Loss: 0.6519\n",
            "Epoch 466/1000 | Loss: 0.6516\n",
            "Epoch 467/1000 | Loss: 0.6514\n",
            "Epoch 468/1000 | Loss: 0.6511\n",
            "Epoch 469/1000 | Loss: 0.6508\n",
            "Epoch 470/1000 | Loss: 0.6505\n",
            "Epoch 471/1000 | Loss: 0.6503\n",
            "Epoch 472/1000 | Loss: 0.6500\n",
            "Epoch 473/1000 | Loss: 0.6497\n",
            "Epoch 474/1000 | Loss: 0.6495\n",
            "Epoch 475/1000 | Loss: 0.6492\n",
            "Epoch 476/1000 | Loss: 0.6489\n",
            "Epoch 477/1000 | Loss: 0.6486\n",
            "Epoch 478/1000 | Loss: 0.6484\n",
            "Epoch 479/1000 | Loss: 0.6481\n",
            "Epoch 480/1000 | Loss: 0.6478\n",
            "Epoch 481/1000 | Loss: 0.6476\n",
            "Epoch 482/1000 | Loss: 0.6473\n",
            "Epoch 483/1000 | Loss: 0.6470\n",
            "Epoch 484/1000 | Loss: 0.6468\n",
            "Epoch 485/1000 | Loss: 0.6465\n",
            "Epoch 486/1000 | Loss: 0.6462\n",
            "Epoch 487/1000 | Loss: 0.6459\n",
            "Epoch 488/1000 | Loss: 0.6457\n",
            "Epoch 489/1000 | Loss: 0.6454\n",
            "Epoch 490/1000 | Loss: 0.6451\n",
            "Epoch 491/1000 | Loss: 0.6449\n",
            "Epoch 492/1000 | Loss: 0.6446\n",
            "Epoch 493/1000 | Loss: 0.6443\n",
            "Epoch 494/1000 | Loss: 0.6441\n",
            "Epoch 495/1000 | Loss: 0.6438\n",
            "Epoch 496/1000 | Loss: 0.6435\n",
            "Epoch 497/1000 | Loss: 0.6433\n",
            "Epoch 498/1000 | Loss: 0.6430\n",
            "Epoch 499/1000 | Loss: 0.6427\n",
            "Epoch 500/1000 | Loss: 0.6425\n",
            "Epoch 501/1000 | Loss: 0.6422\n",
            "Epoch 502/1000 | Loss: 0.6419\n",
            "Epoch 503/1000 | Loss: 0.6417\n",
            "Epoch 504/1000 | Loss: 0.6414\n",
            "Epoch 505/1000 | Loss: 0.6411\n",
            "Epoch 506/1000 | Loss: 0.6409\n",
            "Epoch 507/1000 | Loss: 0.6406\n",
            "Epoch 508/1000 | Loss: 0.6403\n",
            "Epoch 509/1000 | Loss: 0.6401\n",
            "Epoch 510/1000 | Loss: 0.6398\n",
            "Epoch 511/1000 | Loss: 0.6395\n",
            "Epoch 512/1000 | Loss: 0.6393\n",
            "Epoch 513/1000 | Loss: 0.6390\n",
            "Epoch 514/1000 | Loss: 0.6388\n",
            "Epoch 515/1000 | Loss: 0.6385\n",
            "Epoch 516/1000 | Loss: 0.6382\n",
            "Epoch 517/1000 | Loss: 0.6380\n",
            "Epoch 518/1000 | Loss: 0.6377\n",
            "Epoch 519/1000 | Loss: 0.6374\n",
            "Epoch 520/1000 | Loss: 0.6372\n",
            "Epoch 521/1000 | Loss: 0.6369\n",
            "Epoch 522/1000 | Loss: 0.6366\n",
            "Epoch 523/1000 | Loss: 0.6364\n",
            "Epoch 524/1000 | Loss: 0.6361\n",
            "Epoch 525/1000 | Loss: 0.6359\n",
            "Epoch 526/1000 | Loss: 0.6356\n",
            "Epoch 527/1000 | Loss: 0.6353\n",
            "Epoch 528/1000 | Loss: 0.6351\n",
            "Epoch 529/1000 | Loss: 0.6348\n",
            "Epoch 530/1000 | Loss: 0.6346\n",
            "Epoch 531/1000 | Loss: 0.6343\n",
            "Epoch 532/1000 | Loss: 0.6340\n",
            "Epoch 533/1000 | Loss: 0.6338\n",
            "Epoch 534/1000 | Loss: 0.6335\n",
            "Epoch 535/1000 | Loss: 0.6333\n",
            "Epoch 536/1000 | Loss: 0.6330\n",
            "Epoch 537/1000 | Loss: 0.6327\n",
            "Epoch 538/1000 | Loss: 0.6325\n",
            "Epoch 539/1000 | Loss: 0.6322\n",
            "Epoch 540/1000 | Loss: 0.6320\n",
            "Epoch 541/1000 | Loss: 0.6317\n",
            "Epoch 542/1000 | Loss: 0.6314\n",
            "Epoch 543/1000 | Loss: 0.6312\n",
            "Epoch 544/1000 | Loss: 0.6309\n",
            "Epoch 545/1000 | Loss: 0.6307\n",
            "Epoch 546/1000 | Loss: 0.6304\n",
            "Epoch 547/1000 | Loss: 0.6301\n",
            "Epoch 548/1000 | Loss: 0.6299\n",
            "Epoch 549/1000 | Loss: 0.6296\n",
            "Epoch 550/1000 | Loss: 0.6294\n",
            "Epoch 551/1000 | Loss: 0.6291\n",
            "Epoch 552/1000 | Loss: 0.6289\n",
            "Epoch 553/1000 | Loss: 0.6286\n",
            "Epoch 554/1000 | Loss: 0.6283\n",
            "Epoch 555/1000 | Loss: 0.6281\n",
            "Epoch 556/1000 | Loss: 0.6278\n",
            "Epoch 557/1000 | Loss: 0.6276\n",
            "Epoch 558/1000 | Loss: 0.6273\n",
            "Epoch 559/1000 | Loss: 0.6271\n",
            "Epoch 560/1000 | Loss: 0.6268\n",
            "Epoch 561/1000 | Loss: 0.6265\n",
            "Epoch 562/1000 | Loss: 0.6263\n",
            "Epoch 563/1000 | Loss: 0.6260\n",
            "Epoch 564/1000 | Loss: 0.6258\n",
            "Epoch 565/1000 | Loss: 0.6255\n",
            "Epoch 566/1000 | Loss: 0.6253\n",
            "Epoch 567/1000 | Loss: 0.6250\n",
            "Epoch 568/1000 | Loss: 0.6248\n",
            "Epoch 569/1000 | Loss: 0.6245\n",
            "Epoch 570/1000 | Loss: 0.6243\n",
            "Epoch 571/1000 | Loss: 0.6240\n",
            "Epoch 572/1000 | Loss: 0.6238\n",
            "Epoch 573/1000 | Loss: 0.6235\n",
            "Epoch 574/1000 | Loss: 0.6232\n",
            "Epoch 575/1000 | Loss: 0.6230\n",
            "Epoch 576/1000 | Loss: 0.6227\n",
            "Epoch 577/1000 | Loss: 0.6225\n",
            "Epoch 578/1000 | Loss: 0.6222\n",
            "Epoch 579/1000 | Loss: 0.6220\n",
            "Epoch 580/1000 | Loss: 0.6217\n",
            "Epoch 581/1000 | Loss: 0.6215\n",
            "Epoch 582/1000 | Loss: 0.6212\n",
            "Epoch 583/1000 | Loss: 0.6210\n",
            "Epoch 584/1000 | Loss: 0.6207\n",
            "Epoch 585/1000 | Loss: 0.6205\n",
            "Epoch 586/1000 | Loss: 0.6202\n",
            "Epoch 587/1000 | Loss: 0.6200\n",
            "Epoch 588/1000 | Loss: 0.6197\n",
            "Epoch 589/1000 | Loss: 0.6195\n",
            "Epoch 590/1000 | Loss: 0.6192\n",
            "Epoch 591/1000 | Loss: 0.6190\n",
            "Epoch 592/1000 | Loss: 0.6187\n",
            "Epoch 593/1000 | Loss: 0.6185\n",
            "Epoch 594/1000 | Loss: 0.6182\n",
            "Epoch 595/1000 | Loss: 0.6180\n",
            "Epoch 596/1000 | Loss: 0.6177\n",
            "Epoch 597/1000 | Loss: 0.6175\n",
            "Epoch 598/1000 | Loss: 0.6172\n",
            "Epoch 599/1000 | Loss: 0.6170\n",
            "Epoch 600/1000 | Loss: 0.6167\n",
            "Epoch 601/1000 | Loss: 0.6165\n",
            "Epoch 602/1000 | Loss: 0.6162\n",
            "Epoch 603/1000 | Loss: 0.6160\n",
            "Epoch 604/1000 | Loss: 0.6157\n",
            "Epoch 605/1000 | Loss: 0.6155\n",
            "Epoch 606/1000 | Loss: 0.6152\n",
            "Epoch 607/1000 | Loss: 0.6150\n",
            "Epoch 608/1000 | Loss: 0.6147\n",
            "Epoch 609/1000 | Loss: 0.6145\n",
            "Epoch 610/1000 | Loss: 0.6142\n",
            "Epoch 611/1000 | Loss: 0.6140\n",
            "Epoch 612/1000 | Loss: 0.6138\n",
            "Epoch 613/1000 | Loss: 0.6135\n",
            "Epoch 614/1000 | Loss: 0.6133\n",
            "Epoch 615/1000 | Loss: 0.6130\n",
            "Epoch 616/1000 | Loss: 0.6128\n",
            "Epoch 617/1000 | Loss: 0.6125\n",
            "Epoch 618/1000 | Loss: 0.6123\n",
            "Epoch 619/1000 | Loss: 0.6120\n",
            "Epoch 620/1000 | Loss: 0.6118\n",
            "Epoch 621/1000 | Loss: 0.6115\n",
            "Epoch 622/1000 | Loss: 0.6113\n",
            "Epoch 623/1000 | Loss: 0.6111\n",
            "Epoch 624/1000 | Loss: 0.6108\n",
            "Epoch 625/1000 | Loss: 0.6106\n",
            "Epoch 626/1000 | Loss: 0.6103\n",
            "Epoch 627/1000 | Loss: 0.6101\n",
            "Epoch 628/1000 | Loss: 0.6098\n",
            "Epoch 629/1000 | Loss: 0.6096\n",
            "Epoch 630/1000 | Loss: 0.6093\n",
            "Epoch 631/1000 | Loss: 0.6091\n",
            "Epoch 632/1000 | Loss: 0.6089\n",
            "Epoch 633/1000 | Loss: 0.6086\n",
            "Epoch 634/1000 | Loss: 0.6084\n",
            "Epoch 635/1000 | Loss: 0.6081\n",
            "Epoch 636/1000 | Loss: 0.6079\n",
            "Epoch 637/1000 | Loss: 0.6076\n",
            "Epoch 638/1000 | Loss: 0.6074\n",
            "Epoch 639/1000 | Loss: 0.6072\n",
            "Epoch 640/1000 | Loss: 0.6069\n",
            "Epoch 641/1000 | Loss: 0.6067\n",
            "Epoch 642/1000 | Loss: 0.6064\n",
            "Epoch 643/1000 | Loss: 0.6062\n",
            "Epoch 644/1000 | Loss: 0.6060\n",
            "Epoch 645/1000 | Loss: 0.6057\n",
            "Epoch 646/1000 | Loss: 0.6055\n",
            "Epoch 647/1000 | Loss: 0.6052\n",
            "Epoch 648/1000 | Loss: 0.6050\n",
            "Epoch 649/1000 | Loss: 0.6048\n",
            "Epoch 650/1000 | Loss: 0.6045\n",
            "Epoch 651/1000 | Loss: 0.6043\n",
            "Epoch 652/1000 | Loss: 0.6040\n",
            "Epoch 653/1000 | Loss: 0.6038\n",
            "Epoch 654/1000 | Loss: 0.6036\n",
            "Epoch 655/1000 | Loss: 0.6033\n",
            "Epoch 656/1000 | Loss: 0.6031\n",
            "Epoch 657/1000 | Loss: 0.6028\n",
            "Epoch 658/1000 | Loss: 0.6026\n",
            "Epoch 659/1000 | Loss: 0.6024\n",
            "Epoch 660/1000 | Loss: 0.6021\n",
            "Epoch 661/1000 | Loss: 0.6019\n",
            "Epoch 662/1000 | Loss: 0.6016\n",
            "Epoch 663/1000 | Loss: 0.6014\n",
            "Epoch 664/1000 | Loss: 0.6012\n",
            "Epoch 665/1000 | Loss: 0.6009\n",
            "Epoch 666/1000 | Loss: 0.6007\n",
            "Epoch 667/1000 | Loss: 0.6005\n",
            "Epoch 668/1000 | Loss: 0.6002\n",
            "Epoch 669/1000 | Loss: 0.6000\n",
            "Epoch 670/1000 | Loss: 0.5997\n",
            "Epoch 671/1000 | Loss: 0.5995\n",
            "Epoch 672/1000 | Loss: 0.5993\n",
            "Epoch 673/1000 | Loss: 0.5990\n",
            "Epoch 674/1000 | Loss: 0.5988\n",
            "Epoch 675/1000 | Loss: 0.5986\n",
            "Epoch 676/1000 | Loss: 0.5983\n",
            "Epoch 677/1000 | Loss: 0.5981\n",
            "Epoch 678/1000 | Loss: 0.5979\n",
            "Epoch 679/1000 | Loss: 0.5976\n",
            "Epoch 680/1000 | Loss: 0.5974\n",
            "Epoch 681/1000 | Loss: 0.5972\n",
            "Epoch 682/1000 | Loss: 0.5969\n",
            "Epoch 683/1000 | Loss: 0.5967\n",
            "Epoch 684/1000 | Loss: 0.5965\n",
            "Epoch 685/1000 | Loss: 0.5962\n",
            "Epoch 686/1000 | Loss: 0.5960\n",
            "Epoch 687/1000 | Loss: 0.5957\n",
            "Epoch 688/1000 | Loss: 0.5955\n",
            "Epoch 689/1000 | Loss: 0.5953\n",
            "Epoch 690/1000 | Loss: 0.5950\n",
            "Epoch 691/1000 | Loss: 0.5948\n",
            "Epoch 692/1000 | Loss: 0.5946\n",
            "Epoch 693/1000 | Loss: 0.5943\n",
            "Epoch 694/1000 | Loss: 0.5941\n",
            "Epoch 695/1000 | Loss: 0.5939\n",
            "Epoch 696/1000 | Loss: 0.5937\n",
            "Epoch 697/1000 | Loss: 0.5934\n",
            "Epoch 698/1000 | Loss: 0.5932\n",
            "Epoch 699/1000 | Loss: 0.5930\n",
            "Epoch 700/1000 | Loss: 0.5927\n",
            "Epoch 701/1000 | Loss: 0.5925\n",
            "Epoch 702/1000 | Loss: 0.5923\n",
            "Epoch 703/1000 | Loss: 0.5920\n",
            "Epoch 704/1000 | Loss: 0.5918\n",
            "Epoch 705/1000 | Loss: 0.5916\n",
            "Epoch 706/1000 | Loss: 0.5913\n",
            "Epoch 707/1000 | Loss: 0.5911\n",
            "Epoch 708/1000 | Loss: 0.5909\n",
            "Epoch 709/1000 | Loss: 0.5906\n",
            "Epoch 710/1000 | Loss: 0.5904\n",
            "Epoch 711/1000 | Loss: 0.5902\n",
            "Epoch 712/1000 | Loss: 0.5900\n",
            "Epoch 713/1000 | Loss: 0.5897\n",
            "Epoch 714/1000 | Loss: 0.5895\n",
            "Epoch 715/1000 | Loss: 0.5893\n",
            "Epoch 716/1000 | Loss: 0.5890\n",
            "Epoch 717/1000 | Loss: 0.5888\n",
            "Epoch 718/1000 | Loss: 0.5886\n",
            "Epoch 719/1000 | Loss: 0.5883\n",
            "Epoch 720/1000 | Loss: 0.5881\n",
            "Epoch 721/1000 | Loss: 0.5879\n",
            "Epoch 722/1000 | Loss: 0.5877\n",
            "Epoch 723/1000 | Loss: 0.5874\n",
            "Epoch 724/1000 | Loss: 0.5872\n",
            "Epoch 725/1000 | Loss: 0.5870\n",
            "Epoch 726/1000 | Loss: 0.5868\n",
            "Epoch 727/1000 | Loss: 0.5865\n",
            "Epoch 728/1000 | Loss: 0.5863\n",
            "Epoch 729/1000 | Loss: 0.5861\n",
            "Epoch 730/1000 | Loss: 0.5858\n",
            "Epoch 731/1000 | Loss: 0.5856\n",
            "Epoch 732/1000 | Loss: 0.5854\n",
            "Epoch 733/1000 | Loss: 0.5852\n",
            "Epoch 734/1000 | Loss: 0.5849\n",
            "Epoch 735/1000 | Loss: 0.5847\n",
            "Epoch 736/1000 | Loss: 0.5845\n",
            "Epoch 737/1000 | Loss: 0.5843\n",
            "Epoch 738/1000 | Loss: 0.5840\n",
            "Epoch 739/1000 | Loss: 0.5838\n",
            "Epoch 740/1000 | Loss: 0.5836\n",
            "Epoch 741/1000 | Loss: 0.5834\n",
            "Epoch 742/1000 | Loss: 0.5831\n",
            "Epoch 743/1000 | Loss: 0.5829\n",
            "Epoch 744/1000 | Loss: 0.5827\n",
            "Epoch 745/1000 | Loss: 0.5825\n",
            "Epoch 746/1000 | Loss: 0.5822\n",
            "Epoch 747/1000 | Loss: 0.5820\n",
            "Epoch 748/1000 | Loss: 0.5818\n",
            "Epoch 749/1000 | Loss: 0.5816\n",
            "Epoch 750/1000 | Loss: 0.5813\n",
            "Epoch 751/1000 | Loss: 0.5811\n",
            "Epoch 752/1000 | Loss: 0.5809\n",
            "Epoch 753/1000 | Loss: 0.5807\n",
            "Epoch 754/1000 | Loss: 0.5804\n",
            "Epoch 755/1000 | Loss: 0.5802\n",
            "Epoch 756/1000 | Loss: 0.5800\n",
            "Epoch 757/1000 | Loss: 0.5798\n",
            "Epoch 758/1000 | Loss: 0.5796\n",
            "Epoch 759/1000 | Loss: 0.5793\n",
            "Epoch 760/1000 | Loss: 0.5791\n",
            "Epoch 761/1000 | Loss: 0.5789\n",
            "Epoch 762/1000 | Loss: 0.5787\n",
            "Epoch 763/1000 | Loss: 0.5784\n",
            "Epoch 764/1000 | Loss: 0.5782\n",
            "Epoch 765/1000 | Loss: 0.5780\n",
            "Epoch 766/1000 | Loss: 0.5778\n",
            "Epoch 767/1000 | Loss: 0.5776\n",
            "Epoch 768/1000 | Loss: 0.5773\n",
            "Epoch 769/1000 | Loss: 0.5771\n",
            "Epoch 770/1000 | Loss: 0.5769\n",
            "Epoch 771/1000 | Loss: 0.5767\n",
            "Epoch 772/1000 | Loss: 0.5765\n",
            "Epoch 773/1000 | Loss: 0.5762\n",
            "Epoch 774/1000 | Loss: 0.5760\n",
            "Epoch 775/1000 | Loss: 0.5758\n",
            "Epoch 776/1000 | Loss: 0.5756\n",
            "Epoch 777/1000 | Loss: 0.5754\n",
            "Epoch 778/1000 | Loss: 0.5751\n",
            "Epoch 779/1000 | Loss: 0.5749\n",
            "Epoch 780/1000 | Loss: 0.5747\n",
            "Epoch 781/1000 | Loss: 0.5745\n",
            "Epoch 782/1000 | Loss: 0.5743\n",
            "Epoch 783/1000 | Loss: 0.5740\n",
            "Epoch 784/1000 | Loss: 0.5738\n",
            "Epoch 785/1000 | Loss: 0.5736\n",
            "Epoch 786/1000 | Loss: 0.5734\n",
            "Epoch 787/1000 | Loss: 0.5732\n",
            "Epoch 788/1000 | Loss: 0.5729\n",
            "Epoch 789/1000 | Loss: 0.5727\n",
            "Epoch 790/1000 | Loss: 0.5725\n",
            "Epoch 791/1000 | Loss: 0.5723\n",
            "Epoch 792/1000 | Loss: 0.5721\n",
            "Epoch 793/1000 | Loss: 0.5719\n",
            "Epoch 794/1000 | Loss: 0.5716\n",
            "Epoch 795/1000 | Loss: 0.5714\n",
            "Epoch 796/1000 | Loss: 0.5712\n",
            "Epoch 797/1000 | Loss: 0.5710\n",
            "Epoch 798/1000 | Loss: 0.5708\n",
            "Epoch 799/1000 | Loss: 0.5706\n",
            "Epoch 800/1000 | Loss: 0.5703\n",
            "Epoch 801/1000 | Loss: 0.5701\n",
            "Epoch 802/1000 | Loss: 0.5699\n",
            "Epoch 803/1000 | Loss: 0.5697\n",
            "Epoch 804/1000 | Loss: 0.5695\n",
            "Epoch 805/1000 | Loss: 0.5693\n",
            "Epoch 806/1000 | Loss: 0.5691\n",
            "Epoch 807/1000 | Loss: 0.5688\n",
            "Epoch 808/1000 | Loss: 0.5686\n",
            "Epoch 809/1000 | Loss: 0.5684\n",
            "Epoch 810/1000 | Loss: 0.5682\n",
            "Epoch 811/1000 | Loss: 0.5680\n",
            "Epoch 812/1000 | Loss: 0.5678\n",
            "Epoch 813/1000 | Loss: 0.5675\n",
            "Epoch 814/1000 | Loss: 0.5673\n",
            "Epoch 815/1000 | Loss: 0.5671\n",
            "Epoch 816/1000 | Loss: 0.5669\n",
            "Epoch 817/1000 | Loss: 0.5667\n",
            "Epoch 818/1000 | Loss: 0.5665\n",
            "Epoch 819/1000 | Loss: 0.5663\n",
            "Epoch 820/1000 | Loss: 0.5661\n",
            "Epoch 821/1000 | Loss: 0.5658\n",
            "Epoch 822/1000 | Loss: 0.5656\n",
            "Epoch 823/1000 | Loss: 0.5654\n",
            "Epoch 824/1000 | Loss: 0.5652\n",
            "Epoch 825/1000 | Loss: 0.5650\n",
            "Epoch 826/1000 | Loss: 0.5648\n",
            "Epoch 827/1000 | Loss: 0.5646\n",
            "Epoch 828/1000 | Loss: 0.5644\n",
            "Epoch 829/1000 | Loss: 0.5641\n",
            "Epoch 830/1000 | Loss: 0.5639\n",
            "Epoch 831/1000 | Loss: 0.5637\n",
            "Epoch 832/1000 | Loss: 0.5635\n",
            "Epoch 833/1000 | Loss: 0.5633\n",
            "Epoch 834/1000 | Loss: 0.5631\n",
            "Epoch 835/1000 | Loss: 0.5629\n",
            "Epoch 836/1000 | Loss: 0.5627\n",
            "Epoch 837/1000 | Loss: 0.5625\n",
            "Epoch 838/1000 | Loss: 0.5622\n",
            "Epoch 839/1000 | Loss: 0.5620\n",
            "Epoch 840/1000 | Loss: 0.5618\n",
            "Epoch 841/1000 | Loss: 0.5616\n",
            "Epoch 842/1000 | Loss: 0.5614\n",
            "Epoch 843/1000 | Loss: 0.5612\n",
            "Epoch 844/1000 | Loss: 0.5610\n",
            "Epoch 845/1000 | Loss: 0.5608\n",
            "Epoch 846/1000 | Loss: 0.5606\n",
            "Epoch 847/1000 | Loss: 0.5604\n",
            "Epoch 848/1000 | Loss: 0.5601\n",
            "Epoch 849/1000 | Loss: 0.5599\n",
            "Epoch 850/1000 | Loss: 0.5597\n",
            "Epoch 851/1000 | Loss: 0.5595\n",
            "Epoch 852/1000 | Loss: 0.5593\n",
            "Epoch 853/1000 | Loss: 0.5591\n",
            "Epoch 854/1000 | Loss: 0.5589\n",
            "Epoch 855/1000 | Loss: 0.5587\n",
            "Epoch 856/1000 | Loss: 0.5585\n",
            "Epoch 857/1000 | Loss: 0.5583\n",
            "Epoch 858/1000 | Loss: 0.5581\n",
            "Epoch 859/1000 | Loss: 0.5579\n",
            "Epoch 860/1000 | Loss: 0.5576\n",
            "Epoch 861/1000 | Loss: 0.5574\n",
            "Epoch 862/1000 | Loss: 0.5572\n",
            "Epoch 863/1000 | Loss: 0.5570\n",
            "Epoch 864/1000 | Loss: 0.5568\n",
            "Epoch 865/1000 | Loss: 0.5566\n",
            "Epoch 866/1000 | Loss: 0.5564\n",
            "Epoch 867/1000 | Loss: 0.5562\n",
            "Epoch 868/1000 | Loss: 0.5560\n",
            "Epoch 869/1000 | Loss: 0.5558\n",
            "Epoch 870/1000 | Loss: 0.5556\n",
            "Epoch 871/1000 | Loss: 0.5554\n",
            "Epoch 872/1000 | Loss: 0.5552\n",
            "Epoch 873/1000 | Loss: 0.5550\n",
            "Epoch 874/1000 | Loss: 0.5548\n",
            "Epoch 875/1000 | Loss: 0.5546\n",
            "Epoch 876/1000 | Loss: 0.5544\n",
            "Epoch 877/1000 | Loss: 0.5541\n",
            "Epoch 878/1000 | Loss: 0.5539\n",
            "Epoch 879/1000 | Loss: 0.5537\n",
            "Epoch 880/1000 | Loss: 0.5535\n",
            "Epoch 881/1000 | Loss: 0.5533\n",
            "Epoch 882/1000 | Loss: 0.5531\n",
            "Epoch 883/1000 | Loss: 0.5529\n",
            "Epoch 884/1000 | Loss: 0.5527\n",
            "Epoch 885/1000 | Loss: 0.5525\n",
            "Epoch 886/1000 | Loss: 0.5523\n",
            "Epoch 887/1000 | Loss: 0.5521\n",
            "Epoch 888/1000 | Loss: 0.5519\n",
            "Epoch 889/1000 | Loss: 0.5517\n",
            "Epoch 890/1000 | Loss: 0.5515\n",
            "Epoch 891/1000 | Loss: 0.5513\n",
            "Epoch 892/1000 | Loss: 0.5511\n",
            "Epoch 893/1000 | Loss: 0.5509\n",
            "Epoch 894/1000 | Loss: 0.5507\n",
            "Epoch 895/1000 | Loss: 0.5505\n",
            "Epoch 896/1000 | Loss: 0.5503\n",
            "Epoch 897/1000 | Loss: 0.5501\n",
            "Epoch 898/1000 | Loss: 0.5499\n",
            "Epoch 899/1000 | Loss: 0.5497\n",
            "Epoch 900/1000 | Loss: 0.5495\n",
            "Epoch 901/1000 | Loss: 0.5493\n",
            "Epoch 902/1000 | Loss: 0.5491\n",
            "Epoch 903/1000 | Loss: 0.5489\n",
            "Epoch 904/1000 | Loss: 0.5487\n",
            "Epoch 905/1000 | Loss: 0.5485\n",
            "Epoch 906/1000 | Loss: 0.5483\n",
            "Epoch 907/1000 | Loss: 0.5481\n",
            "Epoch 908/1000 | Loss: 0.5479\n",
            "Epoch 909/1000 | Loss: 0.5477\n",
            "Epoch 910/1000 | Loss: 0.5475\n",
            "Epoch 911/1000 | Loss: 0.5473\n",
            "Epoch 912/1000 | Loss: 0.5471\n",
            "Epoch 913/1000 | Loss: 0.5469\n",
            "Epoch 914/1000 | Loss: 0.5467\n",
            "Epoch 915/1000 | Loss: 0.5465\n",
            "Epoch 916/1000 | Loss: 0.5463\n",
            "Epoch 917/1000 | Loss: 0.5461\n",
            "Epoch 918/1000 | Loss: 0.5459\n",
            "Epoch 919/1000 | Loss: 0.5457\n",
            "Epoch 920/1000 | Loss: 0.5455\n",
            "Epoch 921/1000 | Loss: 0.5453\n",
            "Epoch 922/1000 | Loss: 0.5451\n",
            "Epoch 923/1000 | Loss: 0.5449\n",
            "Epoch 924/1000 | Loss: 0.5447\n",
            "Epoch 925/1000 | Loss: 0.5445\n",
            "Epoch 926/1000 | Loss: 0.5443\n",
            "Epoch 927/1000 | Loss: 0.5441\n",
            "Epoch 928/1000 | Loss: 0.5439\n",
            "Epoch 929/1000 | Loss: 0.5437\n",
            "Epoch 930/1000 | Loss: 0.5435\n",
            "Epoch 931/1000 | Loss: 0.5433\n",
            "Epoch 932/1000 | Loss: 0.5431\n",
            "Epoch 933/1000 | Loss: 0.5429\n",
            "Epoch 934/1000 | Loss: 0.5427\n",
            "Epoch 935/1000 | Loss: 0.5425\n",
            "Epoch 936/1000 | Loss: 0.5423\n",
            "Epoch 937/1000 | Loss: 0.5421\n",
            "Epoch 938/1000 | Loss: 0.5419\n",
            "Epoch 939/1000 | Loss: 0.5417\n",
            "Epoch 940/1000 | Loss: 0.5415\n",
            "Epoch 941/1000 | Loss: 0.5413\n",
            "Epoch 942/1000 | Loss: 0.5411\n",
            "Epoch 943/1000 | Loss: 0.5409\n",
            "Epoch 944/1000 | Loss: 0.5407\n",
            "Epoch 945/1000 | Loss: 0.5405\n",
            "Epoch 946/1000 | Loss: 0.5404\n",
            "Epoch 947/1000 | Loss: 0.5402\n",
            "Epoch 948/1000 | Loss: 0.5400\n",
            "Epoch 949/1000 | Loss: 0.5398\n",
            "Epoch 950/1000 | Loss: 0.5396\n",
            "Epoch 951/1000 | Loss: 0.5394\n",
            "Epoch 952/1000 | Loss: 0.5392\n",
            "Epoch 953/1000 | Loss: 0.5390\n",
            "Epoch 954/1000 | Loss: 0.5388\n",
            "Epoch 955/1000 | Loss: 0.5386\n",
            "Epoch 956/1000 | Loss: 0.5384\n",
            "Epoch 957/1000 | Loss: 0.5382\n",
            "Epoch 958/1000 | Loss: 0.5380\n",
            "Epoch 959/1000 | Loss: 0.5378\n",
            "Epoch 960/1000 | Loss: 0.5376\n",
            "Epoch 961/1000 | Loss: 0.5374\n",
            "Epoch 962/1000 | Loss: 0.5372\n",
            "Epoch 963/1000 | Loss: 0.5371\n",
            "Epoch 964/1000 | Loss: 0.5369\n",
            "Epoch 965/1000 | Loss: 0.5367\n",
            "Epoch 966/1000 | Loss: 0.5365\n",
            "Epoch 967/1000 | Loss: 0.5363\n",
            "Epoch 968/1000 | Loss: 0.5361\n",
            "Epoch 969/1000 | Loss: 0.5359\n",
            "Epoch 970/1000 | Loss: 0.5357\n",
            "Epoch 971/1000 | Loss: 0.5355\n",
            "Epoch 972/1000 | Loss: 0.5353\n",
            "Epoch 973/1000 | Loss: 0.5351\n",
            "Epoch 974/1000 | Loss: 0.5349\n",
            "Epoch 975/1000 | Loss: 0.5347\n",
            "Epoch 976/1000 | Loss: 0.5346\n",
            "Epoch 977/1000 | Loss: 0.5344\n",
            "Epoch 978/1000 | Loss: 0.5342\n",
            "Epoch 979/1000 | Loss: 0.5340\n",
            "Epoch 980/1000 | Loss: 0.5338\n",
            "Epoch 981/1000 | Loss: 0.5336\n",
            "Epoch 982/1000 | Loss: 0.5334\n",
            "Epoch 983/1000 | Loss: 0.5332\n",
            "Epoch 984/1000 | Loss: 0.5330\n",
            "Epoch 985/1000 | Loss: 0.5328\n",
            "Epoch 986/1000 | Loss: 0.5327\n",
            "Epoch 987/1000 | Loss: 0.5325\n",
            "Epoch 988/1000 | Loss: 0.5323\n",
            "Epoch 989/1000 | Loss: 0.5321\n",
            "Epoch 990/1000 | Loss: 0.5319\n",
            "Epoch 991/1000 | Loss: 0.5317\n",
            "Epoch 992/1000 | Loss: 0.5315\n",
            "Epoch 993/1000 | Loss: 0.5313\n",
            "Epoch 994/1000 | Loss: 0.5311\n",
            "Epoch 995/1000 | Loss: 0.5309\n",
            "Epoch 996/1000 | Loss: 0.5308\n",
            "Epoch 997/1000 | Loss: 0.5306\n",
            "Epoch 998/1000 | Loss: 0.5304\n",
            "Epoch 999/1000 | Loss: 0.5302\n",
            "Epoch 1000/1000 | Loss: 0.5300\n",
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 0.4657 | Above 50%: False\n",
            "Prediction after 7 hours of training: 0.9349 | Above 50%: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "9GKGDVK7RinD",
        "outputId": "2f31955b-997b-422d-ff04-1f2a23876900"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75be7c65-4a5e-4346-a2d1-066bc6d42ef2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75be7c65-4a5e-4346-a2d1-066bc6d42ef2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving diabetes.csv to diabetes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, from_numpy\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "x_data = from_numpy(xy[:, 0:-1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJhcZE39QvZz",
        "outputId": "8e3cc266-cd45-4969-e277-76e1db3843ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([759, 8]) | Y's shape: torch.Size([759, 1])\n",
            "Epoch: 1/100 | Loss: 0.6670\n",
            "Epoch: 2/100 | Loss: 0.6651\n",
            "Epoch: 3/100 | Loss: 0.6635\n",
            "Epoch: 4/100 | Loss: 0.6619\n",
            "Epoch: 5/100 | Loss: 0.6605\n",
            "Epoch: 6/100 | Loss: 0.6593\n",
            "Epoch: 7/100 | Loss: 0.6581\n",
            "Epoch: 8/100 | Loss: 0.6571\n",
            "Epoch: 9/100 | Loss: 0.6561\n",
            "Epoch: 10/100 | Loss: 0.6553\n",
            "Epoch: 11/100 | Loss: 0.6545\n",
            "Epoch: 12/100 | Loss: 0.6538\n",
            "Epoch: 13/100 | Loss: 0.6531\n",
            "Epoch: 14/100 | Loss: 0.6525\n",
            "Epoch: 15/100 | Loss: 0.6520\n",
            "Epoch: 16/100 | Loss: 0.6515\n",
            "Epoch: 17/100 | Loss: 0.6511\n",
            "Epoch: 18/100 | Loss: 0.6506\n",
            "Epoch: 19/100 | Loss: 0.6503\n",
            "Epoch: 20/100 | Loss: 0.6499\n",
            "Epoch: 21/100 | Loss: 0.6496\n",
            "Epoch: 22/100 | Loss: 0.6493\n",
            "Epoch: 23/100 | Loss: 0.6491\n",
            "Epoch: 24/100 | Loss: 0.6489\n",
            "Epoch: 25/100 | Loss: 0.6486\n",
            "Epoch: 26/100 | Loss: 0.6485\n",
            "Epoch: 27/100 | Loss: 0.6483\n",
            "Epoch: 28/100 | Loss: 0.6481\n",
            "Epoch: 29/100 | Loss: 0.6480\n",
            "Epoch: 30/100 | Loss: 0.6478\n",
            "Epoch: 31/100 | Loss: 0.6477\n",
            "Epoch: 32/100 | Loss: 0.6476\n",
            "Epoch: 33/100 | Loss: 0.6475\n",
            "Epoch: 34/100 | Loss: 0.6474\n",
            "Epoch: 35/100 | Loss: 0.6473\n",
            "Epoch: 36/100 | Loss: 0.6472\n",
            "Epoch: 37/100 | Loss: 0.6472\n",
            "Epoch: 38/100 | Loss: 0.6471\n",
            "Epoch: 39/100 | Loss: 0.6470\n",
            "Epoch: 40/100 | Loss: 0.6470\n",
            "Epoch: 41/100 | Loss: 0.6469\n",
            "Epoch: 42/100 | Loss: 0.6469\n",
            "Epoch: 43/100 | Loss: 0.6469\n",
            "Epoch: 44/100 | Loss: 0.6468\n",
            "Epoch: 45/100 | Loss: 0.6468\n",
            "Epoch: 46/100 | Loss: 0.6467\n",
            "Epoch: 47/100 | Loss: 0.6467\n",
            "Epoch: 48/100 | Loss: 0.6467\n",
            "Epoch: 49/100 | Loss: 0.6467\n",
            "Epoch: 50/100 | Loss: 0.6466\n",
            "Epoch: 51/100 | Loss: 0.6466\n",
            "Epoch: 52/100 | Loss: 0.6466\n",
            "Epoch: 53/100 | Loss: 0.6466\n",
            "Epoch: 54/100 | Loss: 0.6466\n",
            "Epoch: 55/100 | Loss: 0.6466\n",
            "Epoch: 56/100 | Loss: 0.6465\n",
            "Epoch: 57/100 | Loss: 0.6465\n",
            "Epoch: 58/100 | Loss: 0.6465\n",
            "Epoch: 59/100 | Loss: 0.6465\n",
            "Epoch: 60/100 | Loss: 0.6465\n",
            "Epoch: 61/100 | Loss: 0.6465\n",
            "Epoch: 62/100 | Loss: 0.6465\n",
            "Epoch: 63/100 | Loss: 0.6465\n",
            "Epoch: 64/100 | Loss: 0.6465\n",
            "Epoch: 65/100 | Loss: 0.6465\n",
            "Epoch: 66/100 | Loss: 0.6464\n",
            "Epoch: 67/100 | Loss: 0.6464\n",
            "Epoch: 68/100 | Loss: 0.6464\n",
            "Epoch: 69/100 | Loss: 0.6464\n",
            "Epoch: 70/100 | Loss: 0.6464\n",
            "Epoch: 71/100 | Loss: 0.6464\n",
            "Epoch: 72/100 | Loss: 0.6464\n",
            "Epoch: 73/100 | Loss: 0.6464\n",
            "Epoch: 74/100 | Loss: 0.6464\n",
            "Epoch: 75/100 | Loss: 0.6464\n",
            "Epoch: 76/100 | Loss: 0.6464\n",
            "Epoch: 77/100 | Loss: 0.6464\n",
            "Epoch: 78/100 | Loss: 0.6464\n",
            "Epoch: 79/100 | Loss: 0.6464\n",
            "Epoch: 80/100 | Loss: 0.6464\n",
            "Epoch: 81/100 | Loss: 0.6464\n",
            "Epoch: 82/100 | Loss: 0.6464\n",
            "Epoch: 83/100 | Loss: 0.6464\n",
            "Epoch: 84/100 | Loss: 0.6464\n",
            "Epoch: 85/100 | Loss: 0.6464\n",
            "Epoch: 86/100 | Loss: 0.6464\n",
            "Epoch: 87/100 | Loss: 0.6464\n",
            "Epoch: 88/100 | Loss: 0.6464\n",
            "Epoch: 89/100 | Loss: 0.6464\n",
            "Epoch: 90/100 | Loss: 0.6464\n",
            "Epoch: 91/100 | Loss: 0.6464\n",
            "Epoch: 92/100 | Loss: 0.6464\n",
            "Epoch: 93/100 | Loss: 0.6464\n",
            "Epoch: 94/100 | Loss: 0.6464\n",
            "Epoch: 95/100 | Loss: 0.6464\n",
            "Epoch: 96/100 | Loss: 0.6464\n",
            "Epoch: 97/100 | Loss: 0.6464\n",
            "Epoch: 98/100 | Loss: 0.6464\n",
            "Epoch: 99/100 | Loss: 0.6463\n",
            "Epoch: 100/100 | Loss: 0.6463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# References\n",
        "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
        "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import from_numpy, tensor\n",
        "import numpy as np\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('diabetes.csv',\n",
        "                        delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = tensor(inputs), tensor(labels)\n",
        "\n",
        "        # Run your training process\n",
        "        print(f'Epoch: {i} | Inputs {inputs.data} | Labels {labels.data}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyHsh6nOR8ZE",
        "outputId": "b7a47fb0-edf2-46ec-bfd4-186f1f224d45"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Inputs tensor([[-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
            "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
            "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
            "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
            "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
            "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
            "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n",
            "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
            "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
            "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
            "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
            "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
            "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
            "        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
            "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
            "        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
            "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
            "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n",
            "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
            "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
            "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
            "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
            "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
            "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
            "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
            "        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
            "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
            "        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
            "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n",
            "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
            "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 1 | Inputs tensor([[-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
            "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
            "        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
            "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n",
            "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
            "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
            "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
            "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
            "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
            "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n",
            "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
            "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
            "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
            "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
            "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
            "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n",
            "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
            "        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
            "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
            "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n",
            "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n",
            "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
            "        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
            "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
            "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
            "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
            "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
            "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n",
            "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
            "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n",
            "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 2 | Inputs tensor([[-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
            "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
            "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
            "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
            "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
            "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
            "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
            "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
            "        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
            "        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
            "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n",
            "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n",
            "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
            "        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
            "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
            "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
            "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
            "        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
            "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
            "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n",
            "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
            "        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
            "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
            "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
            "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
            "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n",
            "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
            "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
            "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
            "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 3 | Inputs tensor([[ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n",
            "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
            "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
            "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
            "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n",
            "        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
            "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
            "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
            "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
            "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
            "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
            "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
            "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
            "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
            "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
            "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
            "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n",
            "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
            "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
            "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
            "        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
            "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
            "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
            "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
            "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
            "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
            "        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
            "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
            "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n",
            "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n",
            "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
            "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 4 | Inputs tensor([[-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
            "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
            "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
            "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
            "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
            "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
            "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
            "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
            "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
            "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
            "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
            "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
            "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
            "        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
            "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
            "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
            "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
            "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
            "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
            "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
            "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
            "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
            "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
            "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
            "        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
            "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
            "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
            "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
            "        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
            "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
            "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
            "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 5 | Inputs tensor([[-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
            "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
            "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n",
            "        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
            "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
            "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
            "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
            "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
            "        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
            "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
            "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
            "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
            "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n",
            "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n",
            "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
            "        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
            "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
            "        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
            "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
            "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
            "        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
            "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
            "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
            "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
            "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
            "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
            "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
            "        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 6 | Inputs tensor([[-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
            "        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
            "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n",
            "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
            "        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
            "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n",
            "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
            "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
            "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
            "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
            "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n",
            "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
            "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n",
            "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
            "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
            "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
            "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
            "        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
            "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
            "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n",
            "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
            "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
            "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n",
            "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
            "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
            "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n",
            "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n",
            "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
            "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
            "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
            "        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
            "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 7 | Inputs tensor([[-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
            "        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
            "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
            "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
            "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n",
            "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
            "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
            "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
            "        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
            "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
            "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
            "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
            "        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
            "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
            "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
            "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
            "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
            "        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
            "        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
            "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n",
            "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
            "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
            "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
            "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
            "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
            "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
            "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
            "        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
            "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
            "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
            "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 8 | Inputs tensor([[-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
            "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
            "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
            "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
            "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
            "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
            "        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
            "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
            "        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
            "        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n",
            "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
            "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
            "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
            "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
            "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
            "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
            "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
            "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
            "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n",
            "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
            "        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
            "        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
            "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
            "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
            "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
            "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
            "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
            "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
            "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
            "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
            "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
            "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 9 | Inputs tensor([[ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
            "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
            "        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n",
            "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n",
            "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n",
            "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
            "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
            "        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
            "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n",
            "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
            "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
            "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
            "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
            "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
            "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
            "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
            "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n",
            "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
            "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
            "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
            "        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
            "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n",
            "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
            "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n",
            "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
            "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
            "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
            "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
            "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
            "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
            "        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 10 | Inputs tensor([[-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
            "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n",
            "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
            "        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
            "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
            "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
            "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
            "        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
            "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
            "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
            "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
            "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
            "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
            "        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
            "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
            "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
            "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
            "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
            "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
            "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
            "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
            "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n",
            "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
            "        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
            "        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
            "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
            "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
            "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
            "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
            "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
            "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 11 | Inputs tensor([[-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
            "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
            "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
            "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
            "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
            "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
            "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n",
            "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
            "        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
            "        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
            "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n",
            "        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
            "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
            "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
            "        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
            "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
            "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
            "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
            "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
            "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
            "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
            "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
            "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
            "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
            "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n",
            "        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
            "        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
            "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n",
            "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
            "        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
            "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
            "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 12 | Inputs tensor([[-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
            "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
            "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
            "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
            "        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
            "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
            "        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
            "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
            "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
            "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
            "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
            "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
            "        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
            "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
            "        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
            "        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n",
            "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n",
            "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
            "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
            "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
            "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
            "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
            "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
            "        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
            "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
            "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
            "        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
            "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
            "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
            "        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
            "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
            "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 13 | Inputs tensor([[ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
            "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
            "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
            "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
            "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n",
            "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
            "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
            "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
            "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n",
            "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
            "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
            "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
            "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
            "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
            "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
            "        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n",
            "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
            "        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
            "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
            "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
            "        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
            "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
            "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
            "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
            "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
            "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
            "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
            "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
            "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
            "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
            "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
            "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000]]) | Labels tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 14 | Inputs tensor([[-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n",
            "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
            "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
            "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
            "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n",
            "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
            "        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
            "        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
            "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
            "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
            "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n",
            "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
            "        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
            "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n",
            "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
            "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
            "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n",
            "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
            "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n",
            "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
            "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
            "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n",
            "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
            "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
            "        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
            "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
            "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
            "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
            "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
            "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
            "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
            "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 15 | Inputs tensor([[-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
            "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n",
            "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
            "        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
            "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
            "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
            "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
            "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
            "        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
            "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
            "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
            "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
            "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
            "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
            "        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
            "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n",
            "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
            "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n",
            "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
            "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
            "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
            "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
            "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
            "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
            "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n",
            "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
            "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
            "        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
            "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
            "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
            "        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
            "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 16 | Inputs tensor([[-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
            "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
            "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n",
            "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
            "        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
            "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n",
            "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
            "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
            "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n",
            "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n",
            "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
            "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n",
            "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
            "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
            "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
            "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n",
            "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
            "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
            "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
            "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
            "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
            "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
            "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
            "        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n",
            "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
            "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
            "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
            "        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
            "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
            "        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
            "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
            "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 17 | Inputs tensor([[ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
            "        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n",
            "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n",
            "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
            "        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n",
            "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
            "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
            "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
            "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
            "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
            "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
            "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
            "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n",
            "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n",
            "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
            "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
            "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
            "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
            "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
            "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
            "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
            "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n",
            "        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
            "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
            "        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
            "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n",
            "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
            "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
            "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n",
            "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 18 | Inputs tensor([[-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
            "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
            "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
            "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
            "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
            "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
            "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
            "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
            "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n",
            "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
            "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n",
            "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
            "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
            "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
            "        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
            "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
            "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
            "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n",
            "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n",
            "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
            "        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
            "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
            "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
            "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n",
            "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
            "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n",
            "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
            "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
            "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
            "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
            "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 19 | Inputs tensor([[-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
            "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
            "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
            "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
            "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
            "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n",
            "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
            "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
            "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
            "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
            "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
            "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
            "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
            "        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n",
            "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
            "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
            "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n",
            "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
            "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n",
            "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n",
            "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
            "        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
            "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
            "        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
            "        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
            "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n",
            "        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
            "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
            "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
            "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
            "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
            "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 20 | Inputs tensor([[-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
            "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
            "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
            "        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
            "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
            "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
            "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
            "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n",
            "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
            "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
            "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
            "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
            "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
            "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
            "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
            "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n",
            "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
            "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
            "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
            "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
            "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n",
            "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
            "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667],\n",
            "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
            "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
            "        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
            "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
            "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n",
            "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
            "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 21 | Inputs tensor([[-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
            "        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
            "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
            "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n",
            "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
            "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
            "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n",
            "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n",
            "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
            "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
            "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
            "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
            "        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
            "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
            "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
            "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
            "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
            "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n",
            "        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
            "        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
            "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
            "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
            "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
            "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
            "        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
            "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
            "        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n",
            "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
            "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
            "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
            "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
            "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667]]) | Labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 22 | Inputs tensor([[-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
            "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
            "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
            "        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
            "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
            "        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
            "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
            "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
            "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
            "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
            "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
            "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
            "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
            "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n",
            "        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
            "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
            "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
            "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
            "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
            "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
            "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
            "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
            "        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
            "        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
            "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
            "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
            "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
            "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
            "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n",
            "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
            "        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 23 | Inputs tensor([[ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
            "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
            "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
            "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
            "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
            "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
            "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
            "        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
            "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
            "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
            "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
            "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
            "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
            "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
            "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
            "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
            "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
            "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
            "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
            "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
            "        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
            "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c7653b390bd0>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs, labels = tensor(inputs), tensor(labels)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Inputs tensor([[ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
            "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
            "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
            "        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n",
            "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
            "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
            "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
            "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
            "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
            "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
            "        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n",
            "        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
            "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667],\n",
            "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
            "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
            "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
            "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
            "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
            "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
            "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
            "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
            "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
            "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
            "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
            "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
            "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
            "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
            "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n",
            "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n",
            "        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n",
            "        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
            "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 1 | Inputs tensor([[-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
            "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
            "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n",
            "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
            "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
            "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
            "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
            "        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
            "        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
            "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
            "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
            "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
            "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
            "        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
            "        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
            "        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
            "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
            "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
            "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
            "        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
            "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
            "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
            "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
            "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
            "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
            "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
            "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
            "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
            "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
            "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
            "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 2 | Inputs tensor([[-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
            "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
            "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
            "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
            "        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
            "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
            "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
            "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
            "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
            "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n",
            "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
            "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
            "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
            "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n",
            "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n",
            "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
            "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n",
            "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
            "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
            "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
            "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
            "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n",
            "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
            "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
            "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
            "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
            "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
            "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
            "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
            "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
            "        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 3 | Inputs tensor([[-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
            "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
            "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n",
            "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
            "        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
            "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
            "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n",
            "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
            "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
            "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
            "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
            "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
            "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
            "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
            "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
            "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n",
            "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
            "        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
            "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
            "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
            "        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
            "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
            "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n",
            "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
            "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n",
            "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n",
            "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
            "        [-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
            "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
            "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
            "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 4 | Inputs tensor([[ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
            "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
            "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
            "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n",
            "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
            "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
            "        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
            "        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
            "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
            "        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
            "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
            "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n",
            "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
            "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
            "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
            "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
            "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
            "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
            "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n",
            "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
            "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
            "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
            "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
            "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
            "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
            "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n",
            "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
            "        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
            "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
            "        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
            "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000]]) | Labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 5 | Inputs tensor([[ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
            "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
            "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
            "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
            "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n",
            "        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
            "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n",
            "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
            "        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
            "        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
            "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
            "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
            "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
            "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
            "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
            "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
            "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n",
            "        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
            "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
            "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
            "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
            "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
            "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n",
            "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
            "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
            "        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n",
            "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
            "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
            "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
            "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 6 | Inputs tensor([[-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
            "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
            "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n",
            "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n",
            "        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
            "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n",
            "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
            "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
            "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
            "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
            "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
            "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n",
            "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
            "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
            "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n",
            "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
            "        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
            "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
            "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
            "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
            "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
            "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
            "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
            "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
            "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
            "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
            "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
            "        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
            "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
            "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
            "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
            "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 7 | Inputs tensor([[-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
            "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
            "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
            "        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n",
            "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
            "        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
            "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
            "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
            "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
            "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
            "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
            "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
            "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
            "        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
            "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
            "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n",
            "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
            "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
            "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n",
            "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
            "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
            "        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
            "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
            "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
            "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
            "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
            "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n",
            "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
            "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
            "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
            "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
            "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 8 | Inputs tensor([[-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
            "        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
            "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
            "        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
            "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n",
            "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
            "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
            "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
            "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
            "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n",
            "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
            "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n",
            "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
            "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
            "        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
            "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
            "        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
            "        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
            "        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
            "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
            "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
            "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
            "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
            "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
            "        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
            "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
            "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
            "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
            "        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
            "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
            "        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
            "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 9 | Inputs tensor([[-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
            "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
            "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
            "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
            "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
            "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n",
            "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
            "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n",
            "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
            "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
            "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
            "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
            "        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
            "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
            "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
            "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
            "        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
            "        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
            "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
            "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
            "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
            "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n",
            "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
            "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
            "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
            "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
            "        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
            "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n",
            "        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
            "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
            "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 10 | Inputs tensor([[-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
            "        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
            "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
            "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
            "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
            "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
            "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
            "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
            "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
            "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
            "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
            "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
            "        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
            "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n",
            "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
            "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n",
            "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
            "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n",
            "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
            "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
            "        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
            "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
            "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
            "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
            "        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
            "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
            "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
            "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
            "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
            "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
            "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 11 | Inputs tensor([[-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
            "        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
            "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
            "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
            "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
            "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
            "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
            "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
            "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
            "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
            "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
            "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
            "        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n",
            "        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n",
            "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
            "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
            "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
            "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
            "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
            "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
            "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
            "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
            "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
            "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
            "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
            "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
            "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n",
            "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n",
            "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
            "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 12 | Inputs tensor([[ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
            "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
            "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
            "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
            "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
            "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
            "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
            "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n",
            "        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n",
            "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
            "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
            "        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
            "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
            "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
            "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
            "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
            "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n",
            "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
            "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
            "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n",
            "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
            "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
            "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
            "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n",
            "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
            "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
            "        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
            "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
            "        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
            "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
            "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 13 | Inputs tensor([[-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
            "        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
            "        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n",
            "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
            "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
            "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
            "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n",
            "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
            "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
            "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
            "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
            "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
            "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
            "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n",
            "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
            "        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
            "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
            "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n",
            "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
            "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
            "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
            "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
            "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
            "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
            "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
            "        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
            "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
            "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
            "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667],\n",
            "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
            "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
            "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000]]) | Labels tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 14 | Inputs tensor([[-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n",
            "        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
            "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
            "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
            "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
            "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
            "        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
            "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
            "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
            "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
            "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
            "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n",
            "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
            "        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
            "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
            "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
            "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
            "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n",
            "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n",
            "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
            "        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
            "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n",
            "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
            "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
            "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
            "        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
            "        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
            "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
            "        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n",
            "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
            "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 15 | Inputs tensor([[-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
            "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
            "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
            "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
            "        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
            "        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
            "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n",
            "        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
            "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
            "        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
            "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
            "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n",
            "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
            "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
            "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
            "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
            "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
            "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
            "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
            "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
            "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
            "        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
            "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
            "        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
            "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
            "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
            "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
            "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
            "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
            "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n",
            "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
            "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 16 | Inputs tensor([[-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
            "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
            "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
            "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n",
            "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
            "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
            "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
            "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
            "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
            "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
            "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
            "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n",
            "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
            "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
            "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
            "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
            "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
            "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
            "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n",
            "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
            "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
            "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
            "        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
            "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
            "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
            "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
            "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
            "        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
            "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
            "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
            "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 17 | Inputs tensor([[ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n",
            "        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
            "        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
            "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
            "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
            "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
            "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
            "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
            "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
            "        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
            "        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
            "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
            "        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
            "        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
            "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n",
            "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
            "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
            "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
            "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
            "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
            "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
            "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
            "        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
            "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n",
            "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n",
            "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
            "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n",
            "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n",
            "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
            "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n",
            "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
            "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "Epoch: 18 | Inputs tensor([[-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
            "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
            "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n",
            "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
            "        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
            "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
            "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
            "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
            "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
            "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
            "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
            "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
            "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
            "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
            "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
            "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
            "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n",
            "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
            "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n",
            "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
            "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
            "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n",
            "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
            "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
            "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
            "        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
            "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
            "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
            "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
            "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
            "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
            "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667]]) | Labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 19 | Inputs tensor([[-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n",
            "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
            "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
            "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
            "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
            "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
            "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
            "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
            "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
            "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
            "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
            "        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
            "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
            "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
            "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
            "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
            "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
            "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
            "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
            "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
            "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
            "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n",
            "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
            "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
            "        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
            "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
            "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n",
            "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
            "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
            "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
            "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
            "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 20 | Inputs tensor([[-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
            "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
            "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
            "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n",
            "        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
            "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
            "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
            "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
            "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n",
            "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
            "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
            "        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
            "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
            "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
            "        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
            "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
            "        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
            "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
            "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n",
            "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
            "        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
            "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
            "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
            "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
            "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
            "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n",
            "        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
            "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
            "        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
            "        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
            "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
            "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "Epoch: 21 | Inputs tensor([[-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
            "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
            "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n",
            "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
            "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
            "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
            "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
            "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n",
            "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
            "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
            "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
            "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
            "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
            "        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
            "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
            "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
            "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
            "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000],\n",
            "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
            "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
            "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
            "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
            "        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
            "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
            "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n",
            "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n",
            "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n",
            "        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
            "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
            "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
            "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "Epoch: 22 | Inputs tensor([[-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
            "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
            "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
            "        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
            "        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
            "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n",
            "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
            "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
            "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
            "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n",
            "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
            "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
            "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
            "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
            "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
            "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n",
            "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
            "        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
            "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n",
            "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n",
            "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
            "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
            "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
            "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
            "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
            "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
            "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
            "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
            "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
            "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333]]) | Labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "Epoch: 23 | Inputs tensor([[-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
            "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
            "        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
            "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
            "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
            "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
            "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n",
            "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
            "        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
            "        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
            "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
            "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
            "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n",
            "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n",
            "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
            "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
            "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
            "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
            "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
            "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
            "        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
            "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333]]) | Labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# References\n",
        "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py\n",
        "# http://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, from_numpy, optim\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('diabetes.csv',\n",
        "                        delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        y_pred = model(inputs)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(y_pred, labels)\n",
        "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhmapRz3SLIp",
        "outputId": "25f1c7f1-e94d-4ce8-cf1e-32ee057be11d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Batch: 1 | Loss: 24.9803\n",
            "Epoch 1 | Batch: 2 | Loss: 20.7867\n",
            "Epoch 1 | Batch: 3 | Loss: 23.4250\n",
            "Epoch 1 | Batch: 4 | Loss: 22.7326\n",
            "Epoch 1 | Batch: 5 | Loss: 21.9377\n",
            "Epoch 1 | Batch: 6 | Loss: 21.4887\n",
            "Epoch 1 | Batch: 7 | Loss: 17.3808\n",
            "Epoch 1 | Batch: 8 | Loss: 20.2495\n",
            "Epoch 1 | Batch: 9 | Loss: 18.9968\n",
            "Epoch 1 | Batch: 10 | Loss: 21.8378\n",
            "Epoch 1 | Batch: 11 | Loss: 21.9778\n",
            "Epoch 1 | Batch: 12 | Loss: 20.9456\n",
            "Epoch 1 | Batch: 13 | Loss: 20.3834\n",
            "Epoch 1 | Batch: 14 | Loss: 23.4964\n",
            "Epoch 1 | Batch: 15 | Loss: 23.0116\n",
            "Epoch 1 | Batch: 16 | Loss: 21.9683\n",
            "Epoch 1 | Batch: 17 | Loss: 20.6448\n",
            "Epoch 1 | Batch: 18 | Loss: 21.1961\n",
            "Epoch 1 | Batch: 19 | Loss: 22.1262\n",
            "Epoch 1 | Batch: 20 | Loss: 20.8266\n",
            "Epoch 1 | Batch: 21 | Loss: 18.0716\n",
            "Epoch 1 | Batch: 22 | Loss: 23.4846\n",
            "Epoch 1 | Batch: 23 | Loss: 20.8158\n",
            "Epoch 1 | Batch: 24 | Loss: 12.3609\n",
            "Epoch 2 | Batch: 1 | Loss: 22.6472\n",
            "Epoch 2 | Batch: 2 | Loss: 21.1799\n",
            "Epoch 2 | Batch: 3 | Loss: 19.5843\n",
            "Epoch 2 | Batch: 4 | Loss: 19.9843\n",
            "Epoch 2 | Batch: 5 | Loss: 23.7373\n",
            "Epoch 2 | Batch: 6 | Loss: 22.1696\n",
            "Epoch 2 | Batch: 7 | Loss: 23.0061\n",
            "Epoch 2 | Batch: 8 | Loss: 19.6979\n",
            "Epoch 2 | Batch: 9 | Loss: 20.9300\n",
            "Epoch 2 | Batch: 10 | Loss: 20.5683\n",
            "Epoch 2 | Batch: 11 | Loss: 21.8645\n",
            "Epoch 2 | Batch: 12 | Loss: 20.5671\n",
            "Epoch 2 | Batch: 13 | Loss: 21.5462\n",
            "Epoch 2 | Batch: 14 | Loss: 20.6521\n",
            "Epoch 2 | Batch: 15 | Loss: 19.2387\n",
            "Epoch 2 | Batch: 16 | Loss: 19.9400\n",
            "Epoch 2 | Batch: 17 | Loss: 20.6407\n",
            "Epoch 2 | Batch: 18 | Loss: 18.6626\n",
            "Epoch 2 | Batch: 19 | Loss: 21.2997\n",
            "Epoch 2 | Batch: 20 | Loss: 20.5493\n",
            "Epoch 2 | Batch: 21 | Loss: 22.5182\n",
            "Epoch 2 | Batch: 22 | Loss: 21.7085\n",
            "Epoch 2 | Batch: 23 | Loss: 21.1814\n",
            "Epoch 2 | Batch: 24 | Loss: 15.3941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, tensor, max\n",
        "import numpy as np\n",
        "\n",
        "# Cross entropy example\n",
        "# One hot\n",
        "# 0: 1 0 0\n",
        "# 1: 0 1 0\n",
        "# 2: 0 0 1\n",
        "Y = np.array([1, 0, 0])\n",
        "Y_pred1 = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred2 = np.array([0.1, 0.3, 0.6])\n",
        "print(f'Loss1: {np.sum(-Y * np.log(Y_pred1)):.4f}')\n",
        "print(f'Loss2: {np.sum(-Y * np.log(Y_pred2)):.4f}')\n",
        "\n",
        "# Softmax + CrossEntropy (logSoftmax + NLLLoss)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "\n",
        "# target is of size nBatch\n",
        "# each element in target has to have 0 <= value < nClasses (0-2)\n",
        "# Input is class, not one-hot\n",
        "Y = tensor([0], requires_grad=False)\n",
        "\n",
        "# input is of size nBatch x nClasses = 1 x 4\n",
        "# Y_pred are logits (not softmax)\n",
        "Y_pred1 = tensor([[2.0, 1.0, 0.1]])\n",
        "Y_pred2 = tensor([[0.5, 2.0, 0.3]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "\n",
        "print(f'PyTorch Loss1: {l1.item():.4f} \\nPyTorch Loss2: {l2.item():.4f}')\n",
        "print(f'Y_pred1: {max(Y_pred1.data, 1)[1].item()}')\n",
        "print(f'Y_pred2: {max(Y_pred2.data, 1)[1].item()}')\n",
        "\n",
        "# target is of size nBatch\n",
        "# each element in target has to have 0 <= value < nClasses (0-2)\n",
        "# Input is class, not one-hot\n",
        "Y = tensor([2, 0, 1], requires_grad=False)\n",
        "\n",
        "# input is of size nBatch x nClasses = 2 x 4\n",
        "# Y_pred are logits (not softmax)\n",
        "Y_pred1 = tensor([[0.1, 0.2, 0.9],\n",
        "                  [1.1, 0.1, 0.2],\n",
        "                  [0.2, 2.1, 0.1]])\n",
        "\n",
        "Y_pred2 = tensor([[0.8, 0.2, 0.3],\n",
        "                  [0.2, 0.3, 0.5],\n",
        "                  [0.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss(Y_pred1, Y)\n",
        "l2 = loss(Y_pred2, Y)\n",
        "print(f'Batch Loss1:  {l1.item():.4f} \\nBatch Loss2: {l2.data:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCHvv6rzSRv0",
        "outputId": "7b303168-4196-4a16-befa-f7d03dd8433c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss1: 0.3567\n",
            "Loss2: 2.3026\n",
            "PyTorch Loss1: 0.4170 \n",
            "PyTorch Loss2: 1.8406\n",
            "Y_pred1: 0\n",
            "Y_pred2: 1\n",
            "Batch Loss1:  0.4966 \n",
            "Batch Loss2: 1.2389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)\n",
        "        self.l2 = nn.Linear(520, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KLLEl0kSUri",
        "outputId": "bf69586c-09e0-49c4-9818-ac8eff526774"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MNIST Model on cpu\n",
            "============================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 166MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 30.5MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 134MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.95MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.304503\n",
            "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.310247\n",
            "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.303436\n",
            "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.322111\n",
            "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.295401\n",
            "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.298698\n",
            "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.296729\n",
            "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.303859\n",
            "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.304043\n",
            "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.296468\n",
            "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.311330\n",
            "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 2.306973\n",
            "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 2.301888\n",
            "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 2.297114\n",
            "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 2.297594\n",
            "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 2.303133\n",
            "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 2.296496\n",
            "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 2.302392\n",
            "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 2.298867\n",
            "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 2.293020\n",
            "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 2.293763\n",
            "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 2.296165\n",
            "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 2.300661\n",
            "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 2.295427\n",
            "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 2.291758\n",
            "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 2.289420\n",
            "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 2.295523\n",
            "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 2.292025\n",
            "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 2.288558\n",
            "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 2.295568\n",
            "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 2.286896\n",
            "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 2.290713\n",
            "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 2.291775\n",
            "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 2.288996\n",
            "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 2.287573\n",
            "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 2.290914\n",
            "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 2.283641\n",
            "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 2.283633\n",
            "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 2.280152\n",
            "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 2.285541\n",
            "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 2.276851\n",
            "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 2.274038\n",
            "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 2.284279\n",
            "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 2.266865\n",
            "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 2.274144\n",
            "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 2.268786\n",
            "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 2.277704\n",
            "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 2.271193\n",
            "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 2.269440\n",
            "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 2.279292\n",
            "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 2.262126\n",
            "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 2.261438\n",
            "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 2.264241\n",
            "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 2.263940\n",
            "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 2.270072\n",
            "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 2.236461\n",
            "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 2.244435\n",
            "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 2.233147\n",
            "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 2.245197\n",
            "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 2.231483\n",
            "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 2.229762\n",
            "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 2.228805\n",
            "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 2.232059\n",
            "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 2.222492\n",
            "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 2.232924\n",
            "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 2.214772\n",
            "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 2.223469\n",
            "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 2.192865\n",
            "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 2.177771\n",
            "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 2.173119\n",
            "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 2.203690\n",
            "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 2.155382\n",
            "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 2.140654\n",
            "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 2.104331\n",
            "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 2.080914\n",
            "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 2.076079\n",
            "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 2.047943\n",
            "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 2.055989\n",
            "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 2.046635\n",
            "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 1.958372\n",
            "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 1.959091\n",
            "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 1.840763\n",
            "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 1.741925\n",
            "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 1.820124\n",
            "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 1.741399\n",
            "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 1.739470\n",
            "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 1.662897\n",
            "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 1.734227\n",
            "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 1.604198\n",
            "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 1.480098\n",
            "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 1.469770\n",
            "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 1.438703\n",
            "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 1.387838\n",
            "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 1.315268\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0198, Accuracy: 5952/10000 (60%)\n",
            "Testing time: 0m 16s\n",
            "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 1.274573\n",
            "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 1.146796\n",
            "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 1.284975\n",
            "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 1.113065\n",
            "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 1.187942\n",
            "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 1.102693\n",
            "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 1.044783\n",
            "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.995556\n",
            "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.896727\n",
            "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 1.118727\n",
            "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 1.011104\n",
            "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 1.055564\n",
            "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.859495\n",
            "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.900341\n",
            "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.920817\n",
            "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.806569\n",
            "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.680469\n",
            "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.862614\n",
            "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.843914\n",
            "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.651272\n",
            "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.823695\n",
            "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.795545\n",
            "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.710995\n",
            "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.703373\n",
            "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.707349\n",
            "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.799668\n",
            "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.697684\n",
            "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.817912\n",
            "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.762807\n",
            "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.632626\n",
            "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.651404\n",
            "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.533715\n",
            "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.592577\n",
            "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.418468\n",
            "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.648755\n",
            "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.514587\n",
            "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.664715\n",
            "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.586711\n",
            "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.440773\n",
            "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.699883\n",
            "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.777543\n",
            "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.451320\n",
            "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.753191\n",
            "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.535750\n",
            "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.560138\n",
            "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.502770\n",
            "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.731445\n",
            "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.477038\n",
            "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.415022\n",
            "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.704112\n",
            "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.551808\n",
            "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.506191\n",
            "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.407194\n",
            "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.465716\n",
            "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.470442\n",
            "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.474690\n",
            "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.613253\n",
            "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.540375\n",
            "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.413916\n",
            "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.269816\n",
            "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.445651\n",
            "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.352234\n",
            "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.395094\n",
            "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.409598\n",
            "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.609046\n",
            "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.529138\n",
            "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.367670\n",
            "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.226387\n",
            "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.557803\n",
            "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.452773\n",
            "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.337046\n",
            "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.364945\n",
            "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.681805\n",
            "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.464312\n",
            "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.306780\n",
            "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.307292\n",
            "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.396746\n",
            "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.459496\n",
            "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.416135\n",
            "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.710287\n",
            "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.644414\n",
            "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.403469\n",
            "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.500104\n",
            "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.510574\n",
            "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.538165\n",
            "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.474575\n",
            "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.470809\n",
            "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.226320\n",
            "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.415227\n",
            "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.477752\n",
            "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.249792\n",
            "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.437694\n",
            "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.467805\n",
            "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.324746\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0064, Accuracy: 8795/10000 (88%)\n",
            "Testing time: 0m 17s\n",
            "Train Epoch: 3 | Batch Status: 0/60000 (0%) | Loss: 0.667949\n",
            "Train Epoch: 3 | Batch Status: 640/60000 (1%) | Loss: 0.420769\n",
            "Train Epoch: 3 | Batch Status: 1280/60000 (2%) | Loss: 0.350238\n",
            "Train Epoch: 3 | Batch Status: 1920/60000 (3%) | Loss: 0.505551\n",
            "Train Epoch: 3 | Batch Status: 2560/60000 (4%) | Loss: 0.346054\n",
            "Train Epoch: 3 | Batch Status: 3200/60000 (5%) | Loss: 0.338466\n",
            "Train Epoch: 3 | Batch Status: 3840/60000 (6%) | Loss: 0.525333\n",
            "Train Epoch: 3 | Batch Status: 4480/60000 (7%) | Loss: 0.588171\n",
            "Train Epoch: 3 | Batch Status: 5120/60000 (9%) | Loss: 0.359657\n",
            "Train Epoch: 3 | Batch Status: 5760/60000 (10%) | Loss: 0.269954\n",
            "Train Epoch: 3 | Batch Status: 6400/60000 (11%) | Loss: 0.567098\n",
            "Train Epoch: 3 | Batch Status: 7040/60000 (12%) | Loss: 0.221672\n",
            "Train Epoch: 3 | Batch Status: 7680/60000 (13%) | Loss: 0.310991\n",
            "Train Epoch: 3 | Batch Status: 8320/60000 (14%) | Loss: 0.356258\n",
            "Train Epoch: 3 | Batch Status: 8960/60000 (15%) | Loss: 0.284211\n",
            "Train Epoch: 3 | Batch Status: 9600/60000 (16%) | Loss: 0.483585\n",
            "Train Epoch: 3 | Batch Status: 10240/60000 (17%) | Loss: 0.433634\n",
            "Train Epoch: 3 | Batch Status: 10880/60000 (18%) | Loss: 0.353861\n",
            "Train Epoch: 3 | Batch Status: 11520/60000 (19%) | Loss: 0.338891\n",
            "Train Epoch: 3 | Batch Status: 12160/60000 (20%) | Loss: 0.293364\n",
            "Train Epoch: 3 | Batch Status: 12800/60000 (21%) | Loss: 0.214780\n",
            "Train Epoch: 3 | Batch Status: 13440/60000 (22%) | Loss: 0.521634\n",
            "Train Epoch: 3 | Batch Status: 14080/60000 (23%) | Loss: 0.395627\n",
            "Train Epoch: 3 | Batch Status: 14720/60000 (25%) | Loss: 0.695276\n",
            "Train Epoch: 3 | Batch Status: 15360/60000 (26%) | Loss: 0.374571\n",
            "Train Epoch: 3 | Batch Status: 16000/60000 (27%) | Loss: 0.286636\n",
            "Train Epoch: 3 | Batch Status: 16640/60000 (28%) | Loss: 0.394594\n",
            "Train Epoch: 3 | Batch Status: 17280/60000 (29%) | Loss: 0.373764\n",
            "Train Epoch: 3 | Batch Status: 17920/60000 (30%) | Loss: 0.260227\n",
            "Train Epoch: 3 | Batch Status: 18560/60000 (31%) | Loss: 0.365070\n",
            "Train Epoch: 3 | Batch Status: 19200/60000 (32%) | Loss: 0.349275\n",
            "Train Epoch: 3 | Batch Status: 19840/60000 (33%) | Loss: 0.193446\n",
            "Train Epoch: 3 | Batch Status: 20480/60000 (34%) | Loss: 0.588444\n",
            "Train Epoch: 3 | Batch Status: 21120/60000 (35%) | Loss: 0.319965\n",
            "Train Epoch: 3 | Batch Status: 21760/60000 (36%) | Loss: 0.306721\n",
            "Train Epoch: 3 | Batch Status: 22400/60000 (37%) | Loss: 0.528419\n",
            "Train Epoch: 3 | Batch Status: 23040/60000 (38%) | Loss: 0.221845\n",
            "Train Epoch: 3 | Batch Status: 23680/60000 (39%) | Loss: 0.493090\n",
            "Train Epoch: 3 | Batch Status: 24320/60000 (41%) | Loss: 0.432875\n",
            "Train Epoch: 3 | Batch Status: 24960/60000 (42%) | Loss: 0.357002\n",
            "Train Epoch: 3 | Batch Status: 25600/60000 (43%) | Loss: 0.351562\n",
            "Train Epoch: 3 | Batch Status: 26240/60000 (44%) | Loss: 0.317975\n",
            "Train Epoch: 3 | Batch Status: 26880/60000 (45%) | Loss: 0.256761\n",
            "Train Epoch: 3 | Batch Status: 27520/60000 (46%) | Loss: 0.329891\n",
            "Train Epoch: 3 | Batch Status: 28160/60000 (47%) | Loss: 0.289947\n",
            "Train Epoch: 3 | Batch Status: 28800/60000 (48%) | Loss: 0.162800\n",
            "Train Epoch: 3 | Batch Status: 29440/60000 (49%) | Loss: 0.220964\n",
            "Train Epoch: 3 | Batch Status: 30080/60000 (50%) | Loss: 0.425672\n",
            "Train Epoch: 3 | Batch Status: 30720/60000 (51%) | Loss: 0.300681\n",
            "Train Epoch: 3 | Batch Status: 31360/60000 (52%) | Loss: 0.192123\n",
            "Train Epoch: 3 | Batch Status: 32000/60000 (53%) | Loss: 0.485147\n",
            "Train Epoch: 3 | Batch Status: 32640/60000 (54%) | Loss: 0.524973\n",
            "Train Epoch: 3 | Batch Status: 33280/60000 (55%) | Loss: 0.227703\n",
            "Train Epoch: 3 | Batch Status: 33920/60000 (57%) | Loss: 0.403149\n",
            "Train Epoch: 3 | Batch Status: 34560/60000 (58%) | Loss: 0.245199\n",
            "Train Epoch: 3 | Batch Status: 35200/60000 (59%) | Loss: 0.282199\n",
            "Train Epoch: 3 | Batch Status: 35840/60000 (60%) | Loss: 0.313372\n",
            "Train Epoch: 3 | Batch Status: 36480/60000 (61%) | Loss: 0.165439\n",
            "Train Epoch: 3 | Batch Status: 37120/60000 (62%) | Loss: 0.270546\n",
            "Train Epoch: 3 | Batch Status: 37760/60000 (63%) | Loss: 0.200271\n",
            "Train Epoch: 3 | Batch Status: 38400/60000 (64%) | Loss: 0.251421\n",
            "Train Epoch: 3 | Batch Status: 39040/60000 (65%) | Loss: 0.363943\n",
            "Train Epoch: 3 | Batch Status: 39680/60000 (66%) | Loss: 0.448302\n",
            "Train Epoch: 3 | Batch Status: 40320/60000 (67%) | Loss: 0.384585\n",
            "Train Epoch: 3 | Batch Status: 40960/60000 (68%) | Loss: 0.422849\n",
            "Train Epoch: 3 | Batch Status: 41600/60000 (69%) | Loss: 0.241628\n",
            "Train Epoch: 3 | Batch Status: 42240/60000 (70%) | Loss: 0.477922\n",
            "Train Epoch: 3 | Batch Status: 42880/60000 (71%) | Loss: 0.351922\n",
            "Train Epoch: 3 | Batch Status: 43520/60000 (72%) | Loss: 0.296394\n",
            "Train Epoch: 3 | Batch Status: 44160/60000 (74%) | Loss: 0.206476\n",
            "Train Epoch: 3 | Batch Status: 44800/60000 (75%) | Loss: 0.178611\n",
            "Train Epoch: 3 | Batch Status: 45440/60000 (76%) | Loss: 0.332151\n",
            "Train Epoch: 3 | Batch Status: 46080/60000 (77%) | Loss: 0.282069\n",
            "Train Epoch: 3 | Batch Status: 46720/60000 (78%) | Loss: 0.134556\n",
            "Train Epoch: 3 | Batch Status: 47360/60000 (79%) | Loss: 0.278465\n",
            "Train Epoch: 3 | Batch Status: 48000/60000 (80%) | Loss: 0.207784\n",
            "Train Epoch: 3 | Batch Status: 48640/60000 (81%) | Loss: 0.355455\n",
            "Train Epoch: 3 | Batch Status: 49280/60000 (82%) | Loss: 0.322822\n",
            "Train Epoch: 3 | Batch Status: 49920/60000 (83%) | Loss: 0.601913\n",
            "Train Epoch: 3 | Batch Status: 50560/60000 (84%) | Loss: 0.232912\n",
            "Train Epoch: 3 | Batch Status: 51200/60000 (85%) | Loss: 0.380306\n",
            "Train Epoch: 3 | Batch Status: 51840/60000 (86%) | Loss: 0.398008\n",
            "Train Epoch: 3 | Batch Status: 52480/60000 (87%) | Loss: 0.303740\n",
            "Train Epoch: 3 | Batch Status: 53120/60000 (88%) | Loss: 0.249375\n",
            "Train Epoch: 3 | Batch Status: 53760/60000 (90%) | Loss: 0.228444\n",
            "Train Epoch: 3 | Batch Status: 54400/60000 (91%) | Loss: 0.387054\n",
            "Train Epoch: 3 | Batch Status: 55040/60000 (92%) | Loss: 0.242080\n",
            "Train Epoch: 3 | Batch Status: 55680/60000 (93%) | Loss: 0.319230\n",
            "Train Epoch: 3 | Batch Status: 56320/60000 (94%) | Loss: 0.300322\n",
            "Train Epoch: 3 | Batch Status: 56960/60000 (95%) | Loss: 0.370159\n",
            "Train Epoch: 3 | Batch Status: 57600/60000 (96%) | Loss: 0.513934\n",
            "Train Epoch: 3 | Batch Status: 58240/60000 (97%) | Loss: 0.236487\n",
            "Train Epoch: 3 | Batch Status: 58880/60000 (98%) | Loss: 0.403379\n",
            "Train Epoch: 3 | Batch Status: 59520/60000 (99%) | Loss: 0.294849\n",
            "Training time: 0m 14s\n",
            "===========================\n",
            "Test set: Average loss: 0.0046, Accuracy: 9134/10000 (91%)\n",
            "Testing time: 0m 16s\n",
            "Train Epoch: 4 | Batch Status: 0/60000 (0%) | Loss: 0.493254\n",
            "Train Epoch: 4 | Batch Status: 640/60000 (1%) | Loss: 0.303645\n",
            "Train Epoch: 4 | Batch Status: 1280/60000 (2%) | Loss: 0.261517\n",
            "Train Epoch: 4 | Batch Status: 1920/60000 (3%) | Loss: 0.286766\n",
            "Train Epoch: 4 | Batch Status: 2560/60000 (4%) | Loss: 0.242792\n",
            "Train Epoch: 4 | Batch Status: 3200/60000 (5%) | Loss: 0.389155\n",
            "Train Epoch: 4 | Batch Status: 3840/60000 (6%) | Loss: 0.358169\n",
            "Train Epoch: 4 | Batch Status: 4480/60000 (7%) | Loss: 0.252090\n",
            "Train Epoch: 4 | Batch Status: 5120/60000 (9%) | Loss: 0.352936\n",
            "Train Epoch: 4 | Batch Status: 5760/60000 (10%) | Loss: 0.360495\n",
            "Train Epoch: 4 | Batch Status: 6400/60000 (11%) | Loss: 0.173381\n",
            "Train Epoch: 4 | Batch Status: 7040/60000 (12%) | Loss: 0.179708\n",
            "Train Epoch: 4 | Batch Status: 7680/60000 (13%) | Loss: 0.232977\n",
            "Train Epoch: 4 | Batch Status: 8320/60000 (14%) | Loss: 0.207483\n",
            "Train Epoch: 4 | Batch Status: 8960/60000 (15%) | Loss: 0.327507\n",
            "Train Epoch: 4 | Batch Status: 9600/60000 (16%) | Loss: 0.351548\n",
            "Train Epoch: 4 | Batch Status: 10240/60000 (17%) | Loss: 0.186356\n",
            "Train Epoch: 4 | Batch Status: 10880/60000 (18%) | Loss: 0.421440\n",
            "Train Epoch: 4 | Batch Status: 11520/60000 (19%) | Loss: 0.216812\n",
            "Train Epoch: 4 | Batch Status: 12160/60000 (20%) | Loss: 0.332219\n",
            "Train Epoch: 4 | Batch Status: 12800/60000 (21%) | Loss: 0.315369\n",
            "Train Epoch: 4 | Batch Status: 13440/60000 (22%) | Loss: 0.198967\n",
            "Train Epoch: 4 | Batch Status: 14080/60000 (23%) | Loss: 0.259024\n",
            "Train Epoch: 4 | Batch Status: 14720/60000 (25%) | Loss: 0.358387\n",
            "Train Epoch: 4 | Batch Status: 15360/60000 (26%) | Loss: 0.113467\n",
            "Train Epoch: 4 | Batch Status: 16000/60000 (27%) | Loss: 0.421143\n",
            "Train Epoch: 4 | Batch Status: 16640/60000 (28%) | Loss: 0.138946\n",
            "Train Epoch: 4 | Batch Status: 17280/60000 (29%) | Loss: 0.257432\n",
            "Train Epoch: 4 | Batch Status: 17920/60000 (30%) | Loss: 0.454245\n",
            "Train Epoch: 4 | Batch Status: 18560/60000 (31%) | Loss: 0.158216\n",
            "Train Epoch: 4 | Batch Status: 19200/60000 (32%) | Loss: 0.158744\n",
            "Train Epoch: 4 | Batch Status: 19840/60000 (33%) | Loss: 0.196183\n",
            "Train Epoch: 4 | Batch Status: 20480/60000 (34%) | Loss: 0.255084\n",
            "Train Epoch: 4 | Batch Status: 21120/60000 (35%) | Loss: 0.300273\n",
            "Train Epoch: 4 | Batch Status: 21760/60000 (36%) | Loss: 0.507608\n",
            "Train Epoch: 4 | Batch Status: 22400/60000 (37%) | Loss: 0.220111\n",
            "Train Epoch: 4 | Batch Status: 23040/60000 (38%) | Loss: 0.227759\n",
            "Train Epoch: 4 | Batch Status: 23680/60000 (39%) | Loss: 0.376848\n",
            "Train Epoch: 4 | Batch Status: 24320/60000 (41%) | Loss: 0.208865\n",
            "Train Epoch: 4 | Batch Status: 24960/60000 (42%) | Loss: 0.498851\n",
            "Train Epoch: 4 | Batch Status: 25600/60000 (43%) | Loss: 0.177945\n",
            "Train Epoch: 4 | Batch Status: 26240/60000 (44%) | Loss: 0.375115\n",
            "Train Epoch: 4 | Batch Status: 26880/60000 (45%) | Loss: 0.174386\n",
            "Train Epoch: 4 | Batch Status: 27520/60000 (46%) | Loss: 0.190695\n",
            "Train Epoch: 4 | Batch Status: 28160/60000 (47%) | Loss: 0.102192\n",
            "Train Epoch: 4 | Batch Status: 28800/60000 (48%) | Loss: 0.361912\n",
            "Train Epoch: 4 | Batch Status: 29440/60000 (49%) | Loss: 0.356642\n",
            "Train Epoch: 4 | Batch Status: 30080/60000 (50%) | Loss: 0.506236\n",
            "Train Epoch: 4 | Batch Status: 30720/60000 (51%) | Loss: 0.170681\n",
            "Train Epoch: 4 | Batch Status: 31360/60000 (52%) | Loss: 0.189420\n",
            "Train Epoch: 4 | Batch Status: 32000/60000 (53%) | Loss: 0.228190\n",
            "Train Epoch: 4 | Batch Status: 32640/60000 (54%) | Loss: 0.142105\n",
            "Train Epoch: 4 | Batch Status: 33280/60000 (55%) | Loss: 0.214333\n",
            "Train Epoch: 4 | Batch Status: 33920/60000 (57%) | Loss: 0.270572\n",
            "Train Epoch: 4 | Batch Status: 34560/60000 (58%) | Loss: 0.173737\n",
            "Train Epoch: 4 | Batch Status: 35200/60000 (59%) | Loss: 0.140495\n",
            "Train Epoch: 4 | Batch Status: 35840/60000 (60%) | Loss: 0.162597\n",
            "Train Epoch: 4 | Batch Status: 36480/60000 (61%) | Loss: 0.250033\n",
            "Train Epoch: 4 | Batch Status: 37120/60000 (62%) | Loss: 0.365856\n",
            "Train Epoch: 4 | Batch Status: 37760/60000 (63%) | Loss: 0.338961\n",
            "Train Epoch: 4 | Batch Status: 38400/60000 (64%) | Loss: 0.246996\n",
            "Train Epoch: 4 | Batch Status: 39040/60000 (65%) | Loss: 0.162124\n",
            "Train Epoch: 4 | Batch Status: 39680/60000 (66%) | Loss: 0.138147\n",
            "Train Epoch: 4 | Batch Status: 40320/60000 (67%) | Loss: 0.333833\n",
            "Train Epoch: 4 | Batch Status: 40960/60000 (68%) | Loss: 0.104348\n",
            "Train Epoch: 4 | Batch Status: 41600/60000 (69%) | Loss: 0.408861\n",
            "Train Epoch: 4 | Batch Status: 42240/60000 (70%) | Loss: 0.253222\n",
            "Train Epoch: 4 | Batch Status: 42880/60000 (71%) | Loss: 0.203171\n",
            "Train Epoch: 4 | Batch Status: 43520/60000 (72%) | Loss: 0.187597\n",
            "Train Epoch: 4 | Batch Status: 44160/60000 (74%) | Loss: 0.374174\n",
            "Train Epoch: 4 | Batch Status: 44800/60000 (75%) | Loss: 0.195875\n",
            "Train Epoch: 4 | Batch Status: 45440/60000 (76%) | Loss: 0.135879\n",
            "Train Epoch: 4 | Batch Status: 46080/60000 (77%) | Loss: 0.249152\n",
            "Train Epoch: 4 | Batch Status: 46720/60000 (78%) | Loss: 0.199150\n",
            "Train Epoch: 4 | Batch Status: 47360/60000 (79%) | Loss: 0.150320\n",
            "Train Epoch: 4 | Batch Status: 48000/60000 (80%) | Loss: 0.313564\n",
            "Train Epoch: 4 | Batch Status: 48640/60000 (81%) | Loss: 0.132090\n",
            "Train Epoch: 4 | Batch Status: 49280/60000 (82%) | Loss: 0.174044\n",
            "Train Epoch: 4 | Batch Status: 49920/60000 (83%) | Loss: 0.385024\n",
            "Train Epoch: 4 | Batch Status: 50560/60000 (84%) | Loss: 0.234701\n",
            "Train Epoch: 4 | Batch Status: 51200/60000 (85%) | Loss: 0.236656\n",
            "Train Epoch: 4 | Batch Status: 51840/60000 (86%) | Loss: 0.184678\n",
            "Train Epoch: 4 | Batch Status: 52480/60000 (87%) | Loss: 0.146332\n",
            "Train Epoch: 4 | Batch Status: 53120/60000 (88%) | Loss: 0.249971\n",
            "Train Epoch: 4 | Batch Status: 53760/60000 (90%) | Loss: 0.183871\n",
            "Train Epoch: 4 | Batch Status: 54400/60000 (91%) | Loss: 0.165125\n",
            "Train Epoch: 4 | Batch Status: 55040/60000 (92%) | Loss: 0.119454\n",
            "Train Epoch: 4 | Batch Status: 55680/60000 (93%) | Loss: 0.112682\n",
            "Train Epoch: 4 | Batch Status: 56320/60000 (94%) | Loss: 0.298908\n",
            "Train Epoch: 4 | Batch Status: 56960/60000 (95%) | Loss: 0.201831\n",
            "Train Epoch: 4 | Batch Status: 57600/60000 (96%) | Loss: 0.161890\n",
            "Train Epoch: 4 | Batch Status: 58240/60000 (97%) | Loss: 0.213945\n",
            "Train Epoch: 4 | Batch Status: 58880/60000 (98%) | Loss: 0.235698\n",
            "Train Epoch: 4 | Batch Status: 59520/60000 (99%) | Loss: 0.256379\n",
            "Training time: 0m 14s\n",
            "===========================\n",
            "Test set: Average loss: 0.0033, Accuracy: 9387/10000 (94%)\n",
            "Testing time: 0m 16s\n",
            "Train Epoch: 5 | Batch Status: 0/60000 (0%) | Loss: 0.179128\n",
            "Train Epoch: 5 | Batch Status: 640/60000 (1%) | Loss: 0.228838\n",
            "Train Epoch: 5 | Batch Status: 1280/60000 (2%) | Loss: 0.248755\n",
            "Train Epoch: 5 | Batch Status: 1920/60000 (3%) | Loss: 0.272939\n",
            "Train Epoch: 5 | Batch Status: 2560/60000 (4%) | Loss: 0.113976\n",
            "Train Epoch: 5 | Batch Status: 3200/60000 (5%) | Loss: 0.168429\n",
            "Train Epoch: 5 | Batch Status: 3840/60000 (6%) | Loss: 0.207138\n",
            "Train Epoch: 5 | Batch Status: 4480/60000 (7%) | Loss: 0.135722\n",
            "Train Epoch: 5 | Batch Status: 5120/60000 (9%) | Loss: 0.182768\n",
            "Train Epoch: 5 | Batch Status: 5760/60000 (10%) | Loss: 0.139246\n",
            "Train Epoch: 5 | Batch Status: 6400/60000 (11%) | Loss: 0.359012\n",
            "Train Epoch: 5 | Batch Status: 7040/60000 (12%) | Loss: 0.187971\n",
            "Train Epoch: 5 | Batch Status: 7680/60000 (13%) | Loss: 0.143881\n",
            "Train Epoch: 5 | Batch Status: 8320/60000 (14%) | Loss: 0.110818\n",
            "Train Epoch: 5 | Batch Status: 8960/60000 (15%) | Loss: 0.208565\n",
            "Train Epoch: 5 | Batch Status: 9600/60000 (16%) | Loss: 0.291437\n",
            "Train Epoch: 5 | Batch Status: 10240/60000 (17%) | Loss: 0.123941\n",
            "Train Epoch: 5 | Batch Status: 10880/60000 (18%) | Loss: 0.205215\n",
            "Train Epoch: 5 | Batch Status: 11520/60000 (19%) | Loss: 0.150077\n",
            "Train Epoch: 5 | Batch Status: 12160/60000 (20%) | Loss: 0.172835\n",
            "Train Epoch: 5 | Batch Status: 12800/60000 (21%) | Loss: 0.146536\n",
            "Train Epoch: 5 | Batch Status: 13440/60000 (22%) | Loss: 0.235628\n",
            "Train Epoch: 5 | Batch Status: 14080/60000 (23%) | Loss: 0.057339\n",
            "Train Epoch: 5 | Batch Status: 14720/60000 (25%) | Loss: 0.195724\n",
            "Train Epoch: 5 | Batch Status: 15360/60000 (26%) | Loss: 0.097038\n",
            "Train Epoch: 5 | Batch Status: 16000/60000 (27%) | Loss: 0.076775\n",
            "Train Epoch: 5 | Batch Status: 16640/60000 (28%) | Loss: 0.122027\n",
            "Train Epoch: 5 | Batch Status: 17280/60000 (29%) | Loss: 0.230727\n",
            "Train Epoch: 5 | Batch Status: 17920/60000 (30%) | Loss: 0.188167\n",
            "Train Epoch: 5 | Batch Status: 18560/60000 (31%) | Loss: 0.236679\n",
            "Train Epoch: 5 | Batch Status: 19200/60000 (32%) | Loss: 0.122937\n",
            "Train Epoch: 5 | Batch Status: 19840/60000 (33%) | Loss: 0.097667\n",
            "Train Epoch: 5 | Batch Status: 20480/60000 (34%) | Loss: 0.054224\n",
            "Train Epoch: 5 | Batch Status: 21120/60000 (35%) | Loss: 0.224410\n",
            "Train Epoch: 5 | Batch Status: 21760/60000 (36%) | Loss: 0.172466\n",
            "Train Epoch: 5 | Batch Status: 22400/60000 (37%) | Loss: 0.237417\n",
            "Train Epoch: 5 | Batch Status: 23040/60000 (38%) | Loss: 0.264614\n",
            "Train Epoch: 5 | Batch Status: 23680/60000 (39%) | Loss: 0.154924\n",
            "Train Epoch: 5 | Batch Status: 24320/60000 (41%) | Loss: 0.222815\n",
            "Train Epoch: 5 | Batch Status: 24960/60000 (42%) | Loss: 0.121228\n",
            "Train Epoch: 5 | Batch Status: 25600/60000 (43%) | Loss: 0.064714\n",
            "Train Epoch: 5 | Batch Status: 26240/60000 (44%) | Loss: 0.229087\n",
            "Train Epoch: 5 | Batch Status: 26880/60000 (45%) | Loss: 0.159940\n",
            "Train Epoch: 5 | Batch Status: 27520/60000 (46%) | Loss: 0.055739\n",
            "Train Epoch: 5 | Batch Status: 28160/60000 (47%) | Loss: 0.087078\n",
            "Train Epoch: 5 | Batch Status: 28800/60000 (48%) | Loss: 0.104805\n",
            "Train Epoch: 5 | Batch Status: 29440/60000 (49%) | Loss: 0.144530\n",
            "Train Epoch: 5 | Batch Status: 30080/60000 (50%) | Loss: 0.198421\n",
            "Train Epoch: 5 | Batch Status: 30720/60000 (51%) | Loss: 0.054576\n",
            "Train Epoch: 5 | Batch Status: 31360/60000 (52%) | Loss: 0.177537\n",
            "Train Epoch: 5 | Batch Status: 32000/60000 (53%) | Loss: 0.155551\n",
            "Train Epoch: 5 | Batch Status: 32640/60000 (54%) | Loss: 0.143365\n",
            "Train Epoch: 5 | Batch Status: 33280/60000 (55%) | Loss: 0.207318\n",
            "Train Epoch: 5 | Batch Status: 33920/60000 (57%) | Loss: 0.127267\n",
            "Train Epoch: 5 | Batch Status: 34560/60000 (58%) | Loss: 0.216639\n",
            "Train Epoch: 5 | Batch Status: 35200/60000 (59%) | Loss: 0.234373\n",
            "Train Epoch: 5 | Batch Status: 35840/60000 (60%) | Loss: 0.037287\n",
            "Train Epoch: 5 | Batch Status: 36480/60000 (61%) | Loss: 0.091585\n",
            "Train Epoch: 5 | Batch Status: 37120/60000 (62%) | Loss: 0.177773\n",
            "Train Epoch: 5 | Batch Status: 37760/60000 (63%) | Loss: 0.232835\n",
            "Train Epoch: 5 | Batch Status: 38400/60000 (64%) | Loss: 0.230429\n",
            "Train Epoch: 5 | Batch Status: 39040/60000 (65%) | Loss: 0.228889\n",
            "Train Epoch: 5 | Batch Status: 39680/60000 (66%) | Loss: 0.081006\n",
            "Train Epoch: 5 | Batch Status: 40320/60000 (67%) | Loss: 0.060905\n",
            "Train Epoch: 5 | Batch Status: 40960/60000 (68%) | Loss: 0.223078\n",
            "Train Epoch: 5 | Batch Status: 41600/60000 (69%) | Loss: 0.107850\n",
            "Train Epoch: 5 | Batch Status: 42240/60000 (70%) | Loss: 0.139446\n",
            "Train Epoch: 5 | Batch Status: 42880/60000 (71%) | Loss: 0.139714\n",
            "Train Epoch: 5 | Batch Status: 43520/60000 (72%) | Loss: 0.259835\n",
            "Train Epoch: 5 | Batch Status: 44160/60000 (74%) | Loss: 0.202314\n",
            "Train Epoch: 5 | Batch Status: 44800/60000 (75%) | Loss: 0.063791\n",
            "Train Epoch: 5 | Batch Status: 45440/60000 (76%) | Loss: 0.100253\n",
            "Train Epoch: 5 | Batch Status: 46080/60000 (77%) | Loss: 0.194938\n",
            "Train Epoch: 5 | Batch Status: 46720/60000 (78%) | Loss: 0.146534\n",
            "Train Epoch: 5 | Batch Status: 47360/60000 (79%) | Loss: 0.143900\n",
            "Train Epoch: 5 | Batch Status: 48000/60000 (80%) | Loss: 0.056402\n",
            "Train Epoch: 5 | Batch Status: 48640/60000 (81%) | Loss: 0.130188\n",
            "Train Epoch: 5 | Batch Status: 49280/60000 (82%) | Loss: 0.188349\n",
            "Train Epoch: 5 | Batch Status: 49920/60000 (83%) | Loss: 0.066970\n",
            "Train Epoch: 5 | Batch Status: 50560/60000 (84%) | Loss: 0.086005\n",
            "Train Epoch: 5 | Batch Status: 51200/60000 (85%) | Loss: 0.136146\n",
            "Train Epoch: 5 | Batch Status: 51840/60000 (86%) | Loss: 0.095599\n",
            "Train Epoch: 5 | Batch Status: 52480/60000 (87%) | Loss: 0.321743\n",
            "Train Epoch: 5 | Batch Status: 53120/60000 (88%) | Loss: 0.269153\n",
            "Train Epoch: 5 | Batch Status: 53760/60000 (90%) | Loss: 0.090155\n",
            "Train Epoch: 5 | Batch Status: 54400/60000 (91%) | Loss: 0.176731\n",
            "Train Epoch: 5 | Batch Status: 55040/60000 (92%) | Loss: 0.392662\n",
            "Train Epoch: 5 | Batch Status: 55680/60000 (93%) | Loss: 0.068388\n",
            "Train Epoch: 5 | Batch Status: 56320/60000 (94%) | Loss: 0.271413\n",
            "Train Epoch: 5 | Batch Status: 56960/60000 (95%) | Loss: 0.190677\n",
            "Train Epoch: 5 | Batch Status: 57600/60000 (96%) | Loss: 0.267818\n",
            "Train Epoch: 5 | Batch Status: 58240/60000 (97%) | Loss: 0.107428\n",
            "Train Epoch: 5 | Batch Status: 58880/60000 (98%) | Loss: 0.265735\n",
            "Train Epoch: 5 | Batch Status: 59520/60000 (99%) | Loss: 0.181616\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0026, Accuracy: 9506/10000 (95%)\n",
            "Testing time: 0m 17s\n",
            "Train Epoch: 6 | Batch Status: 0/60000 (0%) | Loss: 0.198405\n",
            "Train Epoch: 6 | Batch Status: 640/60000 (1%) | Loss: 0.232731\n",
            "Train Epoch: 6 | Batch Status: 1280/60000 (2%) | Loss: 0.210691\n",
            "Train Epoch: 6 | Batch Status: 1920/60000 (3%) | Loss: 0.163518\n",
            "Train Epoch: 6 | Batch Status: 2560/60000 (4%) | Loss: 0.170701\n",
            "Train Epoch: 6 | Batch Status: 3200/60000 (5%) | Loss: 0.112051\n",
            "Train Epoch: 6 | Batch Status: 3840/60000 (6%) | Loss: 0.232641\n",
            "Train Epoch: 6 | Batch Status: 4480/60000 (7%) | Loss: 0.074742\n",
            "Train Epoch: 6 | Batch Status: 5120/60000 (9%) | Loss: 0.034040\n",
            "Train Epoch: 6 | Batch Status: 5760/60000 (10%) | Loss: 0.228049\n",
            "Train Epoch: 6 | Batch Status: 6400/60000 (11%) | Loss: 0.183413\n",
            "Train Epoch: 6 | Batch Status: 7040/60000 (12%) | Loss: 0.096464\n",
            "Train Epoch: 6 | Batch Status: 7680/60000 (13%) | Loss: 0.453511\n",
            "Train Epoch: 6 | Batch Status: 8320/60000 (14%) | Loss: 0.173110\n",
            "Train Epoch: 6 | Batch Status: 8960/60000 (15%) | Loss: 0.167456\n",
            "Train Epoch: 6 | Batch Status: 9600/60000 (16%) | Loss: 0.347992\n",
            "Train Epoch: 6 | Batch Status: 10240/60000 (17%) | Loss: 0.082853\n",
            "Train Epoch: 6 | Batch Status: 10880/60000 (18%) | Loss: 0.236630\n",
            "Train Epoch: 6 | Batch Status: 11520/60000 (19%) | Loss: 0.113202\n",
            "Train Epoch: 6 | Batch Status: 12160/60000 (20%) | Loss: 0.192970\n",
            "Train Epoch: 6 | Batch Status: 12800/60000 (21%) | Loss: 0.130230\n",
            "Train Epoch: 6 | Batch Status: 13440/60000 (22%) | Loss: 0.241775\n",
            "Train Epoch: 6 | Batch Status: 14080/60000 (23%) | Loss: 0.187072\n",
            "Train Epoch: 6 | Batch Status: 14720/60000 (25%) | Loss: 0.108888\n",
            "Train Epoch: 6 | Batch Status: 15360/60000 (26%) | Loss: 0.195885\n",
            "Train Epoch: 6 | Batch Status: 16000/60000 (27%) | Loss: 0.338487\n",
            "Train Epoch: 6 | Batch Status: 16640/60000 (28%) | Loss: 0.134348\n",
            "Train Epoch: 6 | Batch Status: 17280/60000 (29%) | Loss: 0.112950\n",
            "Train Epoch: 6 | Batch Status: 17920/60000 (30%) | Loss: 0.268816\n",
            "Train Epoch: 6 | Batch Status: 18560/60000 (31%) | Loss: 0.094187\n",
            "Train Epoch: 6 | Batch Status: 19200/60000 (32%) | Loss: 0.131509\n",
            "Train Epoch: 6 | Batch Status: 19840/60000 (33%) | Loss: 0.591345\n",
            "Train Epoch: 6 | Batch Status: 20480/60000 (34%) | Loss: 0.393597\n",
            "Train Epoch: 6 | Batch Status: 21120/60000 (35%) | Loss: 0.365494\n",
            "Train Epoch: 6 | Batch Status: 21760/60000 (36%) | Loss: 0.233712\n",
            "Train Epoch: 6 | Batch Status: 22400/60000 (37%) | Loss: 0.132366\n",
            "Train Epoch: 6 | Batch Status: 23040/60000 (38%) | Loss: 0.146151\n",
            "Train Epoch: 6 | Batch Status: 23680/60000 (39%) | Loss: 0.254375\n",
            "Train Epoch: 6 | Batch Status: 24320/60000 (41%) | Loss: 0.219247\n",
            "Train Epoch: 6 | Batch Status: 24960/60000 (42%) | Loss: 0.099106\n",
            "Train Epoch: 6 | Batch Status: 25600/60000 (43%) | Loss: 0.215931\n",
            "Train Epoch: 6 | Batch Status: 26240/60000 (44%) | Loss: 0.147440\n",
            "Train Epoch: 6 | Batch Status: 26880/60000 (45%) | Loss: 0.184804\n",
            "Train Epoch: 6 | Batch Status: 27520/60000 (46%) | Loss: 0.095438\n",
            "Train Epoch: 6 | Batch Status: 28160/60000 (47%) | Loss: 0.234465\n",
            "Train Epoch: 6 | Batch Status: 28800/60000 (48%) | Loss: 0.169244\n",
            "Train Epoch: 6 | Batch Status: 29440/60000 (49%) | Loss: 0.209935\n",
            "Train Epoch: 6 | Batch Status: 30080/60000 (50%) | Loss: 0.115442\n",
            "Train Epoch: 6 | Batch Status: 30720/60000 (51%) | Loss: 0.188043\n",
            "Train Epoch: 6 | Batch Status: 31360/60000 (52%) | Loss: 0.142570\n",
            "Train Epoch: 6 | Batch Status: 32000/60000 (53%) | Loss: 0.202466\n",
            "Train Epoch: 6 | Batch Status: 32640/60000 (54%) | Loss: 0.119405\n",
            "Train Epoch: 6 | Batch Status: 33280/60000 (55%) | Loss: 0.096901\n",
            "Train Epoch: 6 | Batch Status: 33920/60000 (57%) | Loss: 0.107288\n",
            "Train Epoch: 6 | Batch Status: 34560/60000 (58%) | Loss: 0.188402\n",
            "Train Epoch: 6 | Batch Status: 35200/60000 (59%) | Loss: 0.116640\n",
            "Train Epoch: 6 | Batch Status: 35840/60000 (60%) | Loss: 0.040575\n",
            "Train Epoch: 6 | Batch Status: 36480/60000 (61%) | Loss: 0.114540\n",
            "Train Epoch: 6 | Batch Status: 37120/60000 (62%) | Loss: 0.117504\n",
            "Train Epoch: 6 | Batch Status: 37760/60000 (63%) | Loss: 0.172809\n",
            "Train Epoch: 6 | Batch Status: 38400/60000 (64%) | Loss: 0.146359\n",
            "Train Epoch: 6 | Batch Status: 39040/60000 (65%) | Loss: 0.123392\n",
            "Train Epoch: 6 | Batch Status: 39680/60000 (66%) | Loss: 0.199275\n",
            "Train Epoch: 6 | Batch Status: 40320/60000 (67%) | Loss: 0.123946\n",
            "Train Epoch: 6 | Batch Status: 40960/60000 (68%) | Loss: 0.083725\n",
            "Train Epoch: 6 | Batch Status: 41600/60000 (69%) | Loss: 0.172813\n",
            "Train Epoch: 6 | Batch Status: 42240/60000 (70%) | Loss: 0.377459\n",
            "Train Epoch: 6 | Batch Status: 42880/60000 (71%) | Loss: 0.217486\n",
            "Train Epoch: 6 | Batch Status: 43520/60000 (72%) | Loss: 0.089608\n",
            "Train Epoch: 6 | Batch Status: 44160/60000 (74%) | Loss: 0.051885\n",
            "Train Epoch: 6 | Batch Status: 44800/60000 (75%) | Loss: 0.105073\n",
            "Train Epoch: 6 | Batch Status: 45440/60000 (76%) | Loss: 0.131639\n",
            "Train Epoch: 6 | Batch Status: 46080/60000 (77%) | Loss: 0.062101\n",
            "Train Epoch: 6 | Batch Status: 46720/60000 (78%) | Loss: 0.240952\n",
            "Train Epoch: 6 | Batch Status: 47360/60000 (79%) | Loss: 0.054021\n",
            "Train Epoch: 6 | Batch Status: 48000/60000 (80%) | Loss: 0.189058\n",
            "Train Epoch: 6 | Batch Status: 48640/60000 (81%) | Loss: 0.135929\n",
            "Train Epoch: 6 | Batch Status: 49280/60000 (82%) | Loss: 0.071840\n",
            "Train Epoch: 6 | Batch Status: 49920/60000 (83%) | Loss: 0.207637\n",
            "Train Epoch: 6 | Batch Status: 50560/60000 (84%) | Loss: 0.048430\n",
            "Train Epoch: 6 | Batch Status: 51200/60000 (85%) | Loss: 0.111943\n",
            "Train Epoch: 6 | Batch Status: 51840/60000 (86%) | Loss: 0.096755\n",
            "Train Epoch: 6 | Batch Status: 52480/60000 (87%) | Loss: 0.199087\n",
            "Train Epoch: 6 | Batch Status: 53120/60000 (88%) | Loss: 0.123758\n",
            "Train Epoch: 6 | Batch Status: 53760/60000 (90%) | Loss: 0.108760\n",
            "Train Epoch: 6 | Batch Status: 54400/60000 (91%) | Loss: 0.050010\n",
            "Train Epoch: 6 | Batch Status: 55040/60000 (92%) | Loss: 0.172102\n",
            "Train Epoch: 6 | Batch Status: 55680/60000 (93%) | Loss: 0.123594\n",
            "Train Epoch: 6 | Batch Status: 56320/60000 (94%) | Loss: 0.147518\n",
            "Train Epoch: 6 | Batch Status: 56960/60000 (95%) | Loss: 0.097850\n",
            "Train Epoch: 6 | Batch Status: 57600/60000 (96%) | Loss: 0.164156\n",
            "Train Epoch: 6 | Batch Status: 58240/60000 (97%) | Loss: 0.133766\n",
            "Train Epoch: 6 | Batch Status: 58880/60000 (98%) | Loss: 0.121970\n",
            "Train Epoch: 6 | Batch Status: 59520/60000 (99%) | Loss: 0.125720\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0024, Accuracy: 9553/10000 (96%)\n",
            "Testing time: 0m 17s\n",
            "Train Epoch: 7 | Batch Status: 0/60000 (0%) | Loss: 0.072122\n",
            "Train Epoch: 7 | Batch Status: 640/60000 (1%) | Loss: 0.171210\n",
            "Train Epoch: 7 | Batch Status: 1280/60000 (2%) | Loss: 0.129046\n",
            "Train Epoch: 7 | Batch Status: 1920/60000 (3%) | Loss: 0.036265\n",
            "Train Epoch: 7 | Batch Status: 2560/60000 (4%) | Loss: 0.170515\n",
            "Train Epoch: 7 | Batch Status: 3200/60000 (5%) | Loss: 0.221768\n",
            "Train Epoch: 7 | Batch Status: 3840/60000 (6%) | Loss: 0.141401\n",
            "Train Epoch: 7 | Batch Status: 4480/60000 (7%) | Loss: 0.139311\n",
            "Train Epoch: 7 | Batch Status: 5120/60000 (9%) | Loss: 0.174345\n",
            "Train Epoch: 7 | Batch Status: 5760/60000 (10%) | Loss: 0.138149\n",
            "Train Epoch: 7 | Batch Status: 6400/60000 (11%) | Loss: 0.196539\n",
            "Train Epoch: 7 | Batch Status: 7040/60000 (12%) | Loss: 0.117237\n",
            "Train Epoch: 7 | Batch Status: 7680/60000 (13%) | Loss: 0.203669\n",
            "Train Epoch: 7 | Batch Status: 8320/60000 (14%) | Loss: 0.065979\n",
            "Train Epoch: 7 | Batch Status: 8960/60000 (15%) | Loss: 0.071329\n",
            "Train Epoch: 7 | Batch Status: 9600/60000 (16%) | Loss: 0.113481\n",
            "Train Epoch: 7 | Batch Status: 10240/60000 (17%) | Loss: 0.069741\n",
            "Train Epoch: 7 | Batch Status: 10880/60000 (18%) | Loss: 0.106461\n",
            "Train Epoch: 7 | Batch Status: 11520/60000 (19%) | Loss: 0.114097\n",
            "Train Epoch: 7 | Batch Status: 12160/60000 (20%) | Loss: 0.128668\n",
            "Train Epoch: 7 | Batch Status: 12800/60000 (21%) | Loss: 0.087050\n",
            "Train Epoch: 7 | Batch Status: 13440/60000 (22%) | Loss: 0.142082\n",
            "Train Epoch: 7 | Batch Status: 14080/60000 (23%) | Loss: 0.227369\n",
            "Train Epoch: 7 | Batch Status: 14720/60000 (25%) | Loss: 0.124597\n",
            "Train Epoch: 7 | Batch Status: 15360/60000 (26%) | Loss: 0.179182\n",
            "Train Epoch: 7 | Batch Status: 16000/60000 (27%) | Loss: 0.058767\n",
            "Train Epoch: 7 | Batch Status: 16640/60000 (28%) | Loss: 0.043835\n",
            "Train Epoch: 7 | Batch Status: 17280/60000 (29%) | Loss: 0.094487\n",
            "Train Epoch: 7 | Batch Status: 17920/60000 (30%) | Loss: 0.124015\n",
            "Train Epoch: 7 | Batch Status: 18560/60000 (31%) | Loss: 0.141304\n",
            "Train Epoch: 7 | Batch Status: 19200/60000 (32%) | Loss: 0.080849\n",
            "Train Epoch: 7 | Batch Status: 19840/60000 (33%) | Loss: 0.024554\n",
            "Train Epoch: 7 | Batch Status: 20480/60000 (34%) | Loss: 0.109004\n",
            "Train Epoch: 7 | Batch Status: 21120/60000 (35%) | Loss: 0.111811\n",
            "Train Epoch: 7 | Batch Status: 21760/60000 (36%) | Loss: 0.194308\n",
            "Train Epoch: 7 | Batch Status: 22400/60000 (37%) | Loss: 0.096019\n",
            "Train Epoch: 7 | Batch Status: 23040/60000 (38%) | Loss: 0.140916\n",
            "Train Epoch: 7 | Batch Status: 23680/60000 (39%) | Loss: 0.173072\n",
            "Train Epoch: 7 | Batch Status: 24320/60000 (41%) | Loss: 0.039052\n",
            "Train Epoch: 7 | Batch Status: 24960/60000 (42%) | Loss: 0.131995\n",
            "Train Epoch: 7 | Batch Status: 25600/60000 (43%) | Loss: 0.074475\n",
            "Train Epoch: 7 | Batch Status: 26240/60000 (44%) | Loss: 0.108580\n",
            "Train Epoch: 7 | Batch Status: 26880/60000 (45%) | Loss: 0.124779\n",
            "Train Epoch: 7 | Batch Status: 27520/60000 (46%) | Loss: 0.238869\n",
            "Train Epoch: 7 | Batch Status: 28160/60000 (47%) | Loss: 0.067945\n",
            "Train Epoch: 7 | Batch Status: 28800/60000 (48%) | Loss: 0.063712\n",
            "Train Epoch: 7 | Batch Status: 29440/60000 (49%) | Loss: 0.090903\n",
            "Train Epoch: 7 | Batch Status: 30080/60000 (50%) | Loss: 0.106285\n",
            "Train Epoch: 7 | Batch Status: 30720/60000 (51%) | Loss: 0.094932\n",
            "Train Epoch: 7 | Batch Status: 31360/60000 (52%) | Loss: 0.177209\n",
            "Train Epoch: 7 | Batch Status: 32000/60000 (53%) | Loss: 0.059822\n",
            "Train Epoch: 7 | Batch Status: 32640/60000 (54%) | Loss: 0.082052\n",
            "Train Epoch: 7 | Batch Status: 33280/60000 (55%) | Loss: 0.148338\n",
            "Train Epoch: 7 | Batch Status: 33920/60000 (57%) | Loss: 0.187731\n",
            "Train Epoch: 7 | Batch Status: 34560/60000 (58%) | Loss: 0.157324\n",
            "Train Epoch: 7 | Batch Status: 35200/60000 (59%) | Loss: 0.058124\n",
            "Train Epoch: 7 | Batch Status: 35840/60000 (60%) | Loss: 0.195157\n",
            "Train Epoch: 7 | Batch Status: 36480/60000 (61%) | Loss: 0.234488\n",
            "Train Epoch: 7 | Batch Status: 37120/60000 (62%) | Loss: 0.199412\n",
            "Train Epoch: 7 | Batch Status: 37760/60000 (63%) | Loss: 0.189855\n",
            "Train Epoch: 7 | Batch Status: 38400/60000 (64%) | Loss: 0.063313\n",
            "Train Epoch: 7 | Batch Status: 39040/60000 (65%) | Loss: 0.064245\n",
            "Train Epoch: 7 | Batch Status: 39680/60000 (66%) | Loss: 0.060809\n",
            "Train Epoch: 7 | Batch Status: 40320/60000 (67%) | Loss: 0.081079\n",
            "Train Epoch: 7 | Batch Status: 40960/60000 (68%) | Loss: 0.142747\n",
            "Train Epoch: 7 | Batch Status: 41600/60000 (69%) | Loss: 0.309008\n",
            "Train Epoch: 7 | Batch Status: 42240/60000 (70%) | Loss: 0.135262\n",
            "Train Epoch: 7 | Batch Status: 42880/60000 (71%) | Loss: 0.110155\n",
            "Train Epoch: 7 | Batch Status: 43520/60000 (72%) | Loss: 0.191510\n",
            "Train Epoch: 7 | Batch Status: 44160/60000 (74%) | Loss: 0.104894\n",
            "Train Epoch: 7 | Batch Status: 44800/60000 (75%) | Loss: 0.054997\n",
            "Train Epoch: 7 | Batch Status: 45440/60000 (76%) | Loss: 0.076711\n",
            "Train Epoch: 7 | Batch Status: 46080/60000 (77%) | Loss: 0.167664\n",
            "Train Epoch: 7 | Batch Status: 46720/60000 (78%) | Loss: 0.055048\n",
            "Train Epoch: 7 | Batch Status: 47360/60000 (79%) | Loss: 0.076729\n",
            "Train Epoch: 7 | Batch Status: 48000/60000 (80%) | Loss: 0.249712\n",
            "Train Epoch: 7 | Batch Status: 48640/60000 (81%) | Loss: 0.076705\n",
            "Train Epoch: 7 | Batch Status: 49280/60000 (82%) | Loss: 0.059684\n",
            "Train Epoch: 7 | Batch Status: 49920/60000 (83%) | Loss: 0.088181\n",
            "Train Epoch: 7 | Batch Status: 50560/60000 (84%) | Loss: 0.281662\n",
            "Train Epoch: 7 | Batch Status: 51200/60000 (85%) | Loss: 0.105824\n",
            "Train Epoch: 7 | Batch Status: 51840/60000 (86%) | Loss: 0.078977\n",
            "Train Epoch: 7 | Batch Status: 52480/60000 (87%) | Loss: 0.106653\n",
            "Train Epoch: 7 | Batch Status: 53120/60000 (88%) | Loss: 0.146421\n",
            "Train Epoch: 7 | Batch Status: 53760/60000 (90%) | Loss: 0.100656\n",
            "Train Epoch: 7 | Batch Status: 54400/60000 (91%) | Loss: 0.059165\n",
            "Train Epoch: 7 | Batch Status: 55040/60000 (92%) | Loss: 0.195260\n",
            "Train Epoch: 7 | Batch Status: 55680/60000 (93%) | Loss: 0.163571\n",
            "Train Epoch: 7 | Batch Status: 56320/60000 (94%) | Loss: 0.117774\n",
            "Train Epoch: 7 | Batch Status: 56960/60000 (95%) | Loss: 0.136502\n",
            "Train Epoch: 7 | Batch Status: 57600/60000 (96%) | Loss: 0.107552\n",
            "Train Epoch: 7 | Batch Status: 58240/60000 (97%) | Loss: 0.143583\n",
            "Train Epoch: 7 | Batch Status: 58880/60000 (98%) | Loss: 0.151960\n",
            "Train Epoch: 7 | Batch Status: 59520/60000 (99%) | Loss: 0.217093\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0019, Accuracy: 9633/10000 (96%)\n",
            "Testing time: 0m 16s\n",
            "Train Epoch: 8 | Batch Status: 0/60000 (0%) | Loss: 0.071115\n",
            "Train Epoch: 8 | Batch Status: 640/60000 (1%) | Loss: 0.173945\n",
            "Train Epoch: 8 | Batch Status: 1280/60000 (2%) | Loss: 0.095668\n",
            "Train Epoch: 8 | Batch Status: 1920/60000 (3%) | Loss: 0.097533\n",
            "Train Epoch: 8 | Batch Status: 2560/60000 (4%) | Loss: 0.150062\n",
            "Train Epoch: 8 | Batch Status: 3200/60000 (5%) | Loss: 0.448650\n",
            "Train Epoch: 8 | Batch Status: 3840/60000 (6%) | Loss: 0.128462\n",
            "Train Epoch: 8 | Batch Status: 4480/60000 (7%) | Loss: 0.134211\n",
            "Train Epoch: 8 | Batch Status: 5120/60000 (9%) | Loss: 0.054957\n",
            "Train Epoch: 8 | Batch Status: 5760/60000 (10%) | Loss: 0.036892\n",
            "Train Epoch: 8 | Batch Status: 6400/60000 (11%) | Loss: 0.070961\n",
            "Train Epoch: 8 | Batch Status: 7040/60000 (12%) | Loss: 0.155926\n",
            "Train Epoch: 8 | Batch Status: 7680/60000 (13%) | Loss: 0.120153\n",
            "Train Epoch: 8 | Batch Status: 8320/60000 (14%) | Loss: 0.042215\n",
            "Train Epoch: 8 | Batch Status: 8960/60000 (15%) | Loss: 0.183406\n",
            "Train Epoch: 8 | Batch Status: 9600/60000 (16%) | Loss: 0.029611\n",
            "Train Epoch: 8 | Batch Status: 10240/60000 (17%) | Loss: 0.145114\n",
            "Train Epoch: 8 | Batch Status: 10880/60000 (18%) | Loss: 0.088538\n",
            "Train Epoch: 8 | Batch Status: 11520/60000 (19%) | Loss: 0.113250\n",
            "Train Epoch: 8 | Batch Status: 12160/60000 (20%) | Loss: 0.052634\n",
            "Train Epoch: 8 | Batch Status: 12800/60000 (21%) | Loss: 0.142041\n",
            "Train Epoch: 8 | Batch Status: 13440/60000 (22%) | Loss: 0.101924\n",
            "Train Epoch: 8 | Batch Status: 14080/60000 (23%) | Loss: 0.056550\n",
            "Train Epoch: 8 | Batch Status: 14720/60000 (25%) | Loss: 0.157505\n",
            "Train Epoch: 8 | Batch Status: 15360/60000 (26%) | Loss: 0.030381\n",
            "Train Epoch: 8 | Batch Status: 16000/60000 (27%) | Loss: 0.058885\n",
            "Train Epoch: 8 | Batch Status: 16640/60000 (28%) | Loss: 0.345775\n",
            "Train Epoch: 8 | Batch Status: 17280/60000 (29%) | Loss: 0.114337\n",
            "Train Epoch: 8 | Batch Status: 17920/60000 (30%) | Loss: 0.061760\n",
            "Train Epoch: 8 | Batch Status: 18560/60000 (31%) | Loss: 0.028179\n",
            "Train Epoch: 8 | Batch Status: 19200/60000 (32%) | Loss: 0.128337\n",
            "Train Epoch: 8 | Batch Status: 19840/60000 (33%) | Loss: 0.094805\n",
            "Train Epoch: 8 | Batch Status: 20480/60000 (34%) | Loss: 0.153487\n",
            "Train Epoch: 8 | Batch Status: 21120/60000 (35%) | Loss: 0.074330\n",
            "Train Epoch: 8 | Batch Status: 21760/60000 (36%) | Loss: 0.026869\n",
            "Train Epoch: 8 | Batch Status: 22400/60000 (37%) | Loss: 0.097752\n",
            "Train Epoch: 8 | Batch Status: 23040/60000 (38%) | Loss: 0.033072\n",
            "Train Epoch: 8 | Batch Status: 23680/60000 (39%) | Loss: 0.046526\n",
            "Train Epoch: 8 | Batch Status: 24320/60000 (41%) | Loss: 0.042098\n",
            "Train Epoch: 8 | Batch Status: 24960/60000 (42%) | Loss: 0.061436\n",
            "Train Epoch: 8 | Batch Status: 25600/60000 (43%) | Loss: 0.047802\n",
            "Train Epoch: 8 | Batch Status: 26240/60000 (44%) | Loss: 0.107439\n",
            "Train Epoch: 8 | Batch Status: 26880/60000 (45%) | Loss: 0.025504\n",
            "Train Epoch: 8 | Batch Status: 27520/60000 (46%) | Loss: 0.055824\n",
            "Train Epoch: 8 | Batch Status: 28160/60000 (47%) | Loss: 0.196443\n",
            "Train Epoch: 8 | Batch Status: 28800/60000 (48%) | Loss: 0.272370\n",
            "Train Epoch: 8 | Batch Status: 29440/60000 (49%) | Loss: 0.028945\n",
            "Train Epoch: 8 | Batch Status: 30080/60000 (50%) | Loss: 0.047426\n",
            "Train Epoch: 8 | Batch Status: 30720/60000 (51%) | Loss: 0.064838\n",
            "Train Epoch: 8 | Batch Status: 31360/60000 (52%) | Loss: 0.089448\n",
            "Train Epoch: 8 | Batch Status: 32000/60000 (53%) | Loss: 0.060980\n",
            "Train Epoch: 8 | Batch Status: 32640/60000 (54%) | Loss: 0.100408\n",
            "Train Epoch: 8 | Batch Status: 33280/60000 (55%) | Loss: 0.186093\n",
            "Train Epoch: 8 | Batch Status: 33920/60000 (57%) | Loss: 0.095612\n",
            "Train Epoch: 8 | Batch Status: 34560/60000 (58%) | Loss: 0.047116\n",
            "Train Epoch: 8 | Batch Status: 35200/60000 (59%) | Loss: 0.201992\n",
            "Train Epoch: 8 | Batch Status: 35840/60000 (60%) | Loss: 0.088090\n",
            "Train Epoch: 8 | Batch Status: 36480/60000 (61%) | Loss: 0.070938\n",
            "Train Epoch: 8 | Batch Status: 37120/60000 (62%) | Loss: 0.095883\n",
            "Train Epoch: 8 | Batch Status: 37760/60000 (63%) | Loss: 0.147761\n",
            "Train Epoch: 8 | Batch Status: 38400/60000 (64%) | Loss: 0.214737\n",
            "Train Epoch: 8 | Batch Status: 39040/60000 (65%) | Loss: 0.135975\n",
            "Train Epoch: 8 | Batch Status: 39680/60000 (66%) | Loss: 0.115876\n",
            "Train Epoch: 8 | Batch Status: 40320/60000 (67%) | Loss: 0.107389\n",
            "Train Epoch: 8 | Batch Status: 40960/60000 (68%) | Loss: 0.094168\n",
            "Train Epoch: 8 | Batch Status: 41600/60000 (69%) | Loss: 0.114173\n",
            "Train Epoch: 8 | Batch Status: 42240/60000 (70%) | Loss: 0.036341\n",
            "Train Epoch: 8 | Batch Status: 42880/60000 (71%) | Loss: 0.087298\n",
            "Train Epoch: 8 | Batch Status: 43520/60000 (72%) | Loss: 0.125247\n",
            "Train Epoch: 8 | Batch Status: 44160/60000 (74%) | Loss: 0.117260\n",
            "Train Epoch: 8 | Batch Status: 44800/60000 (75%) | Loss: 0.057761\n",
            "Train Epoch: 8 | Batch Status: 45440/60000 (76%) | Loss: 0.054698\n",
            "Train Epoch: 8 | Batch Status: 46080/60000 (77%) | Loss: 0.035758\n",
            "Train Epoch: 8 | Batch Status: 46720/60000 (78%) | Loss: 0.095214\n",
            "Train Epoch: 8 | Batch Status: 47360/60000 (79%) | Loss: 0.110372\n",
            "Train Epoch: 8 | Batch Status: 48000/60000 (80%) | Loss: 0.191359\n",
            "Train Epoch: 8 | Batch Status: 48640/60000 (81%) | Loss: 0.105985\n",
            "Train Epoch: 8 | Batch Status: 49280/60000 (82%) | Loss: 0.035361\n",
            "Train Epoch: 8 | Batch Status: 49920/60000 (83%) | Loss: 0.030672\n",
            "Train Epoch: 8 | Batch Status: 50560/60000 (84%) | Loss: 0.104508\n",
            "Train Epoch: 8 | Batch Status: 51200/60000 (85%) | Loss: 0.074436\n",
            "Train Epoch: 8 | Batch Status: 51840/60000 (86%) | Loss: 0.064427\n",
            "Train Epoch: 8 | Batch Status: 52480/60000 (87%) | Loss: 0.043729\n",
            "Train Epoch: 8 | Batch Status: 53120/60000 (88%) | Loss: 0.058179\n",
            "Train Epoch: 8 | Batch Status: 53760/60000 (90%) | Loss: 0.132911\n",
            "Train Epoch: 8 | Batch Status: 54400/60000 (91%) | Loss: 0.053839\n",
            "Train Epoch: 8 | Batch Status: 55040/60000 (92%) | Loss: 0.123407\n",
            "Train Epoch: 8 | Batch Status: 55680/60000 (93%) | Loss: 0.094621\n",
            "Train Epoch: 8 | Batch Status: 56320/60000 (94%) | Loss: 0.075481\n",
            "Train Epoch: 8 | Batch Status: 56960/60000 (95%) | Loss: 0.146716\n",
            "Train Epoch: 8 | Batch Status: 57600/60000 (96%) | Loss: 0.094958\n",
            "Train Epoch: 8 | Batch Status: 58240/60000 (97%) | Loss: 0.136361\n",
            "Train Epoch: 8 | Batch Status: 58880/60000 (98%) | Loss: 0.138918\n",
            "Train Epoch: 8 | Batch Status: 59520/60000 (99%) | Loss: 0.058915\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0019, Accuracy: 9638/10000 (96%)\n",
            "Testing time: 0m 17s\n",
            "Train Epoch: 9 | Batch Status: 0/60000 (0%) | Loss: 0.130506\n",
            "Train Epoch: 9 | Batch Status: 640/60000 (1%) | Loss: 0.055867\n",
            "Train Epoch: 9 | Batch Status: 1280/60000 (2%) | Loss: 0.091604\n",
            "Train Epoch: 9 | Batch Status: 1920/60000 (3%) | Loss: 0.025468\n",
            "Train Epoch: 9 | Batch Status: 2560/60000 (4%) | Loss: 0.163727\n",
            "Train Epoch: 9 | Batch Status: 3200/60000 (5%) | Loss: 0.025787\n",
            "Train Epoch: 9 | Batch Status: 3840/60000 (6%) | Loss: 0.022876\n",
            "Train Epoch: 9 | Batch Status: 4480/60000 (7%) | Loss: 0.108123\n",
            "Train Epoch: 9 | Batch Status: 5120/60000 (9%) | Loss: 0.116889\n",
            "Train Epoch: 9 | Batch Status: 5760/60000 (10%) | Loss: 0.189551\n",
            "Train Epoch: 9 | Batch Status: 6400/60000 (11%) | Loss: 0.197570\n",
            "Train Epoch: 9 | Batch Status: 7040/60000 (12%) | Loss: 0.208667\n",
            "Train Epoch: 9 | Batch Status: 7680/60000 (13%) | Loss: 0.042320\n",
            "Train Epoch: 9 | Batch Status: 8320/60000 (14%) | Loss: 0.192076\n",
            "Train Epoch: 9 | Batch Status: 8960/60000 (15%) | Loss: 0.116759\n",
            "Train Epoch: 9 | Batch Status: 9600/60000 (16%) | Loss: 0.050226\n",
            "Train Epoch: 9 | Batch Status: 10240/60000 (17%) | Loss: 0.046365\n",
            "Train Epoch: 9 | Batch Status: 10880/60000 (18%) | Loss: 0.031315\n",
            "Train Epoch: 9 | Batch Status: 11520/60000 (19%) | Loss: 0.052694\n",
            "Train Epoch: 9 | Batch Status: 12160/60000 (20%) | Loss: 0.107654\n",
            "Train Epoch: 9 | Batch Status: 12800/60000 (21%) | Loss: 0.030000\n",
            "Train Epoch: 9 | Batch Status: 13440/60000 (22%) | Loss: 0.061372\n",
            "Train Epoch: 9 | Batch Status: 14080/60000 (23%) | Loss: 0.115585\n",
            "Train Epoch: 9 | Batch Status: 14720/60000 (25%) | Loss: 0.120659\n",
            "Train Epoch: 9 | Batch Status: 15360/60000 (26%) | Loss: 0.033000\n",
            "Train Epoch: 9 | Batch Status: 16000/60000 (27%) | Loss: 0.149465\n",
            "Train Epoch: 9 | Batch Status: 16640/60000 (28%) | Loss: 0.020592\n",
            "Train Epoch: 9 | Batch Status: 17280/60000 (29%) | Loss: 0.044505\n",
            "Train Epoch: 9 | Batch Status: 17920/60000 (30%) | Loss: 0.274321\n",
            "Train Epoch: 9 | Batch Status: 18560/60000 (31%) | Loss: 0.108919\n",
            "Train Epoch: 9 | Batch Status: 19200/60000 (32%) | Loss: 0.056088\n",
            "Train Epoch: 9 | Batch Status: 19840/60000 (33%) | Loss: 0.037627\n",
            "Train Epoch: 9 | Batch Status: 20480/60000 (34%) | Loss: 0.137780\n",
            "Train Epoch: 9 | Batch Status: 21120/60000 (35%) | Loss: 0.024750\n",
            "Train Epoch: 9 | Batch Status: 21760/60000 (36%) | Loss: 0.224837\n",
            "Train Epoch: 9 | Batch Status: 22400/60000 (37%) | Loss: 0.113105\n",
            "Train Epoch: 9 | Batch Status: 23040/60000 (38%) | Loss: 0.034063\n",
            "Train Epoch: 9 | Batch Status: 23680/60000 (39%) | Loss: 0.043725\n",
            "Train Epoch: 9 | Batch Status: 24320/60000 (41%) | Loss: 0.192010\n",
            "Train Epoch: 9 | Batch Status: 24960/60000 (42%) | Loss: 0.163945\n",
            "Train Epoch: 9 | Batch Status: 25600/60000 (43%) | Loss: 0.088247\n",
            "Train Epoch: 9 | Batch Status: 26240/60000 (44%) | Loss: 0.047734\n",
            "Train Epoch: 9 | Batch Status: 26880/60000 (45%) | Loss: 0.159012\n",
            "Train Epoch: 9 | Batch Status: 27520/60000 (46%) | Loss: 0.064414\n",
            "Train Epoch: 9 | Batch Status: 28160/60000 (47%) | Loss: 0.033617\n",
            "Train Epoch: 9 | Batch Status: 28800/60000 (48%) | Loss: 0.273302\n",
            "Train Epoch: 9 | Batch Status: 29440/60000 (49%) | Loss: 0.118409\n",
            "Train Epoch: 9 | Batch Status: 30080/60000 (50%) | Loss: 0.050377\n",
            "Train Epoch: 9 | Batch Status: 30720/60000 (51%) | Loss: 0.118927\n",
            "Train Epoch: 9 | Batch Status: 31360/60000 (52%) | Loss: 0.058037\n",
            "Train Epoch: 9 | Batch Status: 32000/60000 (53%) | Loss: 0.030230\n",
            "Train Epoch: 9 | Batch Status: 32640/60000 (54%) | Loss: 0.039692\n",
            "Train Epoch: 9 | Batch Status: 33280/60000 (55%) | Loss: 0.082446\n",
            "Train Epoch: 9 | Batch Status: 33920/60000 (57%) | Loss: 0.136459\n",
            "Train Epoch: 9 | Batch Status: 34560/60000 (58%) | Loss: 0.189198\n",
            "Train Epoch: 9 | Batch Status: 35200/60000 (59%) | Loss: 0.068107\n",
            "Train Epoch: 9 | Batch Status: 35840/60000 (60%) | Loss: 0.039043\n",
            "Train Epoch: 9 | Batch Status: 36480/60000 (61%) | Loss: 0.082736\n",
            "Train Epoch: 9 | Batch Status: 37120/60000 (62%) | Loss: 0.063699\n",
            "Train Epoch: 9 | Batch Status: 37760/60000 (63%) | Loss: 0.045918\n",
            "Train Epoch: 9 | Batch Status: 38400/60000 (64%) | Loss: 0.088934\n",
            "Train Epoch: 9 | Batch Status: 39040/60000 (65%) | Loss: 0.116912\n",
            "Train Epoch: 9 | Batch Status: 39680/60000 (66%) | Loss: 0.075557\n",
            "Train Epoch: 9 | Batch Status: 40320/60000 (67%) | Loss: 0.062931\n",
            "Train Epoch: 9 | Batch Status: 40960/60000 (68%) | Loss: 0.071557\n",
            "Train Epoch: 9 | Batch Status: 41600/60000 (69%) | Loss: 0.020080\n",
            "Train Epoch: 9 | Batch Status: 42240/60000 (70%) | Loss: 0.099100\n",
            "Train Epoch: 9 | Batch Status: 42880/60000 (71%) | Loss: 0.120292\n",
            "Train Epoch: 9 | Batch Status: 43520/60000 (72%) | Loss: 0.179596\n",
            "Train Epoch: 9 | Batch Status: 44160/60000 (74%) | Loss: 0.159449\n",
            "Train Epoch: 9 | Batch Status: 44800/60000 (75%) | Loss: 0.108339\n",
            "Train Epoch: 9 | Batch Status: 45440/60000 (76%) | Loss: 0.212401\n",
            "Train Epoch: 9 | Batch Status: 46080/60000 (77%) | Loss: 0.048044\n",
            "Train Epoch: 9 | Batch Status: 46720/60000 (78%) | Loss: 0.046660\n",
            "Train Epoch: 9 | Batch Status: 47360/60000 (79%) | Loss: 0.076887\n",
            "Train Epoch: 9 | Batch Status: 48000/60000 (80%) | Loss: 0.038329\n",
            "Train Epoch: 9 | Batch Status: 48640/60000 (81%) | Loss: 0.026328\n",
            "Train Epoch: 9 | Batch Status: 49280/60000 (82%) | Loss: 0.123614\n",
            "Train Epoch: 9 | Batch Status: 49920/60000 (83%) | Loss: 0.075789\n",
            "Train Epoch: 9 | Batch Status: 50560/60000 (84%) | Loss: 0.109531\n",
            "Train Epoch: 9 | Batch Status: 51200/60000 (85%) | Loss: 0.224643\n",
            "Train Epoch: 9 | Batch Status: 51840/60000 (86%) | Loss: 0.136783\n",
            "Train Epoch: 9 | Batch Status: 52480/60000 (87%) | Loss: 0.088647\n",
            "Train Epoch: 9 | Batch Status: 53120/60000 (88%) | Loss: 0.064357\n",
            "Train Epoch: 9 | Batch Status: 53760/60000 (90%) | Loss: 0.104681\n",
            "Train Epoch: 9 | Batch Status: 54400/60000 (91%) | Loss: 0.035597\n",
            "Train Epoch: 9 | Batch Status: 55040/60000 (92%) | Loss: 0.090053\n",
            "Train Epoch: 9 | Batch Status: 55680/60000 (93%) | Loss: 0.053077\n",
            "Train Epoch: 9 | Batch Status: 56320/60000 (94%) | Loss: 0.102047\n",
            "Train Epoch: 9 | Batch Status: 56960/60000 (95%) | Loss: 0.123732\n",
            "Train Epoch: 9 | Batch Status: 57600/60000 (96%) | Loss: 0.118999\n",
            "Train Epoch: 9 | Batch Status: 58240/60000 (97%) | Loss: 0.017116\n",
            "Train Epoch: 9 | Batch Status: 58880/60000 (98%) | Loss: 0.109571\n",
            "Train Epoch: 9 | Batch Status: 59520/60000 (99%) | Loss: 0.069684\n",
            "Training time: 0m 15s\n",
            "===========================\n",
            "Test set: Average loss: 0.0017, Accuracy: 9675/10000 (97%)\n",
            "Testing time: 0m 16s\n",
            "Total Time: 2m 27s\n",
            "Model was trained on cpu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(320, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udmw1zLBUrfG",
        "outputId": "48bac885-c093-468d-c6a8-8444febe0a94"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 69.8MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 29.8MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 71.4MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.01MB/s]\n",
            "<ipython-input-17-19111ba88540>:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316053\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.299601\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.292191\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.298933\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.273827\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.276397\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.280575\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.258113\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.244192\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.219964\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.216697\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.204368\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.149863\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.084019\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.025456\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.918910\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.778082\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.586082\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.366294\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.151005\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.916184\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.753828\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.602796\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.620592\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.468832\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.395520\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.413652\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.402254\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.508774\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.588826\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.321172\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.284442\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.425122\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.334162\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.231706\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.361686\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.250125\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.272789\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.500591\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.456423\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.319627\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.234718\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.283637\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.248534\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.300345\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.187030\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.176930\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.502750\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.188890\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.158137\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.221272\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.324571\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.553274\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.462657\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.184463\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.195350\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.226880\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.148995\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.300614\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.235833\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.141275\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.152662\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.111257\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.293223\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.110273\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.152584\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.145412\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.246747\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.209721\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.118812\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.194422\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.166314\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.322244\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.153609\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.362038\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.231096\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.305205\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.145148\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.139850\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.289307\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.204162\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.104907\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.253567\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.111875\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.158412\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.117247\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.209318\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.265302\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.136450\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.274246\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.164341\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.182799\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.092123\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.206997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-19111ba88540>:77: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  data, target = Variable(data, volatile=True), Variable(target)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1520, Accuracy: 9570/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.160120\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.210475\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.244786\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.142840\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.142666\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.254016\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.367560\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.258195\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.258318\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.287577\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.374193\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.117046\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.120233\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.096916\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.130285\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.226512\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.196207\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.126785\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.264946\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.056703\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.133386\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.201849\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.072056\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.168273\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.095301\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.181203\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.064584\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.205679\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.310877\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.157278\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.137939\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.166000\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.213786\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.322252\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.107852\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.132789\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.081032\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.042567\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.081692\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.060899\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.170251\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.320926\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.092776\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.175497\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.118051\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.138699\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.194264\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.102318\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.164650\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.273559\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.131265\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.045867\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.080718\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.104501\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.290158\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.205408\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.168619\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.079321\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.153544\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.154423\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.163907\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.130400\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.090045\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.113597\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.163264\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.110953\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.216771\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.080016\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.087516\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.147528\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.081797\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.081969\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.166219\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.099650\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.100315\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.162546\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.083386\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.124608\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.117294\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.068862\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.056955\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.165740\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.118909\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.036470\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.116742\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.213429\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.147670\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.219051\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.070075\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.051720\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.088212\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.118651\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.112833\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.026397\n",
            "\n",
            "Test set: Average loss: 0.0984, Accuracy: 9701/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.268525\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.145961\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.111771\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.134588\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.077300\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.093435\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.094034\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.058641\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.101225\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.078490\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.083118\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.044236\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.023582\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.191482\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.050570\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.187151\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.119855\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.066390\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.040508\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.067202\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.212938\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.136487\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.085453\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.161711\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.177623\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.058853\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.059833\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.068605\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.204670\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.080572\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.150301\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.155776\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.096650\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.030231\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.166960\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.092737\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.122694\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.021763\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.039884\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.157772\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.084049\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.149847\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.156336\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.048428\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.086245\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.032557\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.181304\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.071366\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.085176\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.132828\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.085854\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.089386\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.043966\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.070614\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.086763\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.151825\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.075852\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.077560\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.066255\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.042073\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.073579\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.110476\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.164369\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.090746\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.097512\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.059177\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.052536\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.085646\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.117679\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.099834\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.220707\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.063898\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.063839\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.070091\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.028193\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.156627\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.031597\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.140396\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.229561\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.032989\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.080951\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.068931\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.096746\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.073711\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.057941\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.111509\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.120786\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.100411\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.028347\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.054895\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.215593\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.134677\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.026369\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.049518\n",
            "\n",
            "Test set: Average loss: 0.0819, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.031862\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.080583\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.196944\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.032052\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.154462\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.164662\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.184264\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.157852\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.083540\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.074813\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.061503\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.125114\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.126278\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.071495\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.060930\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.121492\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.076971\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.049371\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.022462\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.136427\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.065836\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.088520\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.066542\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.031551\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.227699\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.179410\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.026229\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.047408\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.106885\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.028906\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.053168\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.040597\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.027846\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.069620\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.055109\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.080196\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.115946\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.024146\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.128683\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.071278\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.061889\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.225664\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.147962\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.096297\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.057650\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.247400\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.050766\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.204463\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.200687\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.116976\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.034122\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.078707\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.110720\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.115119\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.042091\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.144090\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.036223\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.179304\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.067701\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.097424\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.058703\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.044325\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.038924\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.182497\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.038132\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.089130\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.135035\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.155036\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.068873\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.079573\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.024759\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.134749\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.048716\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.080524\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.031490\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.075175\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.022887\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.057516\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.163830\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.054282\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.036668\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.058581\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.176819\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.060783\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.150768\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.035094\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.053286\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.120253\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.039040\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.022537\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.017424\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.053726\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.008271\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.065383\n",
            "\n",
            "Test set: Average loss: 0.0732, Accuracy: 9773/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.067401\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.122023\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.040077\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.151934\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.082289\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.108522\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.071142\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.111216\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.032433\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.054647\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.049001\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.040897\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.082611\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.107300\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.031991\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.033498\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.118703\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.097913\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.018774\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.054873\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.080329\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.051132\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.141585\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.032063\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.048198\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.039156\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.088861\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.050791\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.045802\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.031982\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.134640\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.004953\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.013895\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.112968\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.040911\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.016963\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.016651\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.081351\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.008802\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.242095\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.140845\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.105002\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.184814\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.012400\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.035279\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.070971\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.063174\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.095292\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.043471\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.166469\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.127059\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.016980\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.023482\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.056347\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.145638\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.037664\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.126819\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.028497\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.029645\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.025614\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.056880\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.059329\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.051296\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.111428\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.032643\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.052023\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.193553\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.150905\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.045863\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.058698\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.075179\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.057877\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.171022\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.179344\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.068808\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.028806\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.090599\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.068855\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.033635\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.042297\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.149116\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.098294\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.015557\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.077530\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.097707\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.029508\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.027973\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.107663\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.027978\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.082957\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.022646\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.026682\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.069649\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.137287\n",
            "\n",
            "Test set: Average loss: 0.0638, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.091654\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.092893\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.019448\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.160811\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.099734\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.055369\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.066570\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.069379\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.024636\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.084586\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.083411\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.072375\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.138875\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.058403\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.056469\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.045628\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.075401\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.033086\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.112694\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.154186\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.060154\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.082944\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.032511\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.185384\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.111352\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.038733\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.038807\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.045399\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.095055\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.053488\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.128427\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.239143\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.271341\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.103470\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.045397\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.086795\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.041953\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.069694\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.068711\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.026847\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.115694\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.098618\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.132929\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.118661\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.056349\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.016282\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.147314\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.059149\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.074653\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.018005\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.052493\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.085627\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.087865\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.011984\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.108952\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.030948\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.045982\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.060899\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.032387\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.047832\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.056212\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.075407\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.060652\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.173488\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.004637\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.028296\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.123444\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.249996\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.029774\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.165148\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.070269\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.055960\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.033972\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.132968\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.147878\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.023114\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.040335\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.027226\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.043612\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.042528\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.041451\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.019906\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.167761\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.136352\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.085526\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.022947\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.015207\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.122723\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.040631\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.025877\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.067843\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.067750\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.108741\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.069578\n",
            "\n",
            "Test set: Average loss: 0.0673, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.104854\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.031190\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.056338\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.051620\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.016135\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.088554\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.177200\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.091298\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.037404\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.010922\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.113604\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.042564\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.032770\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.214607\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.033471\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.031724\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.078343\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.084810\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.036426\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.050182\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.039284\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.014235\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.019933\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.044849\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.058254\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.020734\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.042809\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.023323\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.045893\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.120112\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.124542\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.075571\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.003714\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.022008\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.040794\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.008618\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.081707\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.028521\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.022236\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.133871\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.055174\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.091796\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.015289\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.114056\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.008989\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.021051\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.042331\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.081884\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.029318\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.008784\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.098014\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.056263\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.191435\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.035862\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.151852\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.026960\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.136471\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.036441\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.094519\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.023563\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.036141\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.053356\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.077948\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.174965\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.131366\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.033381\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.047325\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.197754\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.070090\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.055385\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.024149\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.089794\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.039732\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.084856\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.009120\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.057481\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.061631\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.070940\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.048404\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.012316\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.027151\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.036620\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.069229\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.024426\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.021199\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.066577\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.047601\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.063416\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.037224\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.039502\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.092050\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.079817\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.099031\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.038063\n",
            "\n",
            "Test set: Average loss: 0.0585, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.134670\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.134997\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.004466\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.020350\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.047545\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.100780\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.040601\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.027255\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.077278\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.132786\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.036499\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.012702\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.057288\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.081322\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.031280\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.031172\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.079765\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.032468\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.012547\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.025177\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.058939\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.089947\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.100385\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.090272\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.057681\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.010005\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.055034\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.095446\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.093973\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.082125\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.095450\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.020893\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.044156\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.017500\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.145028\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.114837\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.068400\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.162616\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.049813\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.050741\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.045587\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.032953\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.008263\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.076853\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.078912\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.057446\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.022729\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.032570\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.053169\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.061737\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.059016\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.069918\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.072761\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.032725\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.055264\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.047880\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.008923\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.005128\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.039443\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.040558\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.013709\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.025111\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.013970\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.105852\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.052963\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.111729\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.011165\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.085916\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.053686\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.006594\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.080969\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.034437\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.036838\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.045377\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.158003\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.044732\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.056813\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.060371\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.030499\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.035212\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.039798\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.094330\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.032574\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.014596\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.090230\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.069193\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.018032\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.151385\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.022793\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.025845\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.227040\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.077967\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.119227\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.018168\n",
            "\n",
            "Test set: Average loss: 0.0496, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.089066\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.016398\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.024593\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.025251\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.086881\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.024802\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.017789\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.072882\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.071217\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.060019\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.021632\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.021988\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.047146\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.102750\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.052746\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.089077\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.161090\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.042218\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.014572\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.044094\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.066195\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.012655\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.018248\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.099486\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.017372\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.047367\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.031960\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.090217\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.081634\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.112942\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.022681\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.025255\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.054331\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.151746\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.031791\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.169853\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.015835\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.020858\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.078352\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.032034\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.018652\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.068995\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.121833\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.061001\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.007955\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.040233\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.016190\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.082539\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.011617\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.029089\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.031862\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.067577\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.054252\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.032363\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.022135\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.018841\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.036366\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.086375\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.002293\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.042859\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.023309\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.031508\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.144399\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.015355\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.061471\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.086810\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.036668\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.055119\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.088626\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.077102\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.068895\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.068308\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.120491\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.089119\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.039606\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.049652\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.007870\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.157864\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.108530\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.042518\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.009849\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.041596\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.074199\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.006111\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.135687\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.032319\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.081577\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.043640\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.005659\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.041908\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.051997\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.024216\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.087217\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.009842\n",
            "\n",
            "Test set: Average loss: 0.0502, Accuracy: 9832/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Device configuration (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
        "\n",
        "        self.incept1 = InceptionA(in_channels=10)\n",
        "        self.incept2 = InceptionA(in_channels=20)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(1408, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = self.incept1(x)\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = self.incept2(x)\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u31uUhl0WbV0",
        "outputId": "82b0ddba-06f2-455a-cefa-a251d2e17cd9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304242\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.297877\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.301882\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.299867\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.300443\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.286656\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.298187\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.270241\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.279405\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.261255\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.262937\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.260894\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.235962\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.194573\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.118273\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.097780\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.904068\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.527344\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.357888\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.207346\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.866401\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.600631\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.602966\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.464285\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.570963\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.828711\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.340814\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.605256\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.608251\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.651976\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.336929\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.647911\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.312835\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.624214\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.525710\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.511091\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.575133\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.706836\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.466164\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.277125\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.469230\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.338101\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.297227\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.529159\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.347868\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.359152\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.326381\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.554803\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.333953\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.384778\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.394349\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.485129\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.307639\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.300060\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.308561\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.399315\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.409275\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.587171\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.347138\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.403764\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.173263\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.328296\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.171957\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.266236\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.294288\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.228940\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.308236\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.246511\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.625469\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.221117\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.485864\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.297481\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.313652\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.176585\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.197294\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.196358\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.249754\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.236932\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.208941\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.174619\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.245573\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.280392\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.121276\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.159419\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.212704\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.137425\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.086181\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.324204\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.103193\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.115160\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.281238\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.302619\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.149170\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.201520\n",
            "\n",
            "Test set: Average loss: 0.1889, Accuracy: 9432/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.063465\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.270177\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.162725\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.173519\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.297741\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.363138\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.207334\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.278114\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.259109\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.287515\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.099144\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.145446\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.203573\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.223392\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.193535\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.262899\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.220169\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.127836\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.316233\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.130857\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.187624\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.136262\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.363708\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.115360\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.293577\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.055337\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.166239\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.257506\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.105600\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.093478\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.141586\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.312209\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.097221\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.124932\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.178205\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.143310\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.122889\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.298990\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.120279\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.207349\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.115902\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.071936\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.409704\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.066168\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.103663\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.076410\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.247356\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.218639\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.139113\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.201928\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.201544\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.144815\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.199723\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.132257\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.067029\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.073351\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.079295\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.095302\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.565003\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.149251\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.089304\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.179377\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.056519\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.071712\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.095916\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.042977\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.168913\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.083600\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.112019\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.114004\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.125360\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.084997\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.071026\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.057157\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.117982\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.072935\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.169008\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.121701\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.244838\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.160238\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.182064\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.103292\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.111028\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.122784\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.127905\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.068057\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.107473\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.133580\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.157010\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.093056\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.040945\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.103653\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.087519\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.237886\n",
            "\n",
            "Test set: Average loss: 0.1147, Accuracy: 9653/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.096683\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.117341\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.052928\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.085920\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.074220\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.121348\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.176667\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.222484\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.177199\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.039789\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.227941\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.119483\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.138727\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.085008\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.022130\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.122486\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.125016\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.098397\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.114394\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.245445\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.234827\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.106090\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.047320\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.010906\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.042744\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.063055\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.421875\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.037672\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.196135\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.135125\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.042834\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.193682\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.136902\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.147430\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.128930\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.083121\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.129294\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.032393\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.190393\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.036035\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.265339\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.049863\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.024594\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.022043\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.125493\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.211379\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.127808\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.037496\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.075647\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.167828\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.250727\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.081367\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.105114\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.034999\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.097357\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.146859\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.099616\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.066120\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.028898\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.038527\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.113387\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.076146\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.126815\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.055019\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.243776\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.111540\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.015710\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.182685\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.091623\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.038249\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.052177\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.089684\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.048263\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.138866\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.035806\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.277037\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.216777\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.129665\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.010287\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.070274\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.095376\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.077734\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.043910\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.032907\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.100522\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.380507\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.050128\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.063705\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.036731\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.077487\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.181019\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.046121\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.064024\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.087740\n",
            "\n",
            "Test set: Average loss: 0.0856, Accuracy: 9715/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.084531\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.119513\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.033709\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.143560\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.085529\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.025894\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.271033\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.062643\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.080054\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.081352\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.066742\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.068032\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.016557\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.142843\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.039172\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.023615\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.029994\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.208480\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.012711\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.036756\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.202905\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.181746\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.081448\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.012988\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.068423\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.083326\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.075272\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.044676\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.125545\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.015014\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.088347\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.036160\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.170895\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.078242\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.060697\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.049881\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.164609\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.032108\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.022234\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.044192\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.085792\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.058356\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.184275\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.065431\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.042720\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.089393\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.024816\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.020203\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.102376\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.156943\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.030918\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.180710\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.023729\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.121644\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.103685\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.013590\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.155668\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.069341\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.113214\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.236016\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.055386\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.064222\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.044815\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.050024\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.074955\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.077607\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.053243\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.135469\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.020726\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.172732\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.019525\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.097493\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.024976\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.047731\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.036259\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.055105\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.142011\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.047754\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.050625\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.157634\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.108103\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.073366\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.312843\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.025259\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.118480\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.085340\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.062514\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.046614\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.025822\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.068672\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.062325\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.034956\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.104557\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.047826\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.035829\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.036343\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.043827\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.221551\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.047879\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.036508\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.080696\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.042323\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.100569\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.069729\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.024896\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.040987\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.265980\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.081771\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.065599\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.398815\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.031126\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.078966\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.318406\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.053519\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.122676\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.052949\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.068137\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.019675\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.091835\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.048740\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.068463\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.039647\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.129798\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.065660\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.079216\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.035973\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.195154\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.024093\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.066211\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.046744\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.094420\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.030523\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.017082\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.110476\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.071597\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.105424\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.013261\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.048863\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.149477\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.073232\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.211313\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.064794\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.060083\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.047219\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.057459\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.033316\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.046735\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.103503\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.038178\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.050886\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.078439\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.087955\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.028198\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.072031\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.058097\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.102715\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.084760\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.012924\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.015653\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.014604\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.412106\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.046936\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.072860\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.041388\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.196936\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.028843\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.190227\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.132292\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.019175\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.038942\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.057941\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.019041\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.064623\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.109126\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.113023\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.066184\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.021794\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.028267\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.100053\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.017965\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.039136\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.019003\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.163831\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.029945\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.163704\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.062819\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.090531\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.077920\n",
            "\n",
            "Test set: Average loss: 0.0686, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.070364\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.086568\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.040923\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.074404\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.072495\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.126657\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.027990\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.161905\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.014293\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.053981\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.076006\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.090729\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.044351\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.046869\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.027502\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.010013\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.066824\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.056289\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.215788\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.138783\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.054860\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.037830\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.200861\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.058924\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.077389\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.027189\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.062868\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.109502\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.041892\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.121986\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.078373\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.101540\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.044338\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.098291\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.012298\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.011569\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.191173\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.105874\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.017934\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.110430\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.005936\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.072029\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.032959\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.148008\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.018507\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.030848\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.014975\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.043929\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.132162\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.048385\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.149542\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.045868\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.027786\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.048921\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.027421\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.019396\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.067312\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.020773\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.029806\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.111132\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.090036\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.084270\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.023355\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.020127\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.043705\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.092119\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.076125\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.150198\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.094649\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.026988\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.042982\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.118604\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.040683\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.099472\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.037486\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.032848\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.033039\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.083265\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.096330\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.010014\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.029371\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.022150\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.037053\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.031236\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.050595\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.052013\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.111597\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.016066\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.016901\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.033329\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.049802\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.023579\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.173539\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.087104\n",
            "\n",
            "Test set: Average loss: 0.0549, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.127672\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.202532\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.022955\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.032389\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.007572\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.193895\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.069649\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.179383\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.058832\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.097482\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.017884\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.176958\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.053860\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.013468\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.021107\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.099652\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.018896\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.054864\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.095359\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.084860\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.084987\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.069418\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.017408\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.031205\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.087864\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.097930\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.023624\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.005688\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.274773\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.066503\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.086262\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.075671\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.022677\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.097955\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.021794\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.169310\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.087613\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.016872\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.076259\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.008382\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.036698\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.052889\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.072178\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.031954\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.016110\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.096182\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.057870\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.046824\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.042347\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.033402\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.054273\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.066649\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.068697\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.059286\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.033117\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.102927\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.046512\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.330014\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.014640\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.026258\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.061865\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.190669\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.033769\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.037984\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.045186\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.075188\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.019668\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.027806\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.068848\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.213631\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.078128\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.036277\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.034206\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.042864\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.062498\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.120750\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.022584\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.011331\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.093105\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.120024\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.016439\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.118085\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.043390\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.056637\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.039984\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.081245\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.280435\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.170155\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.030521\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.036055\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.097102\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.040784\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.015552\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.059676\n",
            "\n",
            "Test set: Average loss: 0.0568, Accuracy: 9827/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.151836\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.010434\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.075998\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.056641\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.045840\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.069646\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.064543\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.104351\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.019209\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.085655\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.055254\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.025202\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.052933\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.037534\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.051670\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.013802\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.074952\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.007263\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.129477\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.056262\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.135240\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.035439\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.062021\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.095863\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.110817\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.091029\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.063498\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.034381\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.015273\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.085895\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.010509\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.032527\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.141700\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.097264\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.081424\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.029708\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.055619\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.056925\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.019659\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.148656\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.054919\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.014911\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.049819\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.034298\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.023184\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.183752\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.030180\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.030490\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.048902\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.071257\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.031989\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.041681\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.053618\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.005315\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.010728\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.111617\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.008264\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.031674\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.121729\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.008633\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.043065\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.063868\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.030068\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.073204\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.144599\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.027654\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.012021\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.081229\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.036009\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.018134\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.056363\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.011867\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.014383\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.072351\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.125168\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.119542\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.006497\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.021713\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.073818\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.014266\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.102566\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.014307\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.075753\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.032020\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.062140\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.230007\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.012709\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.121457\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.049525\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.043096\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.052994\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.073620\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.015399\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.098005\n",
            "\n",
            "Test set: Average loss: 0.0490, Accuracy: 9833/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.004632\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.009773\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.013988\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.071117\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.023527\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.015872\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.065239\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.111476\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.157700\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.085507\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.123256\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.122134\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.032843\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.016835\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.074333\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.059308\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.028213\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.020966\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.049138\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.023282\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.081533\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.028631\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.041648\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.028422\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.062683\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.003376\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.013348\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.151269\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.006951\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.018677\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.042091\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.145151\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.022517\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.040438\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.056427\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.110587\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.021259\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.018494\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.044268\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.042477\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.115733\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.047378\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.133905\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.086241\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.185368\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.185921\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.065395\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.065530\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.023341\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.072765\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.026074\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.057111\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.169073\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.029049\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.019635\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.033257\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.064287\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.039886\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.028176\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.018496\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.029897\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.016156\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.065155\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.077498\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.068339\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.047729\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.057508\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.004962\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.010142\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.067351\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.037926\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.053699\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.035115\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.011987\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.021959\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.206832\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.078541\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.145425\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.088827\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.120483\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.014072\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.004861\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.033904\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.017640\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.072985\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.114224\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.012032\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.020400\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.123867\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.061499\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.005166\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.129902\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.007102\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.092676\n",
            "\n",
            "Test set: Average loss: 0.0470, Accuracy: 9845/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# One hot encoding for each char in 'hello'\n",
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n",
        "\n",
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5\n",
        "cell = nn.RNN(input_size=4, hidden_size=2, batch_first=True)\n",
        "\n",
        "# (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
        "hidden = Variable(torch.randn(1, 1, 2))\n",
        "\n",
        "# Propagate input through RNN\n",
        "# Input: (batch, seq_len, input_size) when batch_first=True\n",
        "inputs = Variable(torch.Tensor([h, e, l, l, o]))\n",
        "for one in inputs:\n",
        "    one = one.view(1, 1, -1)\n",
        "    # Input: (batch, seq_len, input_size) when batch_first=True\n",
        "    out, hidden = cell(one, hidden)\n",
        "    print(\"one input size\", one.size(), \"out size\", out.size())\n",
        "\n",
        "# We can do the whole at once\n",
        "# Propagate input through RNN\n",
        "# Input: (batch, seq_len, input_size) when batch_first=True\n",
        "inputs = inputs.view(1, 5, -1)\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"sequence input size\", inputs.size(), \"out size\", out.size())\n",
        "\n",
        "\n",
        "# hidden : (num_layers * num_directions, batch, hidden_size) whether batch_first=True or False\n",
        "hidden = Variable(torch.randn(1, 3, 2))\n",
        "\n",
        "# One cell RNN input_dim (4) -> output_dim (2). sequence: 5, batch 3\n",
        "# 3 batches 'hello', 'eolll', 'lleel'\n",
        "# rank = (3, 5, 4)\n",
        "inputs = Variable(torch.Tensor([[h, e, l, l, o],\n",
        "                                [e, o, l, l, l],\n",
        "                                [l, l, e, e, l]]))\n",
        "\n",
        "# Propagate input through RNN\n",
        "# Input: (batch, seq_len, input_size) when batch_first=True\n",
        "# B x S x I\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"out size\", out.size())\n",
        "\n",
        "\n",
        "# One cell RNN input_dim (4) -> output_dim (2)\n",
        "cell = nn.RNN(input_size=4, hidden_size=2)\n",
        "\n",
        "# The given dimensions dim0 and dim1 are swapped.\n",
        "inputs = inputs.transpose(dim0=0, dim1=1)\n",
        "# Propagate input through RNN\n",
        "# Input: (seq_len, batch_size, input_size) when batch_first=False (default)\n",
        "# S x B x I\n",
        "out, hidden = cell(inputs, hidden)\n",
        "print(\"batch input size\", inputs.size(), \"out size\", out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzsyZac88-5j",
        "outputId": "c4140bc2-e063-400a-d567-2c72b79c7bc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
            "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
            "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
            "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
            "one input size torch.Size([1, 1, 4]) out size torch.Size([1, 1, 2])\n",
            "sequence input size torch.Size([1, 5, 4]) out size torch.Size([1, 5, 2])\n",
            "batch input size torch.Size([3, 5, 4]) out size torch.Size([3, 5, 2])\n",
            "batch input size torch.Size([5, 3, 4]) out size torch.Size([5, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 12 RNN\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)  # reproducibility\n",
        "#            0    1    2    3    4\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "# Teach hihell -> ihello\n",
        "x_data = [0, 1, 0, 2, 3, 3]   # hihell\n",
        "one_hot_lookup = [[1, 0, 0, 0, 0],  # 0\n",
        "                  [0, 1, 0, 0, 0],  # 1\n",
        "                  [0, 0, 1, 0, 0],  # 2\n",
        "                  [0, 0, 0, 1, 0],  # 3\n",
        "                  [0, 0, 0, 0, 1]]  # 4\n",
        "\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
        "\n",
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.Tensor(x_one_hot))\n",
        "labels = Variable(torch.LongTensor(y_data))\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5  # one-hot size\n",
        "hidden_size = 5  # output from the RNN. 5 to directly predict one-hot\n",
        "batch_size = 1   # one sentence\n",
        "sequence_length = 1  # One by one\n",
        "num_layers = 1  # one-layer rnn\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size=input_size,\n",
        "                          hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, hidden, x):\n",
        "        # Reshape input (batch first)\n",
        "        x = x.view(batch_size, sequence_length, input_size)\n",
        "\n",
        "        # Propagate input through RNN\n",
        "        # Input: (batch, seq_len, input_size)\n",
        "        # hidden: (num_layers * num_directions, batch, hidden_size)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        return hidden, out.view(-1, num_classes)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
        "\n",
        "\n",
        "# Instantiate RNN model\n",
        "model = Model()\n",
        "print(model)\n",
        "\n",
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    hidden = model.init_hidden()\n",
        "\n",
        "    sys.stdout.write(\"predicted string: \")\n",
        "    for input, label in zip(inputs, labels):\n",
        "        # print(input.size(), label.size())\n",
        "        hidden, output = model(hidden, input)\n",
        "        val, idx = output.max(1)\n",
        "        sys.stdout.write(idx2char[idx.data[0]])\n",
        "        loss += criterion(output, torch.LongTensor([label]))\n",
        "\n",
        "    print(\", epoch: %d, loss: %1.3f\" % (epoch + 1, loss))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Learning finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8gROvJH9JDe",
        "outputId": "b233a9ea-d59d-49aa-edcb-0481cc50da38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (rnn): RNN(5, 5, batch_first=True)\n",
            ")\n",
            "predicted string: llllll, epoch: 1, loss: 10.155\n",
            "predicted string: llllll, epoch: 2, loss: 9.137\n",
            "predicted string: llllll, epoch: 3, loss: 8.355\n",
            "predicted string: llllll, epoch: 4, loss: 7.577\n",
            "predicted string: llllll, epoch: 5, loss: 6.876\n",
            "predicted string: lhelll, epoch: 6, loss: 6.327\n",
            "predicted string: ihelll, epoch: 7, loss: 6.014\n",
            "predicted string: ihelll, epoch: 8, loss: 5.787\n",
            "predicted string: ihelll, epoch: 9, loss: 5.477\n",
            "predicted string: ihelll, epoch: 10, loss: 5.274\n",
            "predicted string: ihelll, epoch: 11, loss: 5.041\n",
            "predicted string: ihello, epoch: 12, loss: 4.827\n",
            "predicted string: ihello, epoch: 13, loss: 4.676\n",
            "predicted string: ihello, epoch: 14, loss: 4.550\n",
            "predicted string: ihello, epoch: 15, loss: 4.430\n",
            "predicted string: ihello, epoch: 16, loss: 4.305\n",
            "predicted string: ihello, epoch: 17, loss: 4.164\n",
            "predicted string: ihelll, epoch: 18, loss: 4.003\n",
            "predicted string: ihelll, epoch: 19, loss: 3.860\n",
            "predicted string: ihelll, epoch: 20, loss: 3.879\n",
            "predicted string: ihelll, epoch: 21, loss: 3.768\n",
            "predicted string: ihelll, epoch: 22, loss: 3.642\n",
            "predicted string: ihelll, epoch: 23, loss: 3.599\n",
            "predicted string: ihello, epoch: 24, loss: 3.577\n",
            "predicted string: ihello, epoch: 25, loss: 3.544\n",
            "predicted string: ihello, epoch: 26, loss: 3.498\n",
            "predicted string: ihello, epoch: 27, loss: 3.439\n",
            "predicted string: ihello, epoch: 28, loss: 3.371\n",
            "predicted string: ihello, epoch: 29, loss: 3.303\n",
            "predicted string: ihello, epoch: 30, loss: 3.240\n",
            "predicted string: ihello, epoch: 31, loss: 3.162\n",
            "predicted string: ihello, epoch: 32, loss: 3.147\n",
            "predicted string: ihello, epoch: 33, loss: 3.178\n",
            "predicted string: ihello, epoch: 34, loss: 3.116\n",
            "predicted string: ihello, epoch: 35, loss: 3.042\n",
            "predicted string: ihello, epoch: 36, loss: 3.020\n",
            "predicted string: ihello, epoch: 37, loss: 3.015\n",
            "predicted string: ihello, epoch: 38, loss: 2.998\n",
            "predicted string: ihello, epoch: 39, loss: 2.977\n",
            "predicted string: ihello, epoch: 40, loss: 2.966\n",
            "predicted string: ihello, epoch: 41, loss: 2.961\n",
            "predicted string: ihello, epoch: 42, loss: 2.950\n",
            "predicted string: ihello, epoch: 43, loss: 2.930\n",
            "predicted string: ihello, epoch: 44, loss: 2.904\n",
            "predicted string: ihello, epoch: 45, loss: 2.888\n",
            "predicted string: ihello, epoch: 46, loss: 2.888\n",
            "predicted string: ihello, epoch: 47, loss: 2.879\n",
            "predicted string: ihello, epoch: 48, loss: 2.860\n",
            "predicted string: ihello, epoch: 49, loss: 2.857\n",
            "predicted string: ihello, epoch: 50, loss: 2.859\n",
            "predicted string: ihello, epoch: 51, loss: 2.852\n",
            "predicted string: ihello, epoch: 52, loss: 2.840\n",
            "predicted string: ihello, epoch: 53, loss: 2.834\n",
            "predicted string: ihello, epoch: 54, loss: 2.834\n",
            "predicted string: ihello, epoch: 55, loss: 2.824\n",
            "predicted string: ihello, epoch: 56, loss: 2.817\n",
            "predicted string: ihello, epoch: 57, loss: 2.817\n",
            "predicted string: ihello, epoch: 58, loss: 2.814\n",
            "predicted string: ihello, epoch: 59, loss: 2.808\n",
            "predicted string: ihello, epoch: 60, loss: 2.805\n",
            "predicted string: ihello, epoch: 61, loss: 2.805\n",
            "predicted string: ihello, epoch: 62, loss: 2.801\n",
            "predicted string: ihello, epoch: 63, loss: 2.796\n",
            "predicted string: ihello, epoch: 64, loss: 2.795\n",
            "predicted string: ihello, epoch: 65, loss: 2.793\n",
            "predicted string: ihello, epoch: 66, loss: 2.789\n",
            "predicted string: ihello, epoch: 67, loss: 2.786\n",
            "predicted string: ihello, epoch: 68, loss: 2.786\n",
            "predicted string: ihello, epoch: 69, loss: 2.783\n",
            "predicted string: ihello, epoch: 70, loss: 2.780\n",
            "predicted string: ihello, epoch: 71, loss: 2.780\n",
            "predicted string: ihello, epoch: 72, loss: 2.778\n",
            "predicted string: ihello, epoch: 73, loss: 2.776\n",
            "predicted string: ihello, epoch: 74, loss: 2.775\n",
            "predicted string: ihello, epoch: 75, loss: 2.774\n",
            "predicted string: ihello, epoch: 76, loss: 2.772\n",
            "predicted string: ihello, epoch: 77, loss: 2.770\n",
            "predicted string: ihello, epoch: 78, loss: 2.769\n",
            "predicted string: ihello, epoch: 79, loss: 2.768\n",
            "predicted string: ihello, epoch: 80, loss: 2.766\n",
            "predicted string: ihello, epoch: 81, loss: 2.765\n",
            "predicted string: ihello, epoch: 82, loss: 2.764\n",
            "predicted string: ihello, epoch: 83, loss: 2.763\n",
            "predicted string: ihello, epoch: 84, loss: 2.762\n",
            "predicted string: ihello, epoch: 85, loss: 2.761\n",
            "predicted string: ihello, epoch: 86, loss: 2.759\n",
            "predicted string: ihello, epoch: 87, loss: 2.759\n",
            "predicted string: ihello, epoch: 88, loss: 2.758\n",
            "predicted string: ihello, epoch: 89, loss: 2.757\n",
            "predicted string: ihello, epoch: 90, loss: 2.756\n",
            "predicted string: ihello, epoch: 91, loss: 2.755\n",
            "predicted string: ihello, epoch: 92, loss: 2.754\n",
            "predicted string: ihello, epoch: 93, loss: 2.753\n",
            "predicted string: ihello, epoch: 94, loss: 2.752\n",
            "predicted string: ihello, epoch: 95, loss: 2.751\n",
            "predicted string: ihello, epoch: 96, loss: 2.750\n",
            "predicted string: ihello, epoch: 97, loss: 2.750\n",
            "predicted string: ihello, epoch: 98, loss: 2.749\n",
            "predicted string: ihello, epoch: 99, loss: 2.748\n",
            "predicted string: ihello, epoch: 100, loss: 2.747\n",
            "Learning finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 12 RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(777)  # reproducibility\n",
        "\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "# Teach hihell -> ihello\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
        "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 1, 0, 0, 0],   # i 1\n",
        "              [1, 0, 0, 0, 0],   # h 0\n",
        "              [0, 0, 1, 0, 0],   # e 2\n",
        "              [0, 0, 0, 1, 0],   # l 3\n",
        "              [0, 0, 0, 1, 0]]]  # l 3\n",
        "\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "\n",
        "inputs = torch.Tensor(x_one_hot)\n",
        "labels = torch.LongTensor(y_data)\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5  # one-hot size\n",
        "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
        "batch_size = 1   # one sentence\n",
        "sequence_length = 6  # |ihello| == 6\n",
        "num_layers = 1  # one-layer rnn\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "\n",
        "        # forward propagate RNN\n",
        "        out, _ = self.rnn(x, h_0)\n",
        "        out = out.contiguous().view(-1, self.hidden_size)\n",
        "\n",
        "        # fully connected (to get logits for each character)\n",
        "        # note: since hidden_size == num_classes, no extra linear layer is needed\n",
        "        return out\n",
        "\n",
        "\n",
        "# Instantiate RNN model\n",
        "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
        "print(rnn)\n",
        "\n",
        "# Set loss and optimizer function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    outputs = rnn(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.detach().numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "    print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhzJrdSm9VlD",
        "outputId": "bd56d6e0-5ebe-41e4-c45e-62c973e3aafd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(5, 5, batch_first=True)\n",
            ")\n",
            "epoch: 1, loss: 1.693\n",
            "Predicted string:  llllll\n",
            "epoch: 2, loss: 1.523\n",
            "Predicted string:  llllll\n",
            "epoch: 3, loss: 1.393\n",
            "Predicted string:  llllll\n",
            "epoch: 4, loss: 1.263\n",
            "Predicted string:  llllll\n",
            "epoch: 5, loss: 1.146\n",
            "Predicted string:  llllll\n",
            "epoch: 6, loss: 1.055\n",
            "Predicted string:  lhelll\n",
            "epoch: 7, loss: 1.002\n",
            "Predicted string:  ihelll\n",
            "epoch: 8, loss: 0.965\n",
            "Predicted string:  ihelll\n",
            "epoch: 9, loss: 0.913\n",
            "Predicted string:  ihelll\n",
            "epoch: 10, loss: 0.879\n",
            "Predicted string:  ihelll\n",
            "epoch: 11, loss: 0.840\n",
            "Predicted string:  ihelll\n",
            "epoch: 12, loss: 0.805\n",
            "Predicted string:  ihello\n",
            "epoch: 13, loss: 0.779\n",
            "Predicted string:  ihello\n",
            "epoch: 14, loss: 0.758\n",
            "Predicted string:  ihello\n",
            "epoch: 15, loss: 0.738\n",
            "Predicted string:  ihello\n",
            "epoch: 16, loss: 0.717\n",
            "Predicted string:  ihello\n",
            "epoch: 17, loss: 0.694\n",
            "Predicted string:  ihello\n",
            "epoch: 18, loss: 0.667\n",
            "Predicted string:  ihelll\n",
            "epoch: 19, loss: 0.643\n",
            "Predicted string:  ihelll\n",
            "epoch: 20, loss: 0.647\n",
            "Predicted string:  ihelll\n",
            "epoch: 21, loss: 0.628\n",
            "Predicted string:  ihelll\n",
            "epoch: 22, loss: 0.607\n",
            "Predicted string:  ihelll\n",
            "epoch: 23, loss: 0.600\n",
            "Predicted string:  ihelll\n",
            "epoch: 24, loss: 0.596\n",
            "Predicted string:  ihello\n",
            "epoch: 25, loss: 0.591\n",
            "Predicted string:  ihello\n",
            "epoch: 26, loss: 0.583\n",
            "Predicted string:  ihello\n",
            "epoch: 27, loss: 0.573\n",
            "Predicted string:  ihello\n",
            "epoch: 28, loss: 0.562\n",
            "Predicted string:  ihello\n",
            "epoch: 29, loss: 0.550\n",
            "Predicted string:  ihello\n",
            "epoch: 30, loss: 0.540\n",
            "Predicted string:  ihello\n",
            "epoch: 31, loss: 0.527\n",
            "Predicted string:  ihello\n",
            "epoch: 32, loss: 0.524\n",
            "Predicted string:  ihello\n",
            "epoch: 33, loss: 0.530\n",
            "Predicted string:  ihello\n",
            "epoch: 34, loss: 0.519\n",
            "Predicted string:  ihello\n",
            "epoch: 35, loss: 0.507\n",
            "Predicted string:  ihello\n",
            "epoch: 36, loss: 0.503\n",
            "Predicted string:  ihello\n",
            "epoch: 37, loss: 0.503\n",
            "Predicted string:  ihello\n",
            "epoch: 38, loss: 0.500\n",
            "Predicted string:  ihello\n",
            "epoch: 39, loss: 0.496\n",
            "Predicted string:  ihello\n",
            "epoch: 40, loss: 0.494\n",
            "Predicted string:  ihello\n",
            "epoch: 41, loss: 0.493\n",
            "Predicted string:  ihello\n",
            "epoch: 42, loss: 0.492\n",
            "Predicted string:  ihello\n",
            "epoch: 43, loss: 0.488\n",
            "Predicted string:  ihello\n",
            "epoch: 44, loss: 0.484\n",
            "Predicted string:  ihello\n",
            "epoch: 45, loss: 0.481\n",
            "Predicted string:  ihello\n",
            "epoch: 46, loss: 0.481\n",
            "Predicted string:  ihello\n",
            "epoch: 47, loss: 0.480\n",
            "Predicted string:  ihello\n",
            "epoch: 48, loss: 0.477\n",
            "Predicted string:  ihello\n",
            "epoch: 49, loss: 0.476\n",
            "Predicted string:  ihello\n",
            "epoch: 50, loss: 0.476\n",
            "Predicted string:  ihello\n",
            "epoch: 51, loss: 0.475\n",
            "Predicted string:  ihello\n",
            "epoch: 52, loss: 0.473\n",
            "Predicted string:  ihello\n",
            "epoch: 53, loss: 0.472\n",
            "Predicted string:  ihello\n",
            "epoch: 54, loss: 0.472\n",
            "Predicted string:  ihello\n",
            "epoch: 55, loss: 0.471\n",
            "Predicted string:  ihello\n",
            "epoch: 56, loss: 0.469\n",
            "Predicted string:  ihello\n",
            "epoch: 57, loss: 0.469\n",
            "Predicted string:  ihello\n",
            "epoch: 58, loss: 0.469\n",
            "Predicted string:  ihello\n",
            "epoch: 59, loss: 0.468\n",
            "Predicted string:  ihello\n",
            "epoch: 60, loss: 0.467\n",
            "Predicted string:  ihello\n",
            "epoch: 61, loss: 0.467\n",
            "Predicted string:  ihello\n",
            "epoch: 62, loss: 0.467\n",
            "Predicted string:  ihello\n",
            "epoch: 63, loss: 0.466\n",
            "Predicted string:  ihello\n",
            "epoch: 64, loss: 0.466\n",
            "Predicted string:  ihello\n",
            "epoch: 65, loss: 0.466\n",
            "Predicted string:  ihello\n",
            "epoch: 66, loss: 0.465\n",
            "Predicted string:  ihello\n",
            "epoch: 67, loss: 0.464\n",
            "Predicted string:  ihello\n",
            "epoch: 68, loss: 0.464\n",
            "Predicted string:  ihello\n",
            "epoch: 69, loss: 0.464\n",
            "Predicted string:  ihello\n",
            "epoch: 70, loss: 0.463\n",
            "Predicted string:  ihello\n",
            "epoch: 71, loss: 0.463\n",
            "Predicted string:  ihello\n",
            "epoch: 72, loss: 0.463\n",
            "Predicted string:  ihello\n",
            "epoch: 73, loss: 0.463\n",
            "Predicted string:  ihello\n",
            "epoch: 74, loss: 0.462\n",
            "Predicted string:  ihello\n",
            "epoch: 75, loss: 0.462\n",
            "Predicted string:  ihello\n",
            "epoch: 76, loss: 0.462\n",
            "Predicted string:  ihello\n",
            "epoch: 77, loss: 0.462\n",
            "Predicted string:  ihello\n",
            "epoch: 78, loss: 0.462\n",
            "Predicted string:  ihello\n",
            "epoch: 79, loss: 0.461\n",
            "Predicted string:  ihello\n",
            "epoch: 80, loss: 0.461\n",
            "Predicted string:  ihello\n",
            "epoch: 81, loss: 0.461\n",
            "Predicted string:  ihello\n",
            "epoch: 82, loss: 0.461\n",
            "Predicted string:  ihello\n",
            "epoch: 83, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 84, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 85, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 86, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 87, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 88, loss: 0.460\n",
            "Predicted string:  ihello\n",
            "epoch: 89, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 90, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 91, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 92, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 93, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 94, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 95, loss: 0.459\n",
            "Predicted string:  ihello\n",
            "epoch: 96, loss: 0.458\n",
            "Predicted string:  ihello\n",
            "epoch: 97, loss: 0.458\n",
            "Predicted string:  ihello\n",
            "epoch: 98, loss: 0.458\n",
            "Predicted string:  ihello\n",
            "epoch: 99, loss: 0.458\n",
            "Predicted string:  ihello\n",
            "epoch: 100, loss: 0.458\n",
            "Predicted string:  ihello\n",
            "Learning finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lab 12 RNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.manual_seed(777)  # reproducibility\n",
        "\n",
        "\n",
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "\n",
        "# Teach hihell -> ihello\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
        "\n",
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.LongTensor(x_data))\n",
        "labels = Variable(torch.LongTensor(y_data))\n",
        "\n",
        "num_classes = 5\n",
        "input_size = 5\n",
        "embedding_size = 10  # embedding size\n",
        "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
        "batch_size = 1   # one sentence\n",
        "sequence_length = 6  # |ihello| == 6\n",
        "num_layers = 1  # one-layer rnn\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(input_size=embedding_size,\n",
        "                          hidden_size=5, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(batch_size, sequence_length, -1)\n",
        "\n",
        "        # Propagate embedding through RNN\n",
        "        # Input: (batch, seq_len, embedding_size)\n",
        "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
        "        out, _ = self.rnn(emb, h_0)\n",
        "        return self.fc(out.view(-1, num_classes))\n",
        "\n",
        "\n",
        "# Instantiate RNN model\n",
        "model = Model(num_layers, hidden_size)\n",
        "print(model)\n",
        "\n",
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    outputs = model(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "    print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPMAlf099j4h",
        "outputId": "b22cdee0-76b8-43e6-ab0d-89316e434243"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (embedding): Embedding(5, 10)\n",
            "  (rnn): RNN(10, 5, batch_first=True)\n",
            "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
            ")\n",
            "epoch: 1, loss: 1.768\n",
            "Predicted string:  eheohh\n",
            "epoch: 2, loss: 1.396\n",
            "Predicted string:  oheiii\n",
            "epoch: 3, loss: 1.132\n",
            "Predicted string:  iheloo\n",
            "epoch: 4, loss: 0.949\n",
            "Predicted string:  ihello\n",
            "epoch: 5, loss: 0.798\n",
            "Predicted string:  ihelll\n",
            "epoch: 6, loss: 0.659\n",
            "Predicted string:  ihelll\n",
            "epoch: 7, loss: 0.547\n",
            "Predicted string:  ihelll\n",
            "epoch: 8, loss: 0.449\n",
            "Predicted string:  ihelll\n",
            "epoch: 9, loss: 0.384\n",
            "Predicted string:  ihelll\n",
            "epoch: 10, loss: 0.343\n",
            "Predicted string:  ihello\n",
            "epoch: 11, loss: 0.304\n",
            "Predicted string:  ihello\n",
            "epoch: 12, loss: 0.235\n",
            "Predicted string:  ihello\n",
            "epoch: 13, loss: 0.227\n",
            "Predicted string:  ihello\n",
            "epoch: 14, loss: 0.239\n",
            "Predicted string:  iheloo\n",
            "epoch: 15, loss: 0.233\n",
            "Predicted string:  iheloo\n",
            "epoch: 16, loss: 0.146\n",
            "Predicted string:  ihello\n",
            "epoch: 17, loss: 0.320\n",
            "Predicted string:  ihelll\n",
            "epoch: 18, loss: 0.139\n",
            "Predicted string:  ihello\n",
            "epoch: 19, loss: 0.228\n",
            "Predicted string:  iheloo\n",
            "epoch: 20, loss: 0.242\n",
            "Predicted string:  iheloo\n",
            "epoch: 21, loss: 0.191\n",
            "Predicted string:  iheloo\n",
            "epoch: 22, loss: 0.115\n",
            "Predicted string:  ihello\n",
            "epoch: 23, loss: 0.157\n",
            "Predicted string:  ihelll\n",
            "epoch: 24, loss: 0.133\n",
            "Predicted string:  ihello\n",
            "epoch: 25, loss: 0.105\n",
            "Predicted string:  ihello\n",
            "epoch: 26, loss: 0.127\n",
            "Predicted string:  ihello\n",
            "epoch: 27, loss: 0.135\n",
            "Predicted string:  ihello\n",
            "epoch: 28, loss: 0.109\n",
            "Predicted string:  ihello\n",
            "epoch: 29, loss: 0.078\n",
            "Predicted string:  ihello\n",
            "epoch: 30, loss: 0.099\n",
            "Predicted string:  ihello\n",
            "epoch: 31, loss: 0.074\n",
            "Predicted string:  ihello\n",
            "epoch: 32, loss: 0.061\n",
            "Predicted string:  ihello\n",
            "epoch: 33, loss: 0.074\n",
            "Predicted string:  ihello\n",
            "epoch: 34, loss: 0.063\n",
            "Predicted string:  ihello\n",
            "epoch: 35, loss: 0.045\n",
            "Predicted string:  ihello\n",
            "epoch: 36, loss: 0.062\n",
            "Predicted string:  ihello\n",
            "epoch: 37, loss: 0.039\n",
            "Predicted string:  ihello\n",
            "epoch: 38, loss: 0.045\n",
            "Predicted string:  ihello\n",
            "epoch: 39, loss: 0.047\n",
            "Predicted string:  ihello\n",
            "epoch: 40, loss: 0.034\n",
            "Predicted string:  ihello\n",
            "epoch: 41, loss: 0.038\n",
            "Predicted string:  ihello\n",
            "epoch: 42, loss: 0.034\n",
            "Predicted string:  ihello\n",
            "epoch: 43, loss: 0.030\n",
            "Predicted string:  ihello\n",
            "epoch: 44, loss: 0.034\n",
            "Predicted string:  ihello\n",
            "epoch: 45, loss: 0.029\n",
            "Predicted string:  ihello\n",
            "epoch: 46, loss: 0.026\n",
            "Predicted string:  ihello\n",
            "epoch: 47, loss: 0.030\n",
            "Predicted string:  ihello\n",
            "epoch: 48, loss: 0.024\n",
            "Predicted string:  ihello\n",
            "epoch: 49, loss: 0.025\n",
            "Predicted string:  ihello\n",
            "epoch: 50, loss: 0.025\n",
            "Predicted string:  ihello\n",
            "epoch: 51, loss: 0.022\n",
            "Predicted string:  ihello\n",
            "epoch: 52, loss: 0.022\n",
            "Predicted string:  ihello\n",
            "epoch: 53, loss: 0.022\n",
            "Predicted string:  ihello\n",
            "epoch: 54, loss: 0.019\n",
            "Predicted string:  ihello\n",
            "epoch: 55, loss: 0.020\n",
            "Predicted string:  ihello\n",
            "epoch: 56, loss: 0.020\n",
            "Predicted string:  ihello\n",
            "epoch: 57, loss: 0.018\n",
            "Predicted string:  ihello\n",
            "epoch: 58, loss: 0.018\n",
            "Predicted string:  ihello\n",
            "epoch: 59, loss: 0.018\n",
            "Predicted string:  ihello\n",
            "epoch: 60, loss: 0.016\n",
            "Predicted string:  ihello\n",
            "epoch: 61, loss: 0.016\n",
            "Predicted string:  ihello\n",
            "epoch: 62, loss: 0.016\n",
            "Predicted string:  ihello\n",
            "epoch: 63, loss: 0.015\n",
            "Predicted string:  ihello\n",
            "epoch: 64, loss: 0.015\n",
            "Predicted string:  ihello\n",
            "epoch: 65, loss: 0.015\n",
            "Predicted string:  ihello\n",
            "epoch: 66, loss: 0.014\n",
            "Predicted string:  ihello\n",
            "epoch: 67, loss: 0.014\n",
            "Predicted string:  ihello\n",
            "epoch: 68, loss: 0.014\n",
            "Predicted string:  ihello\n",
            "epoch: 69, loss: 0.013\n",
            "Predicted string:  ihello\n",
            "epoch: 70, loss: 0.013\n",
            "Predicted string:  ihello\n",
            "epoch: 71, loss: 0.013\n",
            "Predicted string:  ihello\n",
            "epoch: 72, loss: 0.013\n",
            "Predicted string:  ihello\n",
            "epoch: 73, loss: 0.012\n",
            "Predicted string:  ihello\n",
            "epoch: 74, loss: 0.012\n",
            "Predicted string:  ihello\n",
            "epoch: 75, loss: 0.012\n",
            "Predicted string:  ihello\n",
            "epoch: 76, loss: 0.012\n",
            "Predicted string:  ihello\n",
            "epoch: 77, loss: 0.011\n",
            "Predicted string:  ihello\n",
            "epoch: 78, loss: 0.011\n",
            "Predicted string:  ihello\n",
            "epoch: 79, loss: 0.011\n",
            "Predicted string:  ihello\n",
            "epoch: 80, loss: 0.011\n",
            "Predicted string:  ihello\n",
            "epoch: 81, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 82, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 83, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 84, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 85, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 86, loss: 0.009\n",
            "Predicted string:  ihello\n",
            "epoch: 87, loss: 0.009\n",
            "Predicted string:  ihello\n",
            "epoch: 88, loss: 0.009\n",
            "Predicted string:  ihello\n",
            "epoch: 89, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 90, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 91, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 92, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 93, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 94, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 95, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 96, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 97, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 98, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 99, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 100, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "Learning finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code is from https://github.com/spro/practical-pytorch\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "HIDDEN_SIZE = 100\n",
        "N_CHARS = 128  # ASCII\n",
        "N_CLASSES = 18\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Note: we run this all at once (over the whole input sequence)\n",
        "\n",
        "        # input = B x S . size(0) = B\n",
        "        batch_size = input.size(0)\n",
        "\n",
        "        # input:  B x S  -- (transpose) --> S x B\n",
        "        input = input.t()\n",
        "\n",
        "        # Embedding S x B -> S x B x I (embedding size)\n",
        "        print(\"  input\", input.size())\n",
        "        embedded = self.embedding(input)\n",
        "        print(\"  embedding\", embedded.size())\n",
        "\n",
        "        # Make a hidden\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        print(\"  gru hidden output\", hidden.size())\n",
        "        # Use the last layer output as FC's input\n",
        "        # No need to unpack, since we are going to use hidden\n",
        "        fc_output = self.fc(hidden)\n",
        "        print(\"  fc output\", fc_output.size())\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "        return Variable(hidden)\n",
        "\n",
        "# Help functions\n",
        "\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "# pad sequences and sort the tensor\n",
        "def pad_sequences(vectorized_seqs, seq_lengths):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "\n",
        "# Create necessary variables, lengths, and target\n",
        "def make_variables(names):\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    names = ['adylov', 'solan', 'hard', 'san']\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
        "\n",
        "    for name in names:\n",
        "        arr, _ = str2ascii_arr(name)\n",
        "        inp = Variable(torch.LongTensor([arr]))\n",
        "        out = classifier(inp)\n",
        "        print(\"in\", inp.size(), \"out\", out.size())\n",
        "\n",
        "\n",
        "    inputs = make_variables(names)\n",
        "    out = classifier(inputs)\n",
        "    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l65z8XLI82J",
        "outputId": "0202e06a-be4d-4760-dd53-6b546319aa4c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  input torch.Size([6, 1])\n",
            "  embedding torch.Size([6, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 6]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([5, 1])\n",
            "  embedding torch.Size([5, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 5]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([4, 1])\n",
            "  embedding torch.Size([4, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 4]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([3, 1])\n",
            "  embedding torch.Size([3, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 3]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([6, 4])\n",
            "  embedding torch.Size([6, 4, 100])\n",
            "  gru hidden output torch.Size([1, 4, 100])\n",
            "  fc output torch.Size([1, 4, 18])\n",
            "batch in torch.Size([4, 6]) batch out torch.Size([1, 4, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code is from https://github.com/spro/practical-pytorch\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "HIDDEN_SIZE = 100\n",
        "N_CHARS = 128  # ASCII\n",
        "N_CLASSES = 18\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Note: we run this all at once (over the whole input sequence)\n",
        "\n",
        "        # input = B x S . size(0) = B\n",
        "        batch_size = input.size(0)\n",
        "\n",
        "        # input:  B x S  -- (transpose) --> S x B\n",
        "        input = input.t()\n",
        "\n",
        "        # Embedding S x B -> S x B x I (embedding size)\n",
        "        print(\"  input\", input.size())\n",
        "        embedded = self.embedding(input)\n",
        "        print(\"  embedding\", embedded.size())\n",
        "\n",
        "        # Make a hidden\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        print(\"  gru hidden output\", hidden.size())\n",
        "        # Use the last layer output as FC's input\n",
        "        # No need to unpack, since we are going to use hidden\n",
        "        fc_output = self.fc(hidden)\n",
        "        print(\"  fc output\", fc_output.size())\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
        "        return Variable(hidden)\n",
        "\n",
        "# Help functions\n",
        "\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "# pad sequences and sort the tensor\n",
        "def pad_sequences(vectorized_seqs, seq_lengths):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "    return seq_tensor\n",
        "\n",
        "# Create necessary variables, lengths, and target\n",
        "def make_variables(names):\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    names = ['adylov', 'solan', 'hard', 'san']\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_CLASSES)\n",
        "\n",
        "    for name in names:\n",
        "        arr, _ = str2ascii_arr(name)\n",
        "        inp = Variable(torch.LongTensor([arr]))\n",
        "        out = classifier(inp)\n",
        "        print(\"in\", inp.size(), \"out\", out.size())\n",
        "\n",
        "\n",
        "    inputs = make_variables(names)\n",
        "    out = classifier(inputs)\n",
        "    print(\"batch in\", inputs.size(), \"batch out\", out.size())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ovg2IyZJdQb",
        "outputId": "3b214ec7-12fd-4f25-a047-18aeaca51faa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  input torch.Size([6, 1])\n",
            "  embedding torch.Size([6, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 6]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([5, 1])\n",
            "  embedding torch.Size([5, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 5]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([4, 1])\n",
            "  embedding torch.Size([4, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 4]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([3, 1])\n",
            "  embedding torch.Size([3, 1, 100])\n",
            "  gru hidden output torch.Size([1, 1, 100])\n",
            "  fc output torch.Size([1, 1, 18])\n",
            "in torch.Size([1, 3]) out torch.Size([1, 1, 18])\n",
            "  input torch.Size([6, 4])\n",
            "  embedding torch.Size([6, 4, 100])\n",
            "  gru hidden output torch.Size([1, 4, 100])\n",
            "  fc output torch.Size([1, 4, 18])\n",
            "batch in torch.Size([4, 6]) batch out torch.Size([1, 4, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original code is from https://github.com/spro/practical-pytorch\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from name_dataset import NameDataset\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "HIDDEN_SIZE = 100\n",
        "N_LAYERS = 2\n",
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 100\n",
        "\n",
        "test_dataset = NameDataset(is_train_set=False)\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\n",
        "train_dataset = NameDataset(is_train_set=True)\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "N_COUNTRIES = len(train_dataset.get_countries())\n",
        "print(N_COUNTRIES, \"countries\")\n",
        "N_CHARS = 128  # ASCII\n",
        "\n",
        "\n",
        "# Some utility functions\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def create_variable(tensor):\n",
        "    # Do cuda() before wrapping with variable\n",
        "    if torch.cuda.is_available():\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "# pad sequences and sort the tensor\n",
        "def pad_sequences(vectorized_seqs, seq_lengths, countries):\n",
        "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
        "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
        "\n",
        "    # Sort tensors by their length\n",
        "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "    seq_tensor = seq_tensor[perm_idx]\n",
        "\n",
        "    # Also sort the target (countries) in the same order\n",
        "    target = countries2tensor(countries)\n",
        "    if len(countries):\n",
        "        target = target[perm_idx]\n",
        "\n",
        "    # Return variables\n",
        "    # DataParallel requires everything to be a Variable\n",
        "    return create_variable(seq_tensor), \\\n",
        "        create_variable(seq_lengths), \\\n",
        "        create_variable(target)\n",
        "\n",
        "\n",
        "# Create necessary variables, lengths, and target\n",
        "def make_variables(names, countries):\n",
        "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
        "    vectorized_seqs = [sl[0] for sl in sequence_and_length]\n",
        "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length])\n",
        "    return pad_sequences(vectorized_seqs, seq_lengths, countries)\n",
        "\n",
        "\n",
        "def str2ascii_arr(msg):\n",
        "    arr = [ord(c) for c in msg]\n",
        "    return arr, len(arr)\n",
        "\n",
        "\n",
        "def countries2tensor(countries):\n",
        "    country_ids = [train_dataset.get_country_id(\n",
        "        country) for country in countries]\n",
        "    return torch.LongTensor(country_ids)\n",
        "\n",
        "\n",
        "class RNNClassifier(nn.Module):\n",
        "    # Our model\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_directions = int(bidirectional) + 1\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, seq_lengths):\n",
        "        # Note: we run this all at once (over the whole input sequence)\n",
        "        # input shape: B x S (input size)\n",
        "        # transpose to make S(sequence) x B (batch)\n",
        "        input = input.t()\n",
        "        batch_size = input.size(1)\n",
        "\n",
        "        # Make a hidden\n",
        "        hidden = self._init_hidden(batch_size)\n",
        "\n",
        "        # Embedding S x B -> S x B x I (embedding size)\n",
        "        embedded = self.embedding(input)\n",
        "\n",
        "        # Pack them up nicely\n",
        "        gru_input = pack_padded_sequence(\n",
        "            embedded, seq_lengths.data.cpu().numpy())\n",
        "\n",
        "        # To compact weights again call flatten_parameters().\n",
        "        self.gru.flatten_parameters()\n",
        "        output, hidden = self.gru(gru_input, hidden)\n",
        "\n",
        "        # Use the last layer output as FC's input\n",
        "        # No need to unpack, since we are going to use hidden\n",
        "        fc_output = self.fc(hidden[-1])\n",
        "        return fc_output\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
        "                             batch_size, self.hidden_size)\n",
        "        return create_variable(hidden)\n",
        "\n",
        "\n",
        "# Train cycle\n",
        "def train():\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (names, countries) in enumerate(train_loader, 1):\n",
        "        input, seq_lengths, target = make_variables(names, countries)\n",
        "        output = classifier(input, seq_lengths)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        classifier.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('[{}] Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.2f}'.format(\n",
        "                time_since(start), epoch,  i *\n",
        "                len(names), len(train_loader.dataset),\n",
        "                100. * i * len(names) / len(train_loader.dataset),\n",
        "                total_loss / i * len(names)))\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# Testing cycle\n",
        "def test(name=None):\n",
        "    # Predict for a given name\n",
        "    if name:\n",
        "        input, seq_lengths, target = make_variables([name], [])\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        country_id = pred.cpu().numpy()[0][0]\n",
        "        print(name, \"is\", train_dataset.get_country(country_id))\n",
        "        return\n",
        "\n",
        "    print(\"evaluating trained model ...\")\n",
        "    correct = 0\n",
        "    train_data_size = len(test_loader.dataset)\n",
        "\n",
        "    for names, countries in test_loader:\n",
        "        input, seq_lengths, target = make_variables(names, countries)\n",
        "        output = classifier(input, seq_lengths)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, train_data_size, 100. * correct / train_data_size))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRIES, N_LAYERS)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "        # dim = 0 [33, xxx] -> [11, ...], [11, ...], [11, ...] on 3 GPUs\n",
        "        classifier = nn.DataParallel(classifier)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        classifier.cuda()\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        # Train cycle\n",
        "        train()\n",
        "\n",
        "        # Testing\n",
        "        test()\n",
        "\n",
        "        # Testing several samples\n",
        "        test(\"Sung\")\n",
        "        test(\"Jungwoo\")\n",
        "        test(\"Soojin\")\n",
        "        test(\"Nako\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6v2fu1KL_OL",
        "outputId": "3e4c1e7c-1e26-43c5-d0bf-2b074d486a44"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 countries\n",
            "Training for 100 epochs...\n",
            "[0m 1s] Train Epoch: 1 [2560/13374 (19%)]\tLoss: 612.23\n",
            "[0m 3s] Train Epoch: 1 [5120/13374 (38%)]\tLoss: 537.83\n",
            "[0m 5s] Train Epoch: 1 [7680/13374 (57%)]\tLoss: 498.32\n",
            "[0m 6s] Train Epoch: 1 [10240/13374 (77%)]\tLoss: 468.74\n",
            "[0m 9s] Train Epoch: 1 [12800/13374 (96%)]\tLoss: 442.91\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 4238/6700 (63%)\n",
            "\n",
            "Sung is Arabic\n",
            "Jungwoo is English\n",
            "Soojin is English\n",
            "Nako is Arabic\n",
            "[0m 12s] Train Epoch: 2 [2560/13374 (19%)]\tLoss: 318.76\n",
            "[0m 14s] Train Epoch: 2 [5120/13374 (38%)]\tLoss: 313.59\n",
            "[0m 15s] Train Epoch: 2 [7680/13374 (57%)]\tLoss: 310.16\n",
            "[0m 17s] Train Epoch: 2 [10240/13374 (77%)]\tLoss: 304.46\n",
            "[0m 19s] Train Epoch: 2 [12800/13374 (96%)]\tLoss: 302.27\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 4628/6700 (69%)\n",
            "\n",
            "Sung is German\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[0m 23s] Train Epoch: 3 [2560/13374 (19%)]\tLoss: 262.73\n",
            "[0m 25s] Train Epoch: 3 [5120/13374 (38%)]\tLoss: 258.89\n",
            "[0m 27s] Train Epoch: 3 [7680/13374 (57%)]\tLoss: 257.55\n",
            "[0m 28s] Train Epoch: 3 [10240/13374 (77%)]\tLoss: 251.57\n",
            "[0m 30s] Train Epoch: 3 [12800/13374 (96%)]\tLoss: 245.85\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5024/6700 (75%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[0m 34s] Train Epoch: 4 [2560/13374 (19%)]\tLoss: 205.65\n",
            "[0m 36s] Train Epoch: 4 [5120/13374 (38%)]\tLoss: 204.50\n",
            "[0m 38s] Train Epoch: 4 [7680/13374 (57%)]\tLoss: 207.05\n",
            "[0m 39s] Train Epoch: 4 [10240/13374 (77%)]\tLoss: 204.99\n",
            "[0m 41s] Train Epoch: 4 [12800/13374 (96%)]\tLoss: 204.04\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5192/6700 (77%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[0m 44s] Train Epoch: 5 [2560/13374 (19%)]\tLoss: 182.41\n",
            "[0m 47s] Train Epoch: 5 [5120/13374 (38%)]\tLoss: 182.00\n",
            "[0m 49s] Train Epoch: 5 [7680/13374 (57%)]\tLoss: 178.85\n",
            "[0m 50s] Train Epoch: 5 [10240/13374 (77%)]\tLoss: 176.26\n",
            "[0m 52s] Train Epoch: 5 [12800/13374 (96%)]\tLoss: 174.55\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5309/6700 (79%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Japanese\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[0m 56s] Train Epoch: 6 [2560/13374 (19%)]\tLoss: 164.19\n",
            "[0m 57s] Train Epoch: 6 [5120/13374 (38%)]\tLoss: 160.79\n",
            "[1m 0s] Train Epoch: 6 [7680/13374 (57%)]\tLoss: 157.89\n",
            "[1m 2s] Train Epoch: 6 [10240/13374 (77%)]\tLoss: 156.99\n",
            "[1m 3s] Train Epoch: 6 [12800/13374 (96%)]\tLoss: 156.98\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5390/6700 (80%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Japanese\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[1m 7s] Train Epoch: 7 [2560/13374 (19%)]\tLoss: 147.61\n",
            "[1m 8s] Train Epoch: 7 [5120/13374 (38%)]\tLoss: 141.12\n",
            "[1m 10s] Train Epoch: 7 [7680/13374 (57%)]\tLoss: 140.37\n",
            "[1m 12s] Train Epoch: 7 [10240/13374 (77%)]\tLoss: 143.06\n",
            "[1m 14s] Train Epoch: 7 [12800/13374 (96%)]\tLoss: 140.77\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5455/6700 (81%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Japanese\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[1m 18s] Train Epoch: 8 [2560/13374 (19%)]\tLoss: 137.65\n",
            "[1m 20s] Train Epoch: 8 [5120/13374 (38%)]\tLoss: 131.56\n",
            "[1m 21s] Train Epoch: 8 [7680/13374 (57%)]\tLoss: 129.98\n",
            "[1m 23s] Train Epoch: 8 [10240/13374 (77%)]\tLoss: 126.97\n",
            "[1m 25s] Train Epoch: 8 [12800/13374 (96%)]\tLoss: 128.06\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5497/6700 (82%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Japanese\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[1m 29s] Train Epoch: 9 [2560/13374 (19%)]\tLoss: 112.22\n",
            "[1m 31s] Train Epoch: 9 [5120/13374 (38%)]\tLoss: 114.14\n",
            "[1m 32s] Train Epoch: 9 [7680/13374 (57%)]\tLoss: 115.35\n",
            "[1m 34s] Train Epoch: 9 [10240/13374 (77%)]\tLoss: 116.46\n",
            "[1m 36s] Train Epoch: 9 [12800/13374 (96%)]\tLoss: 116.74\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5514/6700 (82%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Japanese\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[1m 40s] Train Epoch: 10 [2560/13374 (19%)]\tLoss: 104.28\n",
            "[1m 42s] Train Epoch: 10 [5120/13374 (38%)]\tLoss: 105.73\n",
            "[1m 43s] Train Epoch: 10 [7680/13374 (57%)]\tLoss: 107.25\n",
            "[1m 45s] Train Epoch: 10 [10240/13374 (77%)]\tLoss: 107.27\n",
            "[1m 47s] Train Epoch: 10 [12800/13374 (96%)]\tLoss: 109.06\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5557/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[1m 50s] Train Epoch: 11 [2560/13374 (19%)]\tLoss: 109.42\n",
            "[1m 53s] Train Epoch: 11 [5120/13374 (38%)]\tLoss: 104.23\n",
            "[1m 55s] Train Epoch: 11 [7680/13374 (57%)]\tLoss: 101.03\n",
            "[1m 56s] Train Epoch: 11 [10240/13374 (77%)]\tLoss: 101.87\n",
            "[1m 58s] Train Epoch: 11 [12800/13374 (96%)]\tLoss: 100.92\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5540/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 1s] Train Epoch: 12 [2560/13374 (19%)]\tLoss: 92.34\n",
            "[2m 3s] Train Epoch: 12 [5120/13374 (38%)]\tLoss: 91.55\n",
            "[2m 5s] Train Epoch: 12 [7680/13374 (57%)]\tLoss: 93.21\n",
            "[2m 7s] Train Epoch: 12 [10240/13374 (77%)]\tLoss: 92.17\n",
            "[2m 9s] Train Epoch: 12 [12800/13374 (96%)]\tLoss: 92.55\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5593/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 12s] Train Epoch: 13 [2560/13374 (19%)]\tLoss: 86.31\n",
            "[2m 14s] Train Epoch: 13 [5120/13374 (38%)]\tLoss: 89.50\n",
            "[2m 16s] Train Epoch: 13 [7680/13374 (57%)]\tLoss: 88.36\n",
            "[2m 18s] Train Epoch: 13 [10240/13374 (77%)]\tLoss: 86.49\n",
            "[2m 20s] Train Epoch: 13 [12800/13374 (96%)]\tLoss: 85.65\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5587/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 24s] Train Epoch: 14 [2560/13374 (19%)]\tLoss: 81.95\n",
            "[2m 25s] Train Epoch: 14 [5120/13374 (38%)]\tLoss: 82.79\n",
            "[2m 27s] Train Epoch: 14 [7680/13374 (57%)]\tLoss: 80.83\n",
            "[2m 28s] Train Epoch: 14 [10240/13374 (77%)]\tLoss: 80.28\n",
            "[2m 31s] Train Epoch: 14 [12800/13374 (96%)]\tLoss: 79.67\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5632/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 34s] Train Epoch: 15 [2560/13374 (19%)]\tLoss: 75.53\n",
            "[2m 36s] Train Epoch: 15 [5120/13374 (38%)]\tLoss: 71.92\n",
            "[2m 38s] Train Epoch: 15 [7680/13374 (57%)]\tLoss: 72.58\n",
            "[2m 39s] Train Epoch: 15 [10240/13374 (77%)]\tLoss: 72.43\n",
            "[2m 41s] Train Epoch: 15 [12800/13374 (96%)]\tLoss: 73.33\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5571/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 45s] Train Epoch: 16 [2560/13374 (19%)]\tLoss: 70.57\n",
            "[2m 47s] Train Epoch: 16 [5120/13374 (38%)]\tLoss: 67.43\n",
            "[2m 49s] Train Epoch: 16 [7680/13374 (57%)]\tLoss: 67.87\n",
            "[2m 50s] Train Epoch: 16 [10240/13374 (77%)]\tLoss: 66.93\n",
            "[2m 52s] Train Epoch: 16 [12800/13374 (96%)]\tLoss: 67.48\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5605/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[2m 56s] Train Epoch: 17 [2560/13374 (19%)]\tLoss: 62.64\n",
            "[2m 58s] Train Epoch: 17 [5120/13374 (38%)]\tLoss: 57.17\n",
            "[3m 0s] Train Epoch: 17 [7680/13374 (57%)]\tLoss: 58.20\n",
            "[3m 2s] Train Epoch: 17 [10240/13374 (77%)]\tLoss: 59.95\n",
            "[3m 3s] Train Epoch: 17 [12800/13374 (96%)]\tLoss: 60.92\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5619/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[3m 7s] Train Epoch: 18 [2560/13374 (19%)]\tLoss: 53.58\n",
            "[3m 8s] Train Epoch: 18 [5120/13374 (38%)]\tLoss: 53.72\n",
            "[3m 11s] Train Epoch: 18 [7680/13374 (57%)]\tLoss: 54.37\n",
            "[3m 13s] Train Epoch: 18 [10240/13374 (77%)]\tLoss: 55.21\n",
            "[3m 14s] Train Epoch: 18 [12800/13374 (96%)]\tLoss: 55.64\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5628/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[3m 18s] Train Epoch: 19 [2560/13374 (19%)]\tLoss: 48.09\n",
            "[3m 19s] Train Epoch: 19 [5120/13374 (38%)]\tLoss: 48.60\n",
            "[3m 21s] Train Epoch: 19 [7680/13374 (57%)]\tLoss: 50.27\n",
            "[3m 24s] Train Epoch: 19 [10240/13374 (77%)]\tLoss: 50.22\n",
            "[3m 25s] Train Epoch: 19 [12800/13374 (96%)]\tLoss: 51.39\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5627/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[3m 29s] Train Epoch: 20 [2560/13374 (19%)]\tLoss: 46.20\n",
            "[3m 30s] Train Epoch: 20 [5120/13374 (38%)]\tLoss: 46.42\n",
            "[3m 32s] Train Epoch: 20 [7680/13374 (57%)]\tLoss: 48.09\n",
            "[3m 34s] Train Epoch: 20 [10240/13374 (77%)]\tLoss: 47.57\n",
            "[3m 36s] Train Epoch: 20 [12800/13374 (96%)]\tLoss: 48.19\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5619/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[3m 40s] Train Epoch: 21 [2560/13374 (19%)]\tLoss: 41.47\n",
            "[3m 41s] Train Epoch: 21 [5120/13374 (38%)]\tLoss: 43.28\n",
            "[3m 43s] Train Epoch: 21 [7680/13374 (57%)]\tLoss: 43.31\n",
            "[3m 45s] Train Epoch: 21 [10240/13374 (77%)]\tLoss: 44.10\n",
            "[3m 46s] Train Epoch: 21 [12800/13374 (96%)]\tLoss: 44.34\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5603/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[3m 51s] Train Epoch: 22 [2560/13374 (19%)]\tLoss: 36.50\n",
            "[3m 53s] Train Epoch: 22 [5120/13374 (38%)]\tLoss: 36.88\n",
            "[3m 54s] Train Epoch: 22 [7680/13374 (57%)]\tLoss: 38.74\n",
            "[3m 56s] Train Epoch: 22 [10240/13374 (77%)]\tLoss: 40.41\n",
            "[3m 58s] Train Epoch: 22 [12800/13374 (96%)]\tLoss: 41.60\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5601/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 2s] Train Epoch: 23 [2560/13374 (19%)]\tLoss: 39.42\n",
            "[4m 4s] Train Epoch: 23 [5120/13374 (38%)]\tLoss: 38.91\n",
            "[4m 5s] Train Epoch: 23 [7680/13374 (57%)]\tLoss: 39.25\n",
            "[4m 7s] Train Epoch: 23 [10240/13374 (77%)]\tLoss: 39.41\n",
            "[4m 9s] Train Epoch: 23 [12800/13374 (96%)]\tLoss: 39.01\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5598/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 12s] Train Epoch: 24 [2560/13374 (19%)]\tLoss: 32.19\n",
            "[4m 15s] Train Epoch: 24 [5120/13374 (38%)]\tLoss: 33.40\n",
            "[4m 17s] Train Epoch: 24 [7680/13374 (57%)]\tLoss: 35.02\n",
            "[4m 18s] Train Epoch: 24 [10240/13374 (77%)]\tLoss: 35.31\n",
            "[4m 20s] Train Epoch: 24 [12800/13374 (96%)]\tLoss: 34.78\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5612/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 24s] Train Epoch: 25 [2560/13374 (19%)]\tLoss: 29.05\n",
            "[4m 25s] Train Epoch: 25 [5120/13374 (38%)]\tLoss: 28.47\n",
            "[4m 28s] Train Epoch: 25 [7680/13374 (57%)]\tLoss: 29.47\n",
            "[4m 30s] Train Epoch: 25 [10240/13374 (77%)]\tLoss: 29.79\n",
            "[4m 31s] Train Epoch: 25 [12800/13374 (96%)]\tLoss: 30.69\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5598/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 35s] Train Epoch: 26 [2560/13374 (19%)]\tLoss: 28.09\n",
            "[4m 37s] Train Epoch: 26 [5120/13374 (38%)]\tLoss: 28.16\n",
            "[4m 38s] Train Epoch: 26 [7680/13374 (57%)]\tLoss: 27.32\n",
            "[4m 40s] Train Epoch: 26 [10240/13374 (77%)]\tLoss: 28.83\n",
            "[4m 42s] Train Epoch: 26 [12800/13374 (96%)]\tLoss: 28.84\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5592/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 46s] Train Epoch: 27 [2560/13374 (19%)]\tLoss: 22.92\n",
            "[4m 48s] Train Epoch: 27 [5120/13374 (38%)]\tLoss: 24.80\n",
            "[4m 49s] Train Epoch: 27 [7680/13374 (57%)]\tLoss: 25.15\n",
            "[4m 51s] Train Epoch: 27 [10240/13374 (77%)]\tLoss: 26.22\n",
            "[4m 53s] Train Epoch: 27 [12800/13374 (96%)]\tLoss: 26.33\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5580/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[4m 57s] Train Epoch: 28 [2560/13374 (19%)]\tLoss: 21.26\n",
            "[4m 59s] Train Epoch: 28 [5120/13374 (38%)]\tLoss: 22.65\n",
            "[5m 0s] Train Epoch: 28 [7680/13374 (57%)]\tLoss: 23.36\n",
            "[5m 2s] Train Epoch: 28 [10240/13374 (77%)]\tLoss: 24.37\n",
            "[5m 4s] Train Epoch: 28 [12800/13374 (96%)]\tLoss: 25.62\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5561/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[5m 8s] Train Epoch: 29 [2560/13374 (19%)]\tLoss: 25.72\n",
            "[5m 10s] Train Epoch: 29 [5120/13374 (38%)]\tLoss: 26.96\n",
            "[5m 11s] Train Epoch: 29 [7680/13374 (57%)]\tLoss: 26.66\n",
            "[5m 13s] Train Epoch: 29 [10240/13374 (77%)]\tLoss: 26.22\n",
            "[5m 15s] Train Epoch: 29 [12800/13374 (96%)]\tLoss: 26.25\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5572/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[5m 18s] Train Epoch: 30 [2560/13374 (19%)]\tLoss: 20.06\n",
            "[5m 21s] Train Epoch: 30 [5120/13374 (38%)]\tLoss: 21.43\n",
            "[5m 23s] Train Epoch: 30 [7680/13374 (57%)]\tLoss: 23.08\n",
            "[5m 24s] Train Epoch: 30 [10240/13374 (77%)]\tLoss: 24.46\n",
            "[5m 26s] Train Epoch: 30 [12800/13374 (96%)]\tLoss: 24.43\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5578/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[5m 29s] Train Epoch: 31 [2560/13374 (19%)]\tLoss: 20.28\n",
            "[5m 31s] Train Epoch: 31 [5120/13374 (38%)]\tLoss: 20.77\n",
            "[5m 34s] Train Epoch: 31 [7680/13374 (57%)]\tLoss: 20.61\n",
            "[5m 35s] Train Epoch: 31 [10240/13374 (77%)]\tLoss: 21.06\n",
            "[5m 37s] Train Epoch: 31 [12800/13374 (96%)]\tLoss: 21.85\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5562/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Dutch\n",
            "Nako is Japanese\n",
            "[5m 41s] Train Epoch: 32 [2560/13374 (19%)]\tLoss: 19.41\n",
            "[5m 42s] Train Epoch: 32 [5120/13374 (38%)]\tLoss: 18.94\n",
            "[5m 44s] Train Epoch: 32 [7680/13374 (57%)]\tLoss: 18.70\n",
            "[5m 46s] Train Epoch: 32 [10240/13374 (77%)]\tLoss: 19.11\n",
            "[5m 48s] Train Epoch: 32 [12800/13374 (96%)]\tLoss: 19.93\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5552/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[5m 52s] Train Epoch: 33 [2560/13374 (19%)]\tLoss: 15.94\n",
            "[5m 54s] Train Epoch: 33 [5120/13374 (38%)]\tLoss: 17.30\n",
            "[5m 55s] Train Epoch: 33 [7680/13374 (57%)]\tLoss: 19.45\n",
            "[5m 57s] Train Epoch: 33 [10240/13374 (77%)]\tLoss: 19.59\n",
            "[6m 0s] Train Epoch: 33 [12800/13374 (96%)]\tLoss: 19.72\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5518/6700 (82%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 3s] Train Epoch: 34 [2560/13374 (19%)]\tLoss: 17.17\n",
            "[6m 5s] Train Epoch: 34 [5120/13374 (38%)]\tLoss: 17.12\n",
            "[6m 7s] Train Epoch: 34 [7680/13374 (57%)]\tLoss: 17.79\n",
            "[6m 8s] Train Epoch: 34 [10240/13374 (77%)]\tLoss: 18.17\n",
            "[6m 10s] Train Epoch: 34 [12800/13374 (96%)]\tLoss: 18.49\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5570/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 15s] Train Epoch: 35 [2560/13374 (19%)]\tLoss: 14.57\n",
            "[6m 16s] Train Epoch: 35 [5120/13374 (38%)]\tLoss: 14.77\n",
            "[6m 18s] Train Epoch: 35 [7680/13374 (57%)]\tLoss: 15.68\n",
            "[6m 20s] Train Epoch: 35 [10240/13374 (77%)]\tLoss: 16.33\n",
            "[6m 21s] Train Epoch: 35 [12800/13374 (96%)]\tLoss: 17.39\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5586/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is English\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 26s] Train Epoch: 36 [2560/13374 (19%)]\tLoss: 15.21\n",
            "[6m 27s] Train Epoch: 36 [5120/13374 (38%)]\tLoss: 15.38\n",
            "[6m 29s] Train Epoch: 36 [7680/13374 (57%)]\tLoss: 16.28\n",
            "[6m 31s] Train Epoch: 36 [10240/13374 (77%)]\tLoss: 16.74\n",
            "[6m 32s] Train Epoch: 36 [12800/13374 (96%)]\tLoss: 16.88\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5570/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 36s] Train Epoch: 37 [2560/13374 (19%)]\tLoss: 12.91\n",
            "[6m 38s] Train Epoch: 37 [5120/13374 (38%)]\tLoss: 13.79\n",
            "[6m 40s] Train Epoch: 37 [7680/13374 (57%)]\tLoss: 14.12\n",
            "[6m 42s] Train Epoch: 37 [10240/13374 (77%)]\tLoss: 15.00\n",
            "[6m 43s] Train Epoch: 37 [12800/13374 (96%)]\tLoss: 15.45\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5571/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 47s] Train Epoch: 38 [2560/13374 (19%)]\tLoss: 13.36\n",
            "[6m 49s] Train Epoch: 38 [5120/13374 (38%)]\tLoss: 16.22\n",
            "[6m 51s] Train Epoch: 38 [7680/13374 (57%)]\tLoss: 15.98\n",
            "[6m 53s] Train Epoch: 38 [10240/13374 (77%)]\tLoss: 16.54\n",
            "[6m 55s] Train Epoch: 38 [12800/13374 (96%)]\tLoss: 16.25\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5572/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[6m 58s] Train Epoch: 39 [2560/13374 (19%)]\tLoss: 12.74\n",
            "[7m 0s] Train Epoch: 39 [5120/13374 (38%)]\tLoss: 13.59\n",
            "[7m 1s] Train Epoch: 39 [7680/13374 (57%)]\tLoss: 16.09\n",
            "[7m 3s] Train Epoch: 39 [10240/13374 (77%)]\tLoss: 18.17\n",
            "[7m 6s] Train Epoch: 39 [12800/13374 (96%)]\tLoss: 19.22\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5496/6700 (82%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[7m 9s] Train Epoch: 40 [2560/13374 (19%)]\tLoss: 22.63\n",
            "[7m 11s] Train Epoch: 40 [5120/13374 (38%)]\tLoss: 22.65\n",
            "[7m 12s] Train Epoch: 40 [7680/13374 (57%)]\tLoss: 23.31\n",
            "[7m 14s] Train Epoch: 40 [10240/13374 (77%)]\tLoss: 22.72\n",
            "[7m 16s] Train Epoch: 40 [12800/13374 (96%)]\tLoss: 23.22\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5565/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[7m 20s] Train Epoch: 41 [2560/13374 (19%)]\tLoss: 17.36\n",
            "[7m 22s] Train Epoch: 41 [5120/13374 (38%)]\tLoss: 17.46\n",
            "[7m 23s] Train Epoch: 41 [7680/13374 (57%)]\tLoss: 17.47\n",
            "[7m 25s] Train Epoch: 41 [10240/13374 (77%)]\tLoss: 17.53\n",
            "[7m 27s] Train Epoch: 41 [12800/13374 (96%)]\tLoss: 18.02\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5552/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is English\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[7m 31s] Train Epoch: 42 [2560/13374 (19%)]\tLoss: 13.66\n",
            "[7m 33s] Train Epoch: 42 [5120/13374 (38%)]\tLoss: 14.73\n",
            "[7m 34s] Train Epoch: 42 [7680/13374 (57%)]\tLoss: 15.20\n",
            "[7m 36s] Train Epoch: 42 [10240/13374 (77%)]\tLoss: 15.91\n",
            "[7m 38s] Train Epoch: 42 [12800/13374 (96%)]\tLoss: 15.63\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5567/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[7m 41s] Train Epoch: 43 [2560/13374 (19%)]\tLoss: 12.43\n",
            "[7m 44s] Train Epoch: 43 [5120/13374 (38%)]\tLoss: 12.58\n",
            "[7m 46s] Train Epoch: 43 [7680/13374 (57%)]\tLoss: 12.30\n",
            "[7m 47s] Train Epoch: 43 [10240/13374 (77%)]\tLoss: 13.65\n",
            "[7m 49s] Train Epoch: 43 [12800/13374 (96%)]\tLoss: 14.39\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5559/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[7m 52s] Train Epoch: 44 [2560/13374 (19%)]\tLoss: 12.20\n",
            "[7m 54s] Train Epoch: 44 [5120/13374 (38%)]\tLoss: 12.05\n",
            "[7m 57s] Train Epoch: 44 [7680/13374 (57%)]\tLoss: 12.88\n",
            "[7m 58s] Train Epoch: 44 [10240/13374 (77%)]\tLoss: 13.30\n",
            "[8m 0s] Train Epoch: 44 [12800/13374 (96%)]\tLoss: 14.12\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5565/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[8m 3s] Train Epoch: 45 [2560/13374 (19%)]\tLoss: 10.43\n",
            "[8m 5s] Train Epoch: 45 [5120/13374 (38%)]\tLoss: 12.26\n",
            "[8m 7s] Train Epoch: 45 [7680/13374 (57%)]\tLoss: 12.72\n",
            "[8m 9s] Train Epoch: 45 [10240/13374 (77%)]\tLoss: 12.96\n",
            "[8m 11s] Train Epoch: 45 [12800/13374 (96%)]\tLoss: 13.15\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5581/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[8m 14s] Train Epoch: 46 [2560/13374 (19%)]\tLoss: 10.98\n",
            "[8m 16s] Train Epoch: 46 [5120/13374 (38%)]\tLoss: 11.56\n",
            "[8m 18s] Train Epoch: 46 [7680/13374 (57%)]\tLoss: 12.40\n",
            "[8m 19s] Train Epoch: 46 [10240/13374 (77%)]\tLoss: 12.37\n",
            "[8m 22s] Train Epoch: 46 [12800/13374 (96%)]\tLoss: 12.69\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5566/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[8m 26s] Train Epoch: 47 [2560/13374 (19%)]\tLoss: 10.21\n",
            "[8m 27s] Train Epoch: 47 [5120/13374 (38%)]\tLoss: 10.01\n",
            "[8m 29s] Train Epoch: 47 [7680/13374 (57%)]\tLoss: 11.80\n",
            "[8m 30s] Train Epoch: 47 [10240/13374 (77%)]\tLoss: 11.58\n",
            "[8m 32s] Train Epoch: 47 [12800/13374 (96%)]\tLoss: 12.58\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5581/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[8m 36s] Train Epoch: 48 [2560/13374 (19%)]\tLoss: 10.87\n",
            "[8m 38s] Train Epoch: 48 [5120/13374 (38%)]\tLoss: 10.66\n",
            "[8m 40s] Train Epoch: 48 [7680/13374 (57%)]\tLoss: 11.29\n",
            "[8m 41s] Train Epoch: 48 [10240/13374 (77%)]\tLoss: 11.60\n",
            "[8m 43s] Train Epoch: 48 [12800/13374 (96%)]\tLoss: 12.21\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5567/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[8m 47s] Train Epoch: 49 [2560/13374 (19%)]\tLoss: 10.12\n",
            "[8m 49s] Train Epoch: 49 [5120/13374 (38%)]\tLoss: 11.01\n",
            "[8m 51s] Train Epoch: 49 [7680/13374 (57%)]\tLoss: 11.49\n",
            "[8m 52s] Train Epoch: 49 [10240/13374 (77%)]\tLoss: 11.64\n",
            "[8m 54s] Train Epoch: 49 [12800/13374 (96%)]\tLoss: 11.85\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5563/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[8m 58s] Train Epoch: 50 [2560/13374 (19%)]\tLoss: 10.59\n",
            "[9m 0s] Train Epoch: 50 [5120/13374 (38%)]\tLoss: 10.95\n",
            "[9m 2s] Train Epoch: 50 [7680/13374 (57%)]\tLoss: 11.49\n",
            "[9m 3s] Train Epoch: 50 [10240/13374 (77%)]\tLoss: 11.38\n",
            "[9m 5s] Train Epoch: 50 [12800/13374 (96%)]\tLoss: 11.97\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5587/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[9m 9s] Train Epoch: 51 [2560/13374 (19%)]\tLoss: 10.61\n",
            "[9m 10s] Train Epoch: 51 [5120/13374 (38%)]\tLoss: 10.85\n",
            "[9m 12s] Train Epoch: 51 [7680/13374 (57%)]\tLoss: 11.03\n",
            "[9m 14s] Train Epoch: 51 [10240/13374 (77%)]\tLoss: 11.00\n",
            "[9m 16s] Train Epoch: 51 [12800/13374 (96%)]\tLoss: 11.61\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5558/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[9m 20s] Train Epoch: 52 [2560/13374 (19%)]\tLoss: 10.43\n",
            "[9m 21s] Train Epoch: 52 [5120/13374 (38%)]\tLoss: 11.12\n",
            "[9m 23s] Train Epoch: 52 [7680/13374 (57%)]\tLoss: 10.79\n",
            "[9m 25s] Train Epoch: 52 [10240/13374 (77%)]\tLoss: 11.24\n",
            "[9m 27s] Train Epoch: 52 [12800/13374 (96%)]\tLoss: 11.82\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5568/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[9m 31s] Train Epoch: 53 [2560/13374 (19%)]\tLoss: 10.49\n",
            "[9m 32s] Train Epoch: 53 [5120/13374 (38%)]\tLoss: 10.24\n",
            "[9m 34s] Train Epoch: 53 [7680/13374 (57%)]\tLoss: 9.75\n",
            "[9m 36s] Train Epoch: 53 [10240/13374 (77%)]\tLoss: 10.68\n",
            "[9m 37s] Train Epoch: 53 [12800/13374 (96%)]\tLoss: 11.60\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5567/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[9m 42s] Train Epoch: 54 [2560/13374 (19%)]\tLoss: 10.10\n",
            "[9m 44s] Train Epoch: 54 [5120/13374 (38%)]\tLoss: 9.84\n",
            "[9m 45s] Train Epoch: 54 [7680/13374 (57%)]\tLoss: 10.15\n",
            "[9m 47s] Train Epoch: 54 [10240/13374 (77%)]\tLoss: 10.39\n",
            "[9m 49s] Train Epoch: 54 [12800/13374 (96%)]\tLoss: 11.26\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5564/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[9m 53s] Train Epoch: 55 [2560/13374 (19%)]\tLoss: 8.85\n",
            "[9m 55s] Train Epoch: 55 [5120/13374 (38%)]\tLoss: 8.82\n",
            "[9m 56s] Train Epoch: 55 [7680/13374 (57%)]\tLoss: 9.06\n",
            "[9m 58s] Train Epoch: 55 [10240/13374 (77%)]\tLoss: 10.01\n",
            "[10m 0s] Train Epoch: 55 [12800/13374 (96%)]\tLoss: 11.39\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5562/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[10m 3s] Train Epoch: 56 [2560/13374 (19%)]\tLoss: 9.60\n",
            "[10m 5s] Train Epoch: 56 [5120/13374 (38%)]\tLoss: 9.81\n",
            "[10m 7s] Train Epoch: 56 [7680/13374 (57%)]\tLoss: 10.35\n",
            "[10m 9s] Train Epoch: 56 [10240/13374 (77%)]\tLoss: 11.14\n",
            "[10m 11s] Train Epoch: 56 [12800/13374 (96%)]\tLoss: 11.29\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5580/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[10m 14s] Train Epoch: 57 [2560/13374 (19%)]\tLoss: 10.20\n",
            "[10m 16s] Train Epoch: 57 [5120/13374 (38%)]\tLoss: 9.58\n",
            "[10m 18s] Train Epoch: 57 [7680/13374 (57%)]\tLoss: 10.83\n",
            "[10m 20s] Train Epoch: 57 [10240/13374 (77%)]\tLoss: 10.42\n",
            "[10m 22s] Train Epoch: 57 [12800/13374 (96%)]\tLoss: 11.01\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5544/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[10m 25s] Train Epoch: 58 [2560/13374 (19%)]\tLoss: 8.96\n",
            "[10m 27s] Train Epoch: 58 [5120/13374 (38%)]\tLoss: 10.46\n",
            "[10m 29s] Train Epoch: 58 [7680/13374 (57%)]\tLoss: 10.08\n",
            "[10m 31s] Train Epoch: 58 [10240/13374 (77%)]\tLoss: 10.42\n",
            "[10m 33s] Train Epoch: 58 [12800/13374 (96%)]\tLoss: 11.08\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5558/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[10m 36s] Train Epoch: 59 [2560/13374 (19%)]\tLoss: 9.00\n",
            "[10m 38s] Train Epoch: 59 [5120/13374 (38%)]\tLoss: 10.49\n",
            "[10m 39s] Train Epoch: 59 [7680/13374 (57%)]\tLoss: 11.08\n",
            "[10m 41s] Train Epoch: 59 [10240/13374 (77%)]\tLoss: 11.66\n",
            "[10m 43s] Train Epoch: 59 [12800/13374 (96%)]\tLoss: 12.23\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5572/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[10m 47s] Train Epoch: 60 [2560/13374 (19%)]\tLoss: 10.37\n",
            "[10m 49s] Train Epoch: 60 [5120/13374 (38%)]\tLoss: 10.66\n",
            "[10m 50s] Train Epoch: 60 [7680/13374 (57%)]\tLoss: 11.53\n",
            "[10m 52s] Train Epoch: 60 [10240/13374 (77%)]\tLoss: 11.64\n",
            "[10m 54s] Train Epoch: 60 [12800/13374 (96%)]\tLoss: 11.96\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5604/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is English\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[10m 58s] Train Epoch: 61 [2560/13374 (19%)]\tLoss: 12.66\n",
            "[11m 0s] Train Epoch: 61 [5120/13374 (38%)]\tLoss: 13.10\n",
            "[11m 2s] Train Epoch: 61 [7680/13374 (57%)]\tLoss: 13.71\n",
            "[11m 3s] Train Epoch: 61 [10240/13374 (77%)]\tLoss: 14.34\n",
            "[11m 5s] Train Epoch: 61 [12800/13374 (96%)]\tLoss: 15.17\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5526/6700 (82%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is English\n",
            "Soojin is Dutch\n",
            "Nako is Japanese\n",
            "[11m 8s] Train Epoch: 62 [2560/13374 (19%)]\tLoss: 14.87\n",
            "[11m 11s] Train Epoch: 62 [5120/13374 (38%)]\tLoss: 14.83\n",
            "[11m 12s] Train Epoch: 62 [7680/13374 (57%)]\tLoss: 14.53\n",
            "[11m 14s] Train Epoch: 62 [10240/13374 (77%)]\tLoss: 15.53\n",
            "[11m 16s] Train Epoch: 62 [12800/13374 (96%)]\tLoss: 16.25\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5544/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[11m 19s] Train Epoch: 63 [2560/13374 (19%)]\tLoss: 15.81\n",
            "[11m 21s] Train Epoch: 63 [5120/13374 (38%)]\tLoss: 15.43\n",
            "[11m 23s] Train Epoch: 63 [7680/13374 (57%)]\tLoss: 15.37\n",
            "[11m 25s] Train Epoch: 63 [10240/13374 (77%)]\tLoss: 16.40\n",
            "[11m 26s] Train Epoch: 63 [12800/13374 (96%)]\tLoss: 15.99\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5584/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[11m 30s] Train Epoch: 64 [2560/13374 (19%)]\tLoss: 14.02\n",
            "[11m 31s] Train Epoch: 64 [5120/13374 (38%)]\tLoss: 12.63\n",
            "[11m 33s] Train Epoch: 64 [7680/13374 (57%)]\tLoss: 12.93\n",
            "[11m 35s] Train Epoch: 64 [10240/13374 (77%)]\tLoss: 13.92\n",
            "[11m 37s] Train Epoch: 64 [12800/13374 (96%)]\tLoss: 13.77\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5569/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[11m 41s] Train Epoch: 65 [2560/13374 (19%)]\tLoss: 11.72\n",
            "[11m 42s] Train Epoch: 65 [5120/13374 (38%)]\tLoss: 11.48\n",
            "[11m 44s] Train Epoch: 65 [7680/13374 (57%)]\tLoss: 11.71\n",
            "[11m 45s] Train Epoch: 65 [10240/13374 (77%)]\tLoss: 11.77\n",
            "[11m 47s] Train Epoch: 65 [12800/13374 (96%)]\tLoss: 12.00\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5582/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[11m 51s] Train Epoch: 66 [2560/13374 (19%)]\tLoss: 11.64\n",
            "[11m 53s] Train Epoch: 66 [5120/13374 (38%)]\tLoss: 11.48\n",
            "[11m 55s] Train Epoch: 66 [7680/13374 (57%)]\tLoss: 11.13\n",
            "[11m 56s] Train Epoch: 66 [10240/13374 (77%)]\tLoss: 11.70\n",
            "[11m 58s] Train Epoch: 66 [12800/13374 (96%)]\tLoss: 11.47\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5587/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[12m 2s] Train Epoch: 67 [2560/13374 (19%)]\tLoss: 8.74\n",
            "[12m 4s] Train Epoch: 67 [5120/13374 (38%)]\tLoss: 8.86\n",
            "[12m 6s] Train Epoch: 67 [7680/13374 (57%)]\tLoss: 9.72\n",
            "[12m 7s] Train Epoch: 67 [10240/13374 (77%)]\tLoss: 10.19\n",
            "[12m 9s] Train Epoch: 67 [12800/13374 (96%)]\tLoss: 10.81\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5601/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[12m 12s] Train Epoch: 68 [2560/13374 (19%)]\tLoss: 9.96\n",
            "[12m 14s] Train Epoch: 68 [5120/13374 (38%)]\tLoss: 10.02\n",
            "[12m 16s] Train Epoch: 68 [7680/13374 (57%)]\tLoss: 9.98\n",
            "[12m 18s] Train Epoch: 68 [10240/13374 (77%)]\tLoss: 10.45\n",
            "[12m 20s] Train Epoch: 68 [12800/13374 (96%)]\tLoss: 10.81\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5586/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[12m 23s] Train Epoch: 69 [2560/13374 (19%)]\tLoss: 8.20\n",
            "[12m 25s] Train Epoch: 69 [5120/13374 (38%)]\tLoss: 9.14\n",
            "[12m 27s] Train Epoch: 69 [7680/13374 (57%)]\tLoss: 9.25\n",
            "[12m 29s] Train Epoch: 69 [10240/13374 (77%)]\tLoss: 9.80\n",
            "[12m 31s] Train Epoch: 69 [12800/13374 (96%)]\tLoss: 10.35\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5594/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[12m 34s] Train Epoch: 70 [2560/13374 (19%)]\tLoss: 9.64\n",
            "[12m 36s] Train Epoch: 70 [5120/13374 (38%)]\tLoss: 9.61\n",
            "[12m 37s] Train Epoch: 70 [7680/13374 (57%)]\tLoss: 9.54\n",
            "[12m 39s] Train Epoch: 70 [10240/13374 (77%)]\tLoss: 10.25\n",
            "[12m 41s] Train Epoch: 70 [12800/13374 (96%)]\tLoss: 10.18\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5569/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[12m 45s] Train Epoch: 71 [2560/13374 (19%)]\tLoss: 7.68\n",
            "[12m 46s] Train Epoch: 71 [5120/13374 (38%)]\tLoss: 9.21\n",
            "[12m 48s] Train Epoch: 71 [7680/13374 (57%)]\tLoss: 9.44\n",
            "[12m 50s] Train Epoch: 71 [10240/13374 (77%)]\tLoss: 9.54\n",
            "[12m 51s] Train Epoch: 71 [12800/13374 (96%)]\tLoss: 10.20\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5592/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[12m 56s] Train Epoch: 72 [2560/13374 (19%)]\tLoss: 8.27\n",
            "[12m 57s] Train Epoch: 72 [5120/13374 (38%)]\tLoss: 9.09\n",
            "[12m 59s] Train Epoch: 72 [7680/13374 (57%)]\tLoss: 9.92\n",
            "[13m 1s] Train Epoch: 72 [10240/13374 (77%)]\tLoss: 9.95\n",
            "[13m 2s] Train Epoch: 72 [12800/13374 (96%)]\tLoss: 10.17\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5603/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[13m 6s] Train Epoch: 73 [2560/13374 (19%)]\tLoss: 9.55\n",
            "[13m 8s] Train Epoch: 73 [5120/13374 (38%)]\tLoss: 8.88\n",
            "[13m 10s] Train Epoch: 73 [7680/13374 (57%)]\tLoss: 9.20\n",
            "[13m 12s] Train Epoch: 73 [10240/13374 (77%)]\tLoss: 10.39\n",
            "[13m 13s] Train Epoch: 73 [12800/13374 (96%)]\tLoss: 10.46\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5597/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[13m 16s] Train Epoch: 74 [2560/13374 (19%)]\tLoss: 10.69\n",
            "[13m 18s] Train Epoch: 74 [5120/13374 (38%)]\tLoss: 9.07\n",
            "[13m 21s] Train Epoch: 74 [7680/13374 (57%)]\tLoss: 8.96\n",
            "[13m 22s] Train Epoch: 74 [10240/13374 (77%)]\tLoss: 9.52\n",
            "[13m 24s] Train Epoch: 74 [12800/13374 (96%)]\tLoss: 10.06\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5601/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[13m 27s] Train Epoch: 75 [2560/13374 (19%)]\tLoss: 9.47\n",
            "[13m 29s] Train Epoch: 75 [5120/13374 (38%)]\tLoss: 9.94\n",
            "[13m 31s] Train Epoch: 75 [7680/13374 (57%)]\tLoss: 10.27\n",
            "[13m 33s] Train Epoch: 75 [10240/13374 (77%)]\tLoss: 10.47\n",
            "[13m 35s] Train Epoch: 75 [12800/13374 (96%)]\tLoss: 10.30\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5588/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[13m 38s] Train Epoch: 76 [2560/13374 (19%)]\tLoss: 8.19\n",
            "[13m 40s] Train Epoch: 76 [5120/13374 (38%)]\tLoss: 9.31\n",
            "[13m 41s] Train Epoch: 76 [7680/13374 (57%)]\tLoss: 9.94\n",
            "[13m 43s] Train Epoch: 76 [10240/13374 (77%)]\tLoss: 10.13\n",
            "[13m 45s] Train Epoch: 76 [12800/13374 (96%)]\tLoss: 9.98\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5602/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[13m 49s] Train Epoch: 77 [2560/13374 (19%)]\tLoss: 9.74\n",
            "[13m 50s] Train Epoch: 77 [5120/13374 (38%)]\tLoss: 9.85\n",
            "[13m 52s] Train Epoch: 77 [7680/13374 (57%)]\tLoss: 10.13\n",
            "[13m 54s] Train Epoch: 77 [10240/13374 (77%)]\tLoss: 9.71\n",
            "[13m 55s] Train Epoch: 77 [12800/13374 (96%)]\tLoss: 9.88\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5585/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[14m 0s] Train Epoch: 78 [2560/13374 (19%)]\tLoss: 7.76\n",
            "[14m 1s] Train Epoch: 78 [5120/13374 (38%)]\tLoss: 8.32\n",
            "[14m 3s] Train Epoch: 78 [7680/13374 (57%)]\tLoss: 8.36\n",
            "[14m 5s] Train Epoch: 78 [10240/13374 (77%)]\tLoss: 9.16\n",
            "[14m 6s] Train Epoch: 78 [12800/13374 (96%)]\tLoss: 9.67\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5606/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[14m 10s] Train Epoch: 79 [2560/13374 (19%)]\tLoss: 7.71\n",
            "[14m 12s] Train Epoch: 79 [5120/13374 (38%)]\tLoss: 8.51\n",
            "[14m 14s] Train Epoch: 79 [7680/13374 (57%)]\tLoss: 9.23\n",
            "[14m 15s] Train Epoch: 79 [10240/13374 (77%)]\tLoss: 9.35\n",
            "[14m 17s] Train Epoch: 79 [12800/13374 (96%)]\tLoss: 9.67\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5595/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[14m 20s] Train Epoch: 80 [2560/13374 (19%)]\tLoss: 7.51\n",
            "[14m 22s] Train Epoch: 80 [5120/13374 (38%)]\tLoss: 8.05\n",
            "[14m 24s] Train Epoch: 80 [7680/13374 (57%)]\tLoss: 8.38\n",
            "[14m 26s] Train Epoch: 80 [10240/13374 (77%)]\tLoss: 9.07\n",
            "[14m 28s] Train Epoch: 80 [12800/13374 (96%)]\tLoss: 9.56\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5580/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[14m 31s] Train Epoch: 81 [2560/13374 (19%)]\tLoss: 8.62\n",
            "[14m 33s] Train Epoch: 81 [5120/13374 (38%)]\tLoss: 9.02\n",
            "[14m 35s] Train Epoch: 81 [7680/13374 (57%)]\tLoss: 9.00\n",
            "[14m 37s] Train Epoch: 81 [10240/13374 (77%)]\tLoss: 9.64\n",
            "[14m 39s] Train Epoch: 81 [12800/13374 (96%)]\tLoss: 9.97\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5591/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[14m 42s] Train Epoch: 82 [2560/13374 (19%)]\tLoss: 9.08\n",
            "[14m 44s] Train Epoch: 82 [5120/13374 (38%)]\tLoss: 9.40\n",
            "[14m 46s] Train Epoch: 82 [7680/13374 (57%)]\tLoss: 9.28\n",
            "[14m 47s] Train Epoch: 82 [10240/13374 (77%)]\tLoss: 9.59\n",
            "[14m 49s] Train Epoch: 82 [12800/13374 (96%)]\tLoss: 9.86\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5584/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[14m 53s] Train Epoch: 83 [2560/13374 (19%)]\tLoss: 9.54\n",
            "[14m 55s] Train Epoch: 83 [5120/13374 (38%)]\tLoss: 8.63\n",
            "[14m 57s] Train Epoch: 83 [7680/13374 (57%)]\tLoss: 9.10\n",
            "[14m 58s] Train Epoch: 83 [10240/13374 (77%)]\tLoss: 9.27\n",
            "[15m 0s] Train Epoch: 83 [12800/13374 (96%)]\tLoss: 9.38\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5593/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[15m 4s] Train Epoch: 84 [2560/13374 (19%)]\tLoss: 7.06\n",
            "[15m 6s] Train Epoch: 84 [5120/13374 (38%)]\tLoss: 8.29\n",
            "[15m 7s] Train Epoch: 84 [7680/13374 (57%)]\tLoss: 8.92\n",
            "[15m 9s] Train Epoch: 84 [10240/13374 (77%)]\tLoss: 9.62\n",
            "[15m 11s] Train Epoch: 84 [12800/13374 (96%)]\tLoss: 9.86\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5585/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[15m 14s] Train Epoch: 85 [2560/13374 (19%)]\tLoss: 8.03\n",
            "[15m 16s] Train Epoch: 85 [5120/13374 (38%)]\tLoss: 9.01\n",
            "[15m 18s] Train Epoch: 85 [7680/13374 (57%)]\tLoss: 8.76\n",
            "[15m 20s] Train Epoch: 85 [10240/13374 (77%)]\tLoss: 9.05\n",
            "[15m 21s] Train Epoch: 85 [12800/13374 (96%)]\tLoss: 9.37\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5614/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[15m 25s] Train Epoch: 86 [2560/13374 (19%)]\tLoss: 8.69\n",
            "[15m 26s] Train Epoch: 86 [5120/13374 (38%)]\tLoss: 8.34\n",
            "[15m 28s] Train Epoch: 86 [7680/13374 (57%)]\tLoss: 9.10\n",
            "[15m 30s] Train Epoch: 86 [10240/13374 (77%)]\tLoss: 9.20\n",
            "[15m 32s] Train Epoch: 86 [12800/13374 (96%)]\tLoss: 9.61\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5584/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[15m 35s] Train Epoch: 87 [2560/13374 (19%)]\tLoss: 7.88\n",
            "[15m 37s] Train Epoch: 87 [5120/13374 (38%)]\tLoss: 8.20\n",
            "[15m 39s] Train Epoch: 87 [7680/13374 (57%)]\tLoss: 8.78\n",
            "[15m 40s] Train Epoch: 87 [10240/13374 (77%)]\tLoss: 9.17\n",
            "[15m 43s] Train Epoch: 87 [12800/13374 (96%)]\tLoss: 9.36\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5570/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[15m 46s] Train Epoch: 88 [2560/13374 (19%)]\tLoss: 7.83\n",
            "[15m 48s] Train Epoch: 88 [5120/13374 (38%)]\tLoss: 8.02\n",
            "[15m 49s] Train Epoch: 88 [7680/13374 (57%)]\tLoss: 8.46\n",
            "[15m 51s] Train Epoch: 88 [10240/13374 (77%)]\tLoss: 8.90\n",
            "[15m 53s] Train Epoch: 88 [12800/13374 (96%)]\tLoss: 9.35\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5584/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[15m 57s] Train Epoch: 89 [2560/13374 (19%)]\tLoss: 7.65\n",
            "[15m 59s] Train Epoch: 89 [5120/13374 (38%)]\tLoss: 8.15\n",
            "[16m 0s] Train Epoch: 89 [7680/13374 (57%)]\tLoss: 8.71\n",
            "[16m 2s] Train Epoch: 89 [10240/13374 (77%)]\tLoss: 9.16\n",
            "[16m 4s] Train Epoch: 89 [12800/13374 (96%)]\tLoss: 9.65\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5586/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[16m 7s] Train Epoch: 90 [2560/13374 (19%)]\tLoss: 9.05\n",
            "[16m 10s] Train Epoch: 90 [5120/13374 (38%)]\tLoss: 8.80\n",
            "[16m 11s] Train Epoch: 90 [7680/13374 (57%)]\tLoss: 9.45\n",
            "[16m 13s] Train Epoch: 90 [10240/13374 (77%)]\tLoss: 9.51\n",
            "[16m 14s] Train Epoch: 90 [12800/13374 (96%)]\tLoss: 9.68\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5609/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[16m 18s] Train Epoch: 91 [2560/13374 (19%)]\tLoss: 6.92\n",
            "[16m 20s] Train Epoch: 91 [5120/13374 (38%)]\tLoss: 7.43\n",
            "[16m 22s] Train Epoch: 91 [7680/13374 (57%)]\tLoss: 8.16\n",
            "[16m 24s] Train Epoch: 91 [10240/13374 (77%)]\tLoss: 8.98\n",
            "[16m 25s] Train Epoch: 91 [12800/13374 (96%)]\tLoss: 9.45\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5592/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[16m 29s] Train Epoch: 92 [2560/13374 (19%)]\tLoss: 9.41\n",
            "[16m 30s] Train Epoch: 92 [5120/13374 (38%)]\tLoss: 9.10\n",
            "[16m 32s] Train Epoch: 92 [7680/13374 (57%)]\tLoss: 8.96\n",
            "[16m 34s] Train Epoch: 92 [10240/13374 (77%)]\tLoss: 9.17\n",
            "[16m 36s] Train Epoch: 92 [12800/13374 (96%)]\tLoss: 9.38\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5599/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[16m 39s] Train Epoch: 93 [2560/13374 (19%)]\tLoss: 9.54\n",
            "[16m 41s] Train Epoch: 93 [5120/13374 (38%)]\tLoss: 8.39\n",
            "[16m 43s] Train Epoch: 93 [7680/13374 (57%)]\tLoss: 8.89\n",
            "[16m 44s] Train Epoch: 93 [10240/13374 (77%)]\tLoss: 9.29\n",
            "[16m 47s] Train Epoch: 93 [12800/13374 (96%)]\tLoss: 9.48\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5576/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[16m 50s] Train Epoch: 94 [2560/13374 (19%)]\tLoss: 7.64\n",
            "[16m 52s] Train Epoch: 94 [5120/13374 (38%)]\tLoss: 8.10\n",
            "[16m 54s] Train Epoch: 94 [7680/13374 (57%)]\tLoss: 8.09\n",
            "[16m 55s] Train Epoch: 94 [10240/13374 (77%)]\tLoss: 8.42\n",
            "[16m 57s] Train Epoch: 94 [12800/13374 (96%)]\tLoss: 9.43\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5611/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[17m 1s] Train Epoch: 95 [2560/13374 (19%)]\tLoss: 9.25\n",
            "[17m 3s] Train Epoch: 95 [5120/13374 (38%)]\tLoss: 9.16\n",
            "[17m 5s] Train Epoch: 95 [7680/13374 (57%)]\tLoss: 9.10\n",
            "[17m 6s] Train Epoch: 95 [10240/13374 (77%)]\tLoss: 9.22\n",
            "[17m 8s] Train Epoch: 95 [12800/13374 (96%)]\tLoss: 9.27\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5601/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[17m 11s] Train Epoch: 96 [2560/13374 (19%)]\tLoss: 7.86\n",
            "[17m 14s] Train Epoch: 96 [5120/13374 (38%)]\tLoss: 8.57\n",
            "[17m 15s] Train Epoch: 96 [7680/13374 (57%)]\tLoss: 8.71\n",
            "[17m 17s] Train Epoch: 96 [10240/13374 (77%)]\tLoss: 8.80\n",
            "[17m 18s] Train Epoch: 96 [12800/13374 (96%)]\tLoss: 9.18\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5589/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[17m 22s] Train Epoch: 97 [2560/13374 (19%)]\tLoss: 7.46\n",
            "[17m 23s] Train Epoch: 97 [5120/13374 (38%)]\tLoss: 8.18\n",
            "[17m 26s] Train Epoch: 97 [7680/13374 (57%)]\tLoss: 8.61\n",
            "[17m 28s] Train Epoch: 97 [10240/13374 (77%)]\tLoss: 9.19\n",
            "[17m 29s] Train Epoch: 97 [12800/13374 (96%)]\tLoss: 9.48\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5597/6700 (84%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Korean\n",
            "[17m 33s] Train Epoch: 98 [2560/13374 (19%)]\tLoss: 8.16\n",
            "[17m 34s] Train Epoch: 98 [5120/13374 (38%)]\tLoss: 8.48\n",
            "[17m 36s] Train Epoch: 98 [7680/13374 (57%)]\tLoss: 8.81\n",
            "[17m 38s] Train Epoch: 98 [10240/13374 (77%)]\tLoss: 8.95\n",
            "[17m 40s] Train Epoch: 98 [12800/13374 (96%)]\tLoss: 9.20\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5589/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[17m 43s] Train Epoch: 99 [2560/13374 (19%)]\tLoss: 9.00\n",
            "[17m 45s] Train Epoch: 99 [5120/13374 (38%)]\tLoss: 8.63\n",
            "[17m 46s] Train Epoch: 99 [7680/13374 (57%)]\tLoss: 9.09\n",
            "[17m 48s] Train Epoch: 99 [10240/13374 (77%)]\tLoss: 9.26\n",
            "[17m 50s] Train Epoch: 99 [12800/13374 (96%)]\tLoss: 9.31\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5588/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n",
            "[17m 54s] Train Epoch: 100 [2560/13374 (19%)]\tLoss: 9.28\n",
            "[17m 56s] Train Epoch: 100 [5120/13374 (38%)]\tLoss: 9.15\n",
            "[17m 57s] Train Epoch: 100 [7680/13374 (57%)]\tLoss: 9.35\n",
            "[17m 59s] Train Epoch: 100 [10240/13374 (77%)]\tLoss: 8.79\n",
            "[18m 1s] Train Epoch: 100 [12800/13374 (96%)]\tLoss: 9.36\n",
            "evaluating trained model ...\n",
            "\n",
            "Test set: Accuracy: 5594/6700 (83%)\n",
            "\n",
            "Sung is Chinese\n",
            "Jungwoo is Russian\n",
            "Soojin is Russian\n",
            "Nako is Japanese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/spro/practical-pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from text_loader import TextDataset\n",
        "\n",
        "hidden_size = 100\n",
        "n_layers = 3\n",
        "batch_size = 1\n",
        "n_epochs = 5  # Reduced for faster testing\n",
        "n_characters = 128  # ASCII\n",
        "max_iters_per_epoch = 50  # Limit iterations per epoch\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embed = self.embedding(input.view(1, -1))  # S(=1) x I\n",
        "        embed = embed.view(1, 1, -1)  # S(=1) x B(=1) x I\n",
        "        output, hidden = self.gru(embed, hidden)\n",
        "        output = self.linear(output.view(1, -1))  # S(=1) x I\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size, device=device)\n",
        "        return Variable(hidden)\n",
        "\n",
        "def str2tensor(string):\n",
        "    tensor = torch.LongTensor([ord(c) for c in string])\n",
        "    if torch.cuda.is_available():\n",
        "        tensor = tensor.cuda()\n",
        "    return Variable(tensor)\n",
        "\n",
        "def generate(decoder, prime_str='A', predict_len=50, temperature=0.8):\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = str2tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "        predicted_char = chr(top_i)\n",
        "        predicted += predicted_char\n",
        "        inp = str2tensor(predicted_char)\n",
        "\n",
        "    return predicted\n",
        "\n",
        "def train_no_teacher(line):\n",
        "    input = str2tensor(line[:-1])\n",
        "    target = str2tensor(line[1:])\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder_input = input[0]\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(len(input)):\n",
        "        output, hidden = decoder(decoder_input, hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "        decoder_input = output.max(1)[1]\n",
        "\n",
        "    decoder.zero_grad()\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "    return loss.item() / len(input)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    decoder = RNN(n_characters, hidden_size, n_characters, n_layers).to(device)\n",
        "\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(dataset=TextDataset(),\n",
        "                              batch_size=batch_size,\n",
        "                              shuffle=True)\n",
        "\n",
        "    print(f\"Training for {n_epochs} epochs...\")\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        for i, (lines, _) in enumerate(train_loader):\n",
        "            if i >= max_iters_per_epoch:\n",
        "                break  # stop early\n",
        "\n",
        "            # Skip empty lines\n",
        "            if not lines or len(lines[0]) < 2:\n",
        "                continue\n",
        "            loss = train_no_teacher(lines[0])\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f'[(Epoch {epoch}/{n_epochs}) Iter {i}] Loss: {loss:.4f}')\n",
        "                print(generate(decoder, 'Wh', 50), '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huWVu9GfPtHt",
        "outputId": "74ab6339-2a25-4fb0-8a0a-741c4466682a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 5 epochs...\n",
            "[(Epoch 1/5) Iter 0] Loss: 4.8835\n",
            "Wh|_QMNxT*^&w\\EOnM4\r|Gk3,\u000fn\u0006s>N+L\\4!#S@G]@4\\aDa\u001aAT\u0019v \n",
            "\n",
            "[(Epoch 1/5) Iter 10] Loss: 4.3640\n",
            "Wh\t\u001fS8N\u001eyuCO[0\"x\u0005o\\\u0019~\u0006T%Qoe454UH@\u0015h\u001b\u0003mI+|ekir\u0000@[\u0006g\u0003B \n",
            "\n",
            "[(Epoch 1/5) Iter 20] Loss: 3.6058\n",
            "Whol\u000fn}seo{n:n,et\u0011eeeqmnr#a,i?iyeeu]ehii;troJKlw\u0017\u0016i\u0003 \n",
            "\n",
            "[(Epoch 1/5) Iter 30] Loss: 3.0673\n",
            "Wh=\"kq*:nrrowelseahprasoolcw'sesriYaoe\u0004shelwtwnnVobe \n",
            "\n",
            "[(Epoch 1/5) Iter 40] Loss: 3.0872\n",
            "Whusgoaretweataiteoanpbortueiskfmathtseeooetwiaaltdv \n",
            "\n",
            "[(Epoch 2/5) Iter 0] Loss: 3.7440\n",
            "Whs;oretitofastlddhaddaowhe\u0011solasaeeeent\u001dmhhehorwoei \n",
            "\n",
            "[(Epoch 2/5) Iter 10] Loss: 3.0659\n",
            "Whemreoworoilt,ho:ermwfaoldoltwtoce,nistiootoeoagaol \n",
            "\n",
            "[(Epoch 2/5) Iter 20] Loss: 3.0143\n",
            "Wh}egwtsdotdslfemotatoy'rretstgeeeaoenreoldfd'gokere \n",
            "\n",
            "[(Epoch 2/5) Iter 30] Loss: 3.1307\n",
            "Whthrhtbohhieeehadtroceearcegtncrgererit:aeemhosytne \n",
            "\n",
            "[(Epoch 2/5) Iter 40] Loss: 3.0074\n",
            "Wh$ocr:lenriairktrsddeesdputnoyrugmhe:iteotitdehteoa \n",
            "\n",
            "[(Epoch 3/5) Iter 0] Loss: 3.3336\n",
            "Wh.chehhenraeehoecmalehnrlaiawoeeermv'o:eottosso,haa \n",
            "\n",
            "[(Epoch 3/5) Iter 10] Loss: 2.9854\n",
            "Whukrba'eaeetneemlrhosuns:o'uardeogaheat,inloieheleu \n",
            "\n",
            "[(Epoch 3/5) Iter 20] Loss: 2.9019\n",
            "Whhrwukd,osotonexveealoataul:aatayhtrohnekhdtujymmay \n",
            "\n",
            "[(Epoch 3/5) Iter 30] Loss: 3.3592\n",
            "Whhhthwltpruohnnhnhdnufte,.::ynoisfeottl,ythfoudgr,n \n",
            "\n",
            "[(Epoch 3/5) Iter 40] Loss: 2.9830\n",
            "Wh{itorrthvognttoadotydmmituint,ixosufaldedeehsoeeui \n",
            "\n",
            "[(Epoch 4/5) Iter 0] Loss: 3.1531\n",
            "Whlanofhutoolrteishttltaphhgh.uieahicadtrtnhds,ttlse \n",
            "\n",
            "[(Epoch 4/5) Iter 10] Loss: 2.7809\n",
            "Whdvreussos,eteadneoe,naefrodemhsasaueh,heoiianwetbt \n",
            "\n",
            "[(Epoch 4/5) Iter 20] Loss: 3.0799\n",
            "Whthebhsmmyyylget'rtagysesnuf?dehcrsoeieiihelnsdodek \n",
            "\n",
            "[(Epoch 4/5) Iter 30] Loss: 3.0569\n",
            "Whelsiest:eeerne:nim,waeosh'rshsh,osmonredhlriarstas \n",
            "\n",
            "[(Epoch 4/5) Iter 40] Loss: 3.3561\n",
            "Whrmghplhwidomoutalhwtweydaa.n,srus,atuhohh:oeehthsc \n",
            "\n",
            "[(Epoch 5/5) Iter 0] Loss: 3.0391\n",
            "Whglrda,lsadotatawud:nyiat,lhculeacshlouo,nihihtrdaa \n",
            "\n",
            "[(Epoch 5/5) Iter 10] Loss: 2.9397\n",
            "Whes,oehflnamhesrhTtodpweneihoramgaoisansdshotofieoo \n",
            "\n",
            "[(Epoch 5/5) Iter 20] Loss: 3.0046\n",
            "Whnhstfciodesedciooheaeesoieainsihrtsolmuosehcnclinl \n",
            "\n",
            "[(Epoch 5/5) Iter 30] Loss: 4.0275\n",
            "Whrinuectdmtneoyeedercteelisaae'tcbsrn,iocththcilhnt \n",
            "\n",
            "[(Epoch 5/5) Iter 40] Loss: 3.6162\n",
            "Whnneh,arrec,hsapmotdtiaimireertteneqneengmlfenv,ore \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Original source from\n",
        "# https://gist.github.com/Tushar-N/dfca335e370a2bc3bc79876e6270099e\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "\n",
        "def flatten(l):\n",
        "    return list(itertools.chain.from_iterable(l))\n",
        "\n",
        "seqs = ['ghatmasala', 'nicela', 'chutpakodas']\n",
        "\n",
        "# make <pad> idx 0\n",
        "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))\n",
        "\n",
        "# make model\n",
        "embedding_size = 3\n",
        "embed = nn.Embedding(len(vocab), embedding_size)\n",
        "lstm = nn.LSTM(embedding_size, 5)\n",
        "\n",
        "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
        "print(\"vectorized_seqs\", vectorized_seqs)\n",
        "\n",
        "print([x for x in map(len, vectorized_seqs)])\n",
        "# get the length of each seq in your batch\n",
        "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n",
        "\n",
        "# dump padding everywhere, and place seqs on the left.\n",
        "# NOTE: you only need a tensor as big as your longest sequence\n",
        "seq_tensor = Variable(torch.zeros(\n",
        "    (len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "print(\"seq_tensor\", seq_tensor)\n",
        "\n",
        "# SORT YOUR TENSORS BY LENGTH!\n",
        "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "seq_tensor = seq_tensor[perm_idx]\n",
        "\n",
        "print(\"seq_tensor after sorting\", seq_tensor)\n",
        "\n",
        "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
        "# Otherwise, give (L,B,D) tensors\n",
        "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
        "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n",
        "\n",
        "# embed your sequences\n",
        "embeded_seq_tensor = embed(seq_tensor)\n",
        "print(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n",
        "\n",
        "# pack them up nicely\n",
        "packed_input = pack_padded_sequence(\n",
        "    embeded_seq_tensor, seq_lengths.cpu().numpy())\n",
        "\n",
        "# throw them through your LSTM (remember to give batch_first=True here if\n",
        "# you packed with it)\n",
        "packed_output, (ht, ct) = lstm(packed_input)\n",
        "\n",
        "# unpack your output if required\n",
        "output, _ = pad_packed_sequence(packed_output)\n",
        "print(\"Lstm output\", output.size(), output.data)\n",
        "\n",
        "# Or if you just want the final hidden state?\n",
        "print(\"Last output\", ht[-1].size(), ht[-1].data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4SVThZAYxOd",
        "outputId": "4270bcd9-3eae-49f5-9b0a-39cb3b82ae45"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorized_seqs [[5, 6, 1, 15, 10, 1, 14, 1, 9, 1], [11, 7, 2, 4, 9, 1], [2, 6, 16, 15, 13, 1, 8, 12, 3, 1, 14]]\n",
            "[10, 6, 11]\n",
            "seq_tensor tensor([[ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
            "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0],\n",
            "        [ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14]])\n",
            "seq_tensor after sorting tensor([[ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14],\n",
            "        [ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
            "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0]])\n",
            "seq_tensor after transposing torch.Size([11, 3]) tensor([[ 2,  5, 11],\n",
            "        [ 6,  6,  7],\n",
            "        [16,  1,  2],\n",
            "        [15, 15,  4],\n",
            "        [13, 10,  9],\n",
            "        [ 1,  1,  1],\n",
            "        [ 8, 14,  0],\n",
            "        [12,  1,  0],\n",
            "        [ 3,  9,  0],\n",
            "        [ 1,  1,  0],\n",
            "        [14,  0,  0]])\n",
            "seq_tensor after embeding torch.Size([11, 3, 3]) tensor([[ 2,  5, 11],\n",
            "        [ 6,  6,  7],\n",
            "        [16,  1,  2],\n",
            "        [15, 15,  4],\n",
            "        [13, 10,  9],\n",
            "        [ 1,  1,  1],\n",
            "        [ 8, 14,  0],\n",
            "        [12,  1,  0],\n",
            "        [ 3,  9,  0],\n",
            "        [ 1,  1,  0],\n",
            "        [14,  0,  0]])\n",
            "Lstm output torch.Size([11, 3, 5]) tensor([[[ 0.2472, -0.0630, -0.0046,  0.1349, -0.1221],\n",
            "         [ 0.4533, -0.1031,  0.1777,  0.1143,  0.0689],\n",
            "         [ 0.1617, -0.0149, -0.1134,  0.1331, -0.1961]],\n",
            "\n",
            "        [[ 0.3939, -0.0961, -0.0630,  0.2276, -0.1851],\n",
            "         [ 0.4395, -0.1213, -0.0235,  0.2179, -0.1176],\n",
            "         [ 0.4038, -0.0429, -0.1900,  0.2457, -0.2718]],\n",
            "\n",
            "        [[ 0.2852, -0.0832, -0.1179,  0.2131, -0.2208],\n",
            "         [ 0.4397, -0.0925, -0.0594,  0.2502, -0.1013],\n",
            "         [ 0.3694, -0.0595, -0.1566,  0.2433, -0.1984]],\n",
            "\n",
            "        [[ 0.1044,  0.0146, -0.2620,  0.2917, -0.2528],\n",
            "         [ 0.1668, -0.0565, -0.2532,  0.2909, -0.2367],\n",
            "         [ 0.3872, -0.0847, -0.0896,  0.2358, -0.0928]],\n",
            "\n",
            "        [[ 0.4005,  0.0021, -0.3742,  0.3411, -0.2899],\n",
            "         [ 0.3359, -0.0609, -0.3065,  0.2922, -0.2130],\n",
            "         [ 0.2271,  0.0179, -0.1715,  0.2081, -0.2217]],\n",
            "\n",
            "        [[ 0.4097, -0.0385, -0.2527,  0.2939, -0.1357],\n",
            "         [ 0.3927, -0.0682, -0.2182,  0.2753, -0.1220],\n",
            "         [ 0.3741, -0.0426, -0.1413,  0.2681, -0.1127]],\n",
            "\n",
            "        [[ 0.2160, -0.0077, -0.3274,  0.3077, -0.2532],\n",
            "         [ 0.3301, -0.1056, -0.1803,  0.2393, -0.1232],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.6027, -0.1017,  0.0125,  0.2577,  0.1982],\n",
            "         [ 0.3958, -0.0891, -0.1508,  0.2660, -0.0898],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3440, -0.0969, -0.2654,  0.2990, -0.1177],\n",
            "         [ 0.2311, -0.0202, -0.1999,  0.2105, -0.2237],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.4171, -0.0766, -0.2025,  0.2581, -0.0847],\n",
            "         [ 0.3736, -0.0521, -0.1641,  0.2667, -0.1110],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.3456, -0.1161, -0.1775,  0.2320, -0.1009],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n",
            "Last output torch.Size([3, 5]) tensor([[ 0.3456, -0.1161, -0.1775,  0.2320, -0.1009],\n",
            "        [ 0.3736, -0.0521, -0.1641,  0.2667, -0.1110],\n",
            "        [ 0.3741, -0.0426, -0.1413,  0.2681, -0.1127]])\n"
          ]
        }
      ]
    }
  ]
}